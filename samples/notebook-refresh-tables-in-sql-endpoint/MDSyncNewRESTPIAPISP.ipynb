{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7c13d-69de-49b8-87fe-1135ca991601",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install adal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18bb8b5-a7a1-4b2e-bfb0-7416d519333f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "## Example of using the MD Sync REST API, using Service Prinipal\n",
    "\n",
    "import msal\n",
    "import requests\n",
    "import adal\n",
    "\n",
    " \n",
    "def get_sqlendpoint(workspaceId, header,lakehouse_name):\n",
    "    \"\"\"\n",
    "    This list the SQL endpoints for the given workspace.\n",
    "    \"\"\"\n",
    "    getsqlendpoint = f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/sqlEndpoints'\n",
    "    # Make the GET request\n",
    "    response = requests.get(url=getsqlendpoint, headers=header)\n",
    "    # Extract the list of SQL endpoints from the response and find the SQl Endoint ID for the specfied lakehouse\n",
    "    # git_status = response.json()\n",
    "    # print(git_status)\n",
    "    get_sqlenpoints=response.json().get('value', [])\n",
    "    for sqlendpoint in get_sqlenpoints:\n",
    "        if sqlendpoint['displayName'].lower() == lakehouse_name.lower():\n",
    "            sqlendpoint_id = sqlendpoint['id']\n",
    "            return(sqlendpoint_id)\n",
    "def refresh_sql_endpoint(sqlendpoint_id, header,workspaceId):\n",
    "    \"\"\"\n",
    "    Calls the SQL Endpoint refresh API for the specified SQL Endpoint ID.\n",
    "    \"\"\"\n",
    "    #refresh_url = f'https://api.powerbi.com/v1/workspaces/{workspaceId}/sqlEndpoints/{sqlendpoint_id}/refreshMetadata?preview=true' #preview=true is not needed anymore\n",
    "    # The URL for the SQL Endpoint refresh API\n",
    "    refresh_url = f'https://api.powerbi.com/v1/workspaces/{workspaceId}/sqlEndpoints/{sqlendpoint_id}/refreshMetadata'\n",
    "    print(refresh_url)\n",
    "    # Define the payload for the refresh command\n",
    "    # The payload is a JSON object that specifies the command to refresh the SQL Endpoint\n",
    "    payload = {} \n",
    "    # Make the POST request to refresh the SQL Endpoint\n",
    "    response = requests.post(url=refresh_url, headers=header,json=payload)\n",
    "    if response.status_code == 202:\n",
    "        print(f\"SQL Endpoint with ID '{sqlendpoint_id}' is being refreshed.\")\n",
    "    else:\n",
    "        print(f\"Status Code: {response.status_code}, Response: {response.text}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    #TODO: Replace the below to code to pull from information from Key Vault.\n",
    "    #https://learn.microsoft.com/en-us/azure/key-vault/secrets/quick-create-python?tabs=azure-cli\n",
    "    TENANT_ID = '__add the tenant id__'\n",
    "    CLIENT_ID = '__add the client id__'\n",
    "    CLIENT_SECRET = '__add the client secret__'\n",
    "    workspaceId = '__put_your_workspace_here__'\n",
    "    lakehouse_name='__put_your_lakehouse_here__'\n",
    "    # Azure Resource Manager for your Azure subscription\n",
    "    RESOURCE = 'https://analysis.windows.net/powerbi/api'\n",
    "    # The authority URL\n",
    "    AUTHORITY_URL = f'https://login.microsoftonline.com/{TENANT_ID}'\n",
    "    # The scope for the token\n",
    "    SCOPE = ['https://api.fabric.microsoft.com/.default']\n",
    "\n",
    "    # Create a public client application\n",
    "    app = msal.ConfidentialClientApplication(\n",
    "        client_id=CLIENT_ID,\n",
    "        client_credential=CLIENT_SECRET,\n",
    "        authority=AUTHORITY_URL\n",
    "    )\n",
    "    result = app.acquire_token_for_client(scopes=SCOPE)\n",
    "    if 'access_token' in result:\n",
    "        access_token = result['access_token']\n",
    "    else:\n",
    "        raise Exception(f\"Failed to acquire token: {result.get('error_description')}\")\n",
    "\n",
    "    header = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {access_token}'\n",
    "    }\n",
    "    sqlendpoint_id = get_sqlendpoint(workspaceId, header, lakehouse_name)\n",
    "    refresh_sql_endpoint(sqlendpoint_id, header,workspaceId)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "f6d19585-99bf-40a8-bfe2-d6ece853538e",
    "default_lakehouse_name": "lakehouse",
    "default_lakehouse_workspace_id": "d62d3af9-da91-4500-ac8b-78cd389e34cd",
    "known_lakehouses": [
     {
      "id": "f6d19585-99bf-40a8-bfe2-d6ece853538e"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
