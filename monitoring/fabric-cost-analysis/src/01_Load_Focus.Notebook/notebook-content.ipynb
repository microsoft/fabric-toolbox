{"cells":[{"cell_type":"code","execution_count":null,"id":"ca1690e2-4c4e-4eb4-ab99-c303119f9c6a","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["from delta.tables import *\n","from notebookutils import mssparkutils\n","from datetime import datetime, timedelta\n","from dateutil.relativedelta import relativedelta\n","from pyspark.sql.utils import AnalysisException\n","from pyspark.sql.functions import col, when, from_json, date_format, lit, row_number,max, lower\n","from pyspark.sql.types import StructType, StringType\n","from pyspark.sql.window import Window\n","import re"]},{"cell_type":"code","execution_count":null,"id":"e55c40ac-00e9-42f9-8c10-502e81702d51","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"outputs":[],"source":["fromMonth = 0 #-1, -2,... from datenow -1 day\n","toMonth = 0 #-1, -2,... from datenow -1 day\n","rawSourcePath = \"Files/focuscost\"#\"Files/Costs\"#"]},{"cell_type":"markdown","id":"6b716fa9-6b0f-4159-bcea-e22256a8052d","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## FUNCTIONS"]},{"cell_type":"code","execution_count":null,"id":"eb8b57c4-a4eb-48b5-baa5-861a8444c023","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["def find_first_parquet_file(path):\n","    \"\"\"\n","    Recursively search for the first .parquet file in the given directory.\n","    Args:\n","        path (str): The root directory to start the search.\n","    Returns:\n","        str or None: The full path to the first .parquet file found, or None if not found.\n","    \"\"\"\n","    try:\n","        for entry in mssparkutils.fs.ls(path):\n","            if entry.isFile and entry.name.endswith(\".parquet\"):\n","                return entry.path\n","            elif entry.isDir:\n","                result = find_first_parquet_file(entry.path)\n","                if result:\n","                    return result\n","    except Exception as e:\n","        print(f\"Error accessing {path}: {e}\")\n","    return None\n","\n","def generate_wildcard_path(full_path: str, raw_source_path: str, current_Date_Folder: str, snapshot_folder: str) -> str:\n","    # Find the index where the raw source path starts\n","    idx = full_path.find(raw_source_path)\n","    if idx == -1:\n","        raise ValueError(\"rawSourcePath not found in full path\")\n","\n","    # Extract the base URI before the raw source path\n","    base_uri = full_path[:idx]\n","    detailPath = full_path[idx+len(rawSourcePath):]\n","\n","    # Extract the base URI before the raw source path\n","    idx = detailPath.find(current_Date_Folder)\n","    detailPreMonth = detailPath[:idx]  #/fdfd/fdfd\n","    detailPostMonth = detailPath[idx+len(current_Date_Folder)-1:] # /fdfd/fdfdfd/fdfddf.parquet\n","\n","    startleveltoAddPre = detailPreMonth.count('/')\n","    startleveltoAddPost = detailPostMonth.count('/')\n","\n","    # Construct the wildcard path\n","    wildcard_path = f\"{base_uri}{raw_source_path}{'/*' * startleveltoAddPre}/{snapshot_folder}{'/*' * startleveltoAddPost}.parquet\"\n","    return wildcard_path\n","\n","def generateArrayOFPeriod(from_Month: int, to_month: int):\n","    # Get today's date\n","    today = datetime.today()\n","\n","    # Subtract one day\n","    yesterday = today - timedelta(days=1)\n","    first_day = yesterday.replace(day=1)\n","\n","    periodToLoad = []\n","    if from_Month == to_month:\n","        periodDate = first_day + relativedelta(months=to_month)\n","        periodToLoad.append(periodDate.date())\n","    else:\n","        for i in range(from_Month, to_month+1):\n","            periodDate = first_day + relativedelta(months=i)\n","            periodToLoad.append(periodDate.date())\n","\n","    return periodToLoad\n","\n","\n","def AddCapacityPauseColumn(dfsource):\n","    # Define the schema for the JSON structure. In this version, only for Fabric billingtype\n","    schema = StructType().add(\"BillingType\", StringType())\n","\n","    df_parsed = dfsource.withColumn(\"parsed_json\", from_json(col(\"x_SkuDetails\"), schema))\n","\n","    # Create the new column based on the condition\n","    df_transformed = df_parsed.withColumn(\"CapacityPause\", when(col(\"parsed_json.BillingType\") == \"Capacity Pause/Delete Surcharge\", True).otherwise(False))\n","\n","    # Optionally drop the intermediate parsed column\n","    df_final = df_transformed.drop(\"parsed_json\")\n","\n","    return df_final\n","\n","def AddBillingTypeColumn(dfsource):\n","    # Define the schema for the JSON structure. In this version, only for Fabric billingtype\n","    schema = StructType().add(\"BillingType\", StringType())\n","\n","    df_parsed = dfsource.withColumn(\"parsed_json\", from_json(col(\"x_SkuDetails\"), schema))\n","\n","    # Create the new column based on the condition\n","    df_transformed = df_parsed.withColumn(\"BillingType\", col(\"parsed_json.BillingType\"))\n","\n","    # Optionally drop the intermediate parsed column\n","    df_final = df_transformed.drop(\"parsed_json\")\n","\n","    return df_final\n","\n"]},{"cell_type":"markdown","id":"11ecff50-1936-4942-ad83-d833541a7b2d","metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## STEP 1 Load Silver:\n","Load into bronze table\n","Identify context and prepare load in silver\n","- Delete previous data\n","- Clean date format\n","\n"]},{"cell_type":"code","execution_count":null,"id":"af0cfa93-d2e2-4f77-9447-24068090eae5","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["structurePath = find_first_parquet_file(rawSourcePath)\n","periodsToLoad = generateArrayOFPeriod(fromMonth, toMonth)\n","\n"," #identify the date format\n","current_Date_Folder = \"\"\n","print(f\"Analyze structurePath to find date pattern: {structurePath}\")\n","match = re.search(r\"\\/[1-2][0-9][0-9][0-9]\\/[0-1][0-9]\\/\", structurePath)\n","if match:\n","    current_Date_Folder = match.group()\n","    print(f\"Find monthly date: {current_Date_Folder}\")\n","    date_pattern = \"YYYY/MM\"\n","else:\n","    match = re.search(r\"\\/[1-2][0-9][0-9][0-9][0-1][0-9][0-3][0-9]-[1-2][0-9][0-9][0-9][0-1][0-9][0-3][0-9]\\/\", structurePath)\n","    if match:\n","        current_Date_Folder = match.group()\n","        print(f\"Find monthly date: {current_Date_Folder}\")\n","        date_pattern = \"YYYYMMDD-YYYYMMDD\"\n","\n","if (current_Date_Folder == \"\"):\n","    raise ValueError(\"No Month Pattern found in structurePath\")"]},{"cell_type":"code","execution_count":null,"id":"c3b19339-fb7a-41a8-9deb-18d1ce43066f","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["for per in periodsToLoad:\n","    print(\"Start Period : \" + per.strftime(\"%Y-%m-%d\"))\n","\n","    #drop staging table if exists\n","    spark.sql(\"DROP TABLE IF EXISTS focus_staging\")\n","\n","\n","    #generate storage path date part\n","    if date_pattern == \"YYYY/MM\":\n","        snapshot_folder = per.strftime(\"%Y/%m\")\n","    else:\n","        fromFormatedDate = per.strftime(\"%Y%m%d\")\n","        toFormatedDate = (per + relativedelta(months=1) + relativedelta(days=-1)).strftime(\"%Y%m%d\")\n","        snapshot_folder = fromFormatedDate + \"-\" + toFormatedDate\n","\n","    wildcard = generate_wildcard_path(structurePath, rawSourcePath, current_Date_Folder, snapshot_folder)\n","    print(\"Used path to load data: \" + wildcard)\n","\n","\n","    try:\n","        df = spark.read.parquet(wildcard)\n","        df.write.format('delta').saveAsTable(\"focus_staging\")\n","\n","        #identify period loaded\n","        df = spark.sql(\"SELECT BillingPeriodStart FROM focus_staging LIMIT 1\")\n","        value = df.first()['BillingPeriodStart']\n","\n","        #clean existing data in silver\n","        if spark.catalog.tableExists(\"focus\"):\n","            print(\"Table exists, snapshot will be clean.\")\n","            spark.sql(f\"DELETE FROM focus WHERE BillingPeriodStart = '{value}'\")\n","\n","        #Load data in silver\n","        focus_staging_df = DeltaTable.forPath(spark,\"Tables/focus_staging\").toDF()\n","        focus_staging_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"focus\")\n","\n","    except AnalysisException as e:\n","        if \"PATH_NOT_FOUND\" in str(e):\n","            print(f\"Path not found: {wildcard}\")\n","        else:\n","            raise # re-raise if it's a different AnalysisException\n","\n","    print(\"End Period : \" + per.strftime(\"%Y-%m-%d\"))\n"]},{"cell_type":"code","execution_count":null,"id":"cc70948e-e88a-4aca-920f-f713b03daf7f","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":[]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"897aa7f9-9a59-4b8a-ad72-551118dc1109","default_lakehouse_name":"FCA","default_lakehouse_workspace_id":"57bceddc-a995-44a7-bfb5-1d5f11ad1e98","known_lakehouses":[{"id":"897aa7f9-9a59-4b8a-ad72-551118dc1109"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"state":{},"version":"0.1"}},"nbformat":4,"nbformat_minor":5}