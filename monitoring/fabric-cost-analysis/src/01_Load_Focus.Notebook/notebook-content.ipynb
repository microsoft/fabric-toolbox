{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1690e2-4c4e-4eb4-ab99-c303119f9c6a",
   "metadata": {
    "cellStatus": "{\"System Administrator\":{\"session_start_time\":\"2025-07-11T10:39:14.3754399Z\",\"execution_start_time\":\"2025-07-11T10:39:25.6466912Z\",\"execution_finish_time\":\"2025-07-11T10:39:26.0570192Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from notebookutils import mssparkutils\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import col, when, from_json, date_format, lit, row_number,max, lower\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c40ac-00e9-42f9-8c10-502e81702d51",
   "metadata": {
    "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-07-11T10:39:26.0591634Z\",\"execution_finish_time\":\"2025-07-11T10:39:26.3994384Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "fromMonth = 0 #-1, -2,... from datenow -1 day\n",
    "toMonth = 0 #-1, -2,... from datenow -1 day\n",
    "rawSourcePath = \"Files/focuscost\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b716fa9-6b0f-4159-bcea-e22256a8052d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b57c4-a4eb-48b5-baa5-861a8444c023",
   "metadata": {
    "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-07-11T10:39:26.4016266Z\",\"execution_finish_time\":\"2025-07-11T10:39:26.7086809Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def find_first_parquet_file(path):\n",
    "    \"\"\"\n",
    "    Recursively search for the first .parquet file in the given directory.\n",
    "    Args:\n",
    "        path (str): The root directory to start the search.\n",
    "    Returns:\n",
    "        str or None: The full path to the first .parquet file found, or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for entry in mssparkutils.fs.ls(path):\n",
    "            if entry.isFile and entry.name.endswith(\".parquet\"):\n",
    "                return entry.path\n",
    "            elif entry.isDir:\n",
    "                result = find_first_parquet_file(entry.path)\n",
    "                if result:\n",
    "                    return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def generate_wildcard_path(full_path: str, raw_source_path: str, snapshot_folder: str) -> str:\n",
    "    # Find the index where the raw source path starts\n",
    "    idx = full_path.find(raw_source_path)\n",
    "    if idx == -1:\n",
    "        raise ValueError(\"rawSourcePath not found in full path\")\n",
    "\n",
    "    # Extract the base URI before the raw source path\n",
    "    base_uri = full_path[:idx]\n",
    "    detailPath = structurePath[idx+len(rawSourcePath):]\n",
    "\n",
    "    startleveltoAdd = detailPath.count('/') - '/date-date/Guid/name.parquet'.count('/')\n",
    "\n",
    "    # Construct the wildcard path\n",
    "    wildcard_path = f\"{base_uri}{raw_source_path}{'/*' * startleveltoAdd}/{snapshot_folder}/*/*.parquet\"\n",
    "    return wildcard_path\n",
    "\n",
    "def generateArrayOFPeriod(from_Month: int, to_month: int):\n",
    "    # Get today's date\n",
    "    today = datetime.today()\n",
    "\n",
    "    # Subtract one day\n",
    "    yesterday = today - timedelta(days=1)\n",
    "    first_day = yesterday.replace(day=1)\n",
    "\n",
    "    periodToLoad = []\n",
    "    if from_Month == to_month:\n",
    "        periodDate = first_day + relativedelta(months=to_month)\n",
    "        periodToLoad.append(periodDate.date())\n",
    "    else:\n",
    "        for i in range(from_Month, to_month+1):\n",
    "            periodDate = first_day + relativedelta(months=i)\n",
    "            periodToLoad.append(periodDate.date())\n",
    "\n",
    "    return periodToLoad\n",
    "\n",
    "\n",
    "def AddCapacityPauseColumn(dfsource):\n",
    "    # Define the schema for the JSON structure. In this version, only for Fabric billingtype\n",
    "    schema = StructType().add(\"BillingType\", StringType())\n",
    "\n",
    "    df_parsed = dfsource.withColumn(\"parsed_json\", from_json(col(\"x_SkuDetails\"), schema))\n",
    "\n",
    "    # Create the new column based on the condition\n",
    "    df_transformed = df_parsed.withColumn(\"CapacityPause\", when(col(\"parsed_json.BillingType\") == \"Capacity Pause/Delete Surcharge\", True).otherwise(False))\n",
    "\n",
    "    # Optionally drop the intermediate parsed column\n",
    "    df_final = df_transformed.drop(\"parsed_json\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "def AddBillingTypeColumn(dfsource):\n",
    "    # Define the schema for the JSON structure. In this version, only for Fabric billingtype\n",
    "    schema = StructType().add(\"BillingType\", StringType())\n",
    "\n",
    "    df_parsed = dfsource.withColumn(\"parsed_json\", from_json(col(\"x_SkuDetails\"), schema))\n",
    "\n",
    "    # Create the new column based on the condition\n",
    "    df_transformed = df_parsed.withColumn(\"BillingType\", col(\"parsed_json.BillingType\"))\n",
    "\n",
    "    # Optionally drop the intermediate parsed column\n",
    "    df_final = df_transformed.drop(\"parsed_json\")\n",
    "\n",
    "    return df_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ecff50-1936-4942-ad83-d833541a7b2d",
   "metadata": {
    "microsoft": {
     "language": "sparksql",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## STEP 1 Load Silver:\n",
    "Load into bronze table\n",
    "Identify context and prepare load in silver\n",
    "- Delete previous data\n",
    "- Clean date format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0cfa93-d2e2-4f77-9447-24068090eae5",
   "metadata": {
    "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-07-11T10:39:26.7108458Z\",\"execution_finish_time\":\"2025-07-11T10:39:30.117208Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "structurePath = find_first_parquet_file(rawSourcePath)\n",
    "periodsToLoad = generateArrayOFPeriod(fromMonth, toMonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b19339-fb7a-41a8-9deb-18d1ce43066f",
   "metadata": {
    "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-07-10T08:29:11.3535724Z\",\"execution_finish_time\":\"2025-07-10T08:29:23.4131426Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "for per in periodsToLoad:\n",
    "    print(\"Start Period : \" + per.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    #drop staging table if exists\n",
    "    spark.sql(\"DROP TABLE IF EXISTS focus_staging\")\n",
    "\n",
    "    #generate storage path date part\n",
    "    fromFormatedDate = per.strftime(\"%Y%m%d\")\n",
    "    toFormatedDate = (per + relativedelta(months=1) + relativedelta(days=-1)).strftime(\"%Y%m%d\")\n",
    "    snapshot_folder = fromFormatedDate + \"-\" + toFormatedDate\n",
    "\n",
    "    wildcard = generate_wildcard_path(structurePath, rawSourcePath, snapshot_folder)\n",
    "    print(\"Used path to load data: \" + wildcard)\n",
    "\n",
    "    try:\n",
    "        df = spark.read.parquet(wildcard)\n",
    "        df.write.format('delta').saveAsTable(\"focus_staging\")\n",
    "\n",
    "        #identify period loaded\n",
    "        df = spark.sql(\"SELECT BillingPeriodStart FROM focus_staging LIMIT 1\")\n",
    "        value = df.first()['BillingPeriodStart']\n",
    "\n",
    "        #clean existing data in silver\n",
    "        if spark.catalog.tableExists(\"focus\"):\n",
    "            print(\"Table exists, snapshot will be clean.\")\n",
    "            spark.sql(f\"DELETE FROM focus WHERE BillingPeriodStart = '{value}'\")\n",
    "\n",
    "        #Load data in silver\n",
    "        focus_staging_df = DeltaTable.forPath(spark,\"Tables/focus_staging\").toDF()\n",
    "        focus_staging_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"focus\")\n",
    "\n",
    "    except AnalysisException as e:\n",
    "        if \"PATH_NOT_FOUND\" in str(e):\n",
    "            print(f\"Path not found: {wildcard}\")\n",
    "        else:\n",
    "            raise # re-raise if it's a different AnalysisException\n",
    "\n",
    "    print(\"End Period : \" + per.strftime(\"%Y-%m-%d\"))\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "edea360e-6c6d-4761-aeca-d57df8bd7b91",
    "default_lakehouse_name": "FCA",
    "default_lakehouse_workspace_id": "d7ae03b3-c53b-4ee6-af7e-0091be2e7cc4",
    "known_lakehouses": [
     {
      "id": "edea360e-6c6d-4761-aeca-d57df8bd7b91"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
