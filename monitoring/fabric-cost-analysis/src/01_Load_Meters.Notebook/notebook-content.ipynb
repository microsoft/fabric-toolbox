{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f8607-f28f-495b-bf22-189c501245f8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-17T10:02:47.7689371Z",
       "execution_start_time": "2025-07-17T10:02:47.4258902Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "4646b6e6-aead-414e-a885-973925f6eecf",
       "queued_time": "2025-07-17T10:02:36.8674169Z",
       "session_id": "1425fb1d-bf6d-4ac6-99d7-eebbd3c50181",
       "session_start_time": "2025-07-17T10:02:36.8682922Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 1425fb1d-bf6d-4ac6-99d7-eebbd3c50181, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "from notebookutils import mssparkutils\n",
    "from pyspark.sql.functions import row_number,max, lit, lower\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import os\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\") # needed for automatic schema evolution in merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534a8dc-9c54-4709-be4b-053189f47714",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-17T10:09:47.7811531Z",
       "execution_start_time": "2025-07-17T10:09:47.003869Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "55de5515-2692-4814-ab9e-9b482f171cbe",
       "queued_time": "2025-07-17T10:09:47.002779Z",
       "session_id": "1425fb1d-bf6d-4ac6-99d7-eebbd3c50181",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, 1425fb1d-bf6d-4ac6-99d7-eebbd3c50181, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to /lakehouse/default/Files/Data3/Meters/Meters.csv\n"
     ]
    }
   ],
   "source": [
    "# URL of the CSV file\n",
    "csv_url = \"https://raw.githubusercontent.com/Pulsweb/fabric-toolbox/refs/heads/2025-10-31-Release/monitoring/fabric-cost-analysis/data/Meters.csv\"\n",
    "\n",
    "# Define the target path in the Lakehouse\n",
    "lakehouse_path = \"/lakehouse/default/Files/Data/Meters\"\n",
    "file_name = \"Meters.csv\"\n",
    "full_path = f\"{lakehouse_path}/{file_name}\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(lakehouse_path, exist_ok=True)\n",
    "\n",
    "# Download and save the file\n",
    "response = requests.get(csv_url)\n",
    "if response.status_code == 200:\n",
    "    with open(full_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"File saved to {full_path}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca4fe4-3a74-4521-a688-7247e86a3f95",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-10T13:04:46.019827Z",
       "execution_start_time": "2025-07-10T13:04:45.0943763Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "01eab3c2-399a-4a53-ae7f-34b4ab11e423",
       "queued_time": "2025-07-10T13:04:45.0931932Z",
       "session_id": "ec990d2c-0ce7-4055-9c11-5eaabd5af800",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 47,
       "statement_ids": [
        47
       ]
      },
      "text/plain": [
       "StatementMeta(, ec990d2c-0ce7-4055-9c11-5eaabd5af800, 47, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File exists: Files/Data/Meters/Meters.csv\n"
     ]
    }
   ],
   "source": [
    "meterTableAlreadyExists = spark.catalog.tableExists(\"Meters\")\n",
    "\n",
    "file_path = \"Files/Data/Meters/Meters.csv\"\n",
    "if notebookutils.fs.exists(file_path):\n",
    "    print(f\"✅ File exists: {file_path}\")\n",
    "else:\n",
    "    print(f\"❌ File does not exist: {file_path}\")\n",
    "    if not meterTableAlreadyExists:\n",
    "        raise Exception(\"No Meters.csv in the lakehouse and No Meters table already exists\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114947d5-5762-4685-b958-8a290982c1c3",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-07-10T13:04:54.296764Z",
       "execution_start_time": "2025-07-10T13:04:47.920274Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c036f7ba-2bba-40fa-a3a0-b9ae2b979e86",
       "queued_time": "2025-07-10T13:04:47.9190632Z",
       "session_id": "ec990d2c-0ce7-4055-9c11-5eaabd5af800",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 48,
       "statement_ids": [
        48
       ]
      },
      "text/plain": [
       "StatementMeta(, ec990d2c-0ce7-4055-9c11-5eaabd5af800, 48, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Data Started\n",
      "Merge Data Ended\n"
     ]
    }
   ],
   "source": [
    "source_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Data/Meters/Meters.csv\")\n",
    "source_df = source_df.withColumn(\"Name_Lower\",lower(\"Name\"))\n",
    "source_df = source_df.dropDuplicates([\"Name_Lower\"])\n",
    "\n",
    "\n",
    "if meterTableAlreadyExists:\n",
    "    #Merge to table\n",
    "    # Load the target Delta table\n",
    "    print(\"Merge Data Started\")\n",
    "    target_table = DeltaTable.forPath(spark, \"Tables/meters\")\n",
    "    target_df = target_table.toDF()\n",
    "    target_df = target_df.select(\"Name_Lower\",\"MeterKey\")\n",
    "\n",
    "\n",
    "    max_key = target_df.agg(max(\"MeterKey\")).collect()[0][0]\n",
    "\n",
    "    combined_df = source_df.join(target_df,\"Name_Lower\",\"leftouter\")\n",
    "    existingRows_df = combined_df.where(combined_df.MeterKey.isNotNull())\n",
    "    newRows_df = combined_df.where(combined_df.MeterKey.isNull())\n",
    "    window_spec = Window.orderBy(\"Name_Lower\")\n",
    "    newRows_df = newRows_df.withColumn(\"MeterKey\", row_number().over(window_spec) + max_key )\n",
    "\n",
    "    Src_Merge_df = existingRows_df.union(newRows_df)\n",
    "\n",
    "\n",
    "    merge = (target_table.alias(\"target\")\n",
    "        .merge(\n",
    "            Src_Merge_df.alias(\"source\"),\n",
    "            \"target.MeterKey = source.MeterKey\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        )\n",
    "    merge.execute()\n",
    "\n",
    "    print(\"Merge Data Ended\")\n",
    "else:\n",
    "    print(\"Table creation started\")\n",
    "    window_spec = Window.orderBy(\"Name_Lower\")\n",
    "    source_df = source_df.withColumn(\"MeterKey\", row_number().over(window_spec))\n",
    "    source_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"Meters\")\n",
    "    print(\"Table creation Ended\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "897aa7f9-9a59-4b8a-ad72-551118dc1109",
    "default_lakehouse_name": "FCA",
    "default_lakehouse_workspace_id": "57bceddc-a995-44a7-bfb5-1d5f11ad1e98",
    "known_lakehouses": [
     {
      "id": "897aa7f9-9a59-4b8a-ad72-551118dc1109"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
