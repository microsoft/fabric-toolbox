{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f8607-f28f-495b-bf22-189c501245f8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T10:28:01.6880285Z",
       "execution_start_time": "2025-10-23T10:28:01.3590669Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6b509050-22b9-4464-ad2e-18f7fbf05e13",
       "queued_time": "2025-10-23T10:28:01.3578708Z",
       "session_id": "d61c0051-47a3-459e-ac4d-371c22968bd8",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 22,
       "statement_ids": [
        22
       ]
      },
      "text/plain": [
       "StatementMeta(, d61c0051-47a3-459e-ac4d-371c22968bd8, 22, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "from notebookutils import mssparkutils\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, when, from_json, date_format, lit, row_number,max, lower, to_date, regexp_replace\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import os\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\") # needed for automatic schema evolution in merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a6592d0-c5e1-403f-8c61-6f2fcf74e2a8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T10:28:02.010631Z",
       "execution_start_time": "2025-10-23T10:28:01.6898255Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "3b68c4cc-8070-4514-9913-a5527864905b",
       "queued_time": "2025-10-23T10:28:01.5570518Z",
       "session_id": "d61c0051-47a3-459e-ac4d-371c22968bd8",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 23,
       "statement_ids": [
        23
       ]
      },
      "text/plain": [
       "StatementMeta(, d61c0051-47a3-459e-ac4d-371c22968bd8, 23, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fromMonth = -2 #-1, -2,... from datenow -1 day \n",
    "toMonth = -1 #-1, -2,... from datenow -1 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4b5c39e-976c-4518-b242-1ba0c8852b87",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T10:28:02.3218269Z",
       "execution_start_time": "2025-10-23T10:28:02.0128461Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6855b46d-be5c-4254-96c4-6692d47d124a",
       "queued_time": "2025-10-23T10:28:01.9762682Z",
       "session_id": "d61c0051-47a3-459e-ac4d-371c22968bd8",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 24,
       "statement_ids": [
        24
       ]
      },
      "text/plain": [
       "StatementMeta(, d61c0051-47a3-459e-ac4d-371c22968bd8, 24, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rawSourcePath = \"Files/reservation-transactions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e41ada1-ad51-4b9e-85f5-d2b8e94ca50b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T10:28:02.9668862Z",
       "execution_start_time": "2025-10-23T10:28:02.6730459Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "7c1999b3-253b-4fd2-bb8b-993ecfba48d2",
       "queued_time": "2025-10-23T10:28:02.6718827Z",
       "session_id": "d61c0051-47a3-459e-ac4d-371c22968bd8",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 25,
       "statement_ids": [
        25
       ]
      },
      "text/plain": [
       "StatementMeta(, d61c0051-47a3-459e-ac4d-371c22968bd8, 25, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_first_csv_file(path):\n",
    "    \"\"\"\n",
    "    Recursively search for the first .csv file in the given directory.\n",
    "    Args:\n",
    "        path (str): The root directory to start the search.\n",
    "    Returns:\n",
    "        str or None: The full path to the first .parquet file found, or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for entry in mssparkutils.fs.ls(path):\n",
    "            if entry.isFile and entry.name.endswith(\".csv\"):\n",
    "                return entry.path\n",
    "            elif entry.isDir:\n",
    "                result = find_first_csv_file(entry.path)\n",
    "                if result:\n",
    "                    return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def generate_wildcard_path(full_path: str, raw_source_path: str, snapshot_folder: str) -> str:\n",
    "    # Find the index where the raw source path starts\n",
    "    idx = full_path.find(raw_source_path)\n",
    "    if idx == -1:\n",
    "        raise ValueError(\"rawSourcePath not found in full path\")\n",
    "\n",
    "    # Extract the base URI before the raw source path\n",
    "    base_uri = full_path[:idx]\n",
    "    detailPath = structurePath[idx+len(rawSourcePath):]\n",
    "\n",
    "    startleveltoAdd = detailPath.count('/') - '/date-date/Guid/name.csv'.count('/')\n",
    "\n",
    "    # Construct the wildcard path\n",
    "    wildcard_path = f\"{base_uri}{raw_source_path}{'/*' * startleveltoAdd}/{snapshot_folder}/*.csv\"\n",
    "    return wildcard_path\n",
    "\n",
    "def generateArrayOFPeriod(from_Month: int, to_month: int):\n",
    "    # Get today's date\n",
    "    today = datetime.today()\n",
    "\n",
    "    # Subtract one day\n",
    "    yesterday = today - timedelta(days=1)\n",
    "    first_day = yesterday.replace(day=1)\n",
    "\n",
    "    periodToLoad = []\n",
    "    if from_Month == to_month:\n",
    "        periodDate = first_day + relativedelta(months=to_month)\n",
    "        periodToLoad.append(periodDate.date())\n",
    "    else:\n",
    "        for i in range(from_Month, to_month+1):\n",
    "            periodDate = first_day + relativedelta(months=i)\n",
    "            periodToLoad.append(periodDate.date())\n",
    "\n",
    "    return periodToLoad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc9f2a-905b-4f8c-9cf1-58d71df2111c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Step1: Load Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9287cb91-7eb3-4c5d-9cdc-6eafbb7d1bd1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T10:28:06.4378081Z",
       "execution_start_time": "2025-10-23T10:28:03.5059871Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b794ab35-c894-4697-885e-20a534db1596",
       "queued_time": "2025-10-23T10:28:03.5049375Z",
       "session_id": "d61c0051-47a3-459e-ac4d-371c22968bd8",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 26,
       "statement_ids": [
        26
       ]
      },
      "text/plain": [
       "StatementMeta(, d61c0051-47a3-459e-ac4d-371c22968bd8, 26, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "structurePath = find_first_csv_file(rawSourcePath)\n",
    "periodsToLoad = generateArrayOFPeriod(fromMonth, toMonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6f66a5c-a69d-46b7-a0f4-416acded1b99",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T10:28:58.3818772Z",
       "execution_start_time": "2025-10-23T10:28:06.4399262Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f737be6f-8684-4a16-87b2-0ee9f6dd7fa8",
       "queued_time": "2025-10-23T10:28:04.4597364Z",
       "session_id": "d61c0051-47a3-459e-ac4d-371c22968bd8",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 27,
       "statement_ids": [
        27
       ]
      },
      "text/plain": [
       "StatementMeta(, d61c0051-47a3-459e-ac4d-371c22968bd8, 27, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Period : 2025-08-01\n",
      "Used path to load data: abfss://b5e66845-fd73-4d5d-b28c-da7dbc2d17ec@onelake.dfs.fabric.microsoft.com/e3e0ae4e-208c-4a38-bf39-9671d2e2e0e6/Files/reservation-transactions/20250801-20250831/*.csv\n",
      "Table exists, snapshot will be clean.\n",
      "Data loaded in silver.\n",
      "End Period : 2025-08-01\n",
      "Start Period : 2025-09-01\n",
      "Used path to load data: abfss://b5e66845-fd73-4d5d-b28c-da7dbc2d17ec@onelake.dfs.fabric.microsoft.com/e3e0ae4e-208c-4a38-bf39-9671d2e2e0e6/Files/reservation-transactions/20250901-20250930/*.csv\n",
      "Table exists, snapshot will be clean.\n",
      "Data loaded in silver.\n",
      "End Period : 2025-09-01\n"
     ]
    }
   ],
   "source": [
    "for per in periodsToLoad:\n",
    "    print(\"Start Period : \" + per.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    #drop staging table if exists\n",
    "    spark.sql(\"DROP TABLE IF EXISTS dim_reservations_staging\")\n",
    "\n",
    "    #generate storage path date part\n",
    "    fromFormatedDate = per.strftime(\"%Y%m%d\")\n",
    "    toFormatedDate = (per + relativedelta(months=1) + relativedelta(days=-1)).strftime(\"%Y%m%d\")\n",
    "    snapshot_folder = fromFormatedDate + \"-\" + toFormatedDate\n",
    "\n",
    "    wildcard = generate_wildcard_path(structurePath, rawSourcePath, snapshot_folder)\n",
    "    print(\"Used path to load data: \" + wildcard)\n",
    "\n",
    "    try:\n",
    "        df = spark.read.csv(wildcard, header=True, inferSchema=True)\n",
    "        df= df.withColumn(\"PeriodLoaded\",to_date(date_format(\"EventType\", \"yyyy-MM-01\")) )\n",
    "        df.write.format('delta').saveAsTable(\"dim_reservations_staging\")\n",
    "\n",
    "        #identify period loaded\n",
    "        df = spark.sql(\"SELECT PeriodLoaded FROM dim_reservations_staging LIMIT 1\")\n",
    "        value = df.first()['PeriodLoaded']\n",
    "\n",
    "        #clean existing data in silver\n",
    "        if spark._jsparkSession.catalog().tableExists('dim_reservations_silver'):\n",
    "            print(\"Table exists, snapshot will be clean.\")\n",
    "            spark.sql(f\"DELETE FROM dim_reservations_silver WHERE PeriodLoaded = '{value}'\")\n",
    "\n",
    "        #Load data in silver\n",
    "        dim_reservations_staging_df = DeltaTable.forPath(spark,\"Tables/dim_reservations_staging\").toDF()\n",
    "        dim_reservations_staging_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"dim_reservations_silver\")\n",
    "        print(\"Data loaded in silver.\")\n",
    "\n",
    "    except AnalysisException as e:\n",
    "        if \"PATH_NOT_FOUND\" in str(e):\n",
    "            print(f\"Path not found: {wildcard}\")\n",
    "        else:\n",
    "            raise # re-raise if it's a different AnalysisException\n",
    "\n",
    "    print(\"End Period : \" + per.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074a9edd-518e-43f3-9d99-c58b61795204",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Step2: Load to Gold Dim Reservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8197a2e6-7a06-4a9e-8079-afa356f1a51d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T10:28:59.2200588Z",
       "execution_start_time": "2025-10-23T10:28:58.384079Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "654c7c78-c4e4-46c8-be92-d6468c88a783",
       "queued_time": "2025-10-23T10:28:05.2485355Z",
       "session_id": "d61c0051-47a3-459e-ac4d-371c22968bd8",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 28,
       "statement_ids": [
        28
       ]
      },
      "text/plain": [
       "StatementMeta(, d61c0051-47a3-459e-ac4d-371c22968bd8, 28, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_df = DeltaTable.forPath(spark,\"Tables/dim_reservations_silver\").toDF()\n",
    "source_df = source_df.where(\"lower(ArmSkuName) = 'fabric_capacity_cu_hour'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba1649ea-bccf-4b8c-860f-cb7839dbcf71",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T10:29:11.9432237Z",
       "execution_start_time": "2025-10-23T10:28:59.2224752Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "e74eb29c-bc63-4848-a639-197d2dff6ed3",
       "queued_time": "2025-10-23T10:28:05.705034Z",
       "session_id": "d61c0051-47a3-459e-ac4d-371c22968bd8",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 29,
       "statement_ids": [
        29
       ]
      },
      "text/plain": [
       "StatementMeta(, d61c0051-47a3-459e-ac4d-371c22968bd8, 29, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Data for reservations table Started\n",
      "Merge Data for reservations Ended\n"
     ]
    }
   ],
   "source": [
    "tableName = \"reservations\"\n",
    "logicalKeyColumn = \"ReservationOrderId\"\n",
    "technicalKeyColumn = \"ReservationOrderKey\"\n",
    "tableAlreadyExists = spark._jsparkSession.catalog().tableExists('FCA', tableName)\n",
    "\n",
    "\n",
    "source_merge_df = source_df\\\n",
    "                 .select(\"Amount\",\"BillingFrequency\",\"Currency\",\"Description\",\"EventDate\",\"PurchasingSubscriptionGuid\",\"PurchasingSubscriptionName\",\"Quantity\",\"Region\",\"ReservationOrderId\",\"ReservationOrderName\")\\\n",
    "                 .distinct()\n",
    "\n",
    "if tableAlreadyExists:\n",
    "    #Merge to table\n",
    "\n",
    "    print(f\"Merge Data for {tableName} table Started\")\n",
    "\n",
    "    target_table = DeltaTable.forPath(spark, f\"Tables/{tableName}\")\n",
    "    target_df = target_table.toDF()\n",
    "    target_df = target_df.select(logicalKeyColumn,technicalKeyColumn)\n",
    "\n",
    "\n",
    "    max_key = target_df.agg(max(technicalKeyColumn)).collect()[0][0]\n",
    "\n",
    "    combined_df = source_merge_df.join(target_df,logicalKeyColumn,\"leftouter\")\n",
    "    existingRows_df = combined_df.where(combined_df[technicalKeyColumn].isNotNull())\n",
    "    newRows_df = combined_df.where(combined_df[technicalKeyColumn].isNull())\n",
    "    window_spec = Window.orderBy(logicalKeyColumn)\n",
    "    newRows_df = newRows_df.withColumn(technicalKeyColumn, row_number().over(window_spec) + max_key )\n",
    "\n",
    "    Src_Merge_df = existingRows_df.union(newRows_df)\n",
    "\n",
    "\n",
    "    merge = (target_table.alias(\"target\")\n",
    "        .merge(\n",
    "            Src_Merge_df.alias(\"source\"),\n",
    "            f\"target.{technicalKeyColumn} = source.{technicalKeyColumn}\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        )\n",
    "    merge.execute()\n",
    "\n",
    "    print(f\"Merge Data for {tableName} Ended\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(f\"Table {tableName} creation started\")\n",
    "    window_spec = Window.orderBy(logicalKeyColumn)\n",
    "    source_merge_df = source_merge_df.withColumn(technicalKeyColumn, row_number().over(window_spec))\n",
    "    source_merge_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(tableName)\n",
    "    print(f\"Table {tableName} creation Ended\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "e3e0ae4e-208c-4a38-bf39-9671d2e2e0e6",
    "default_lakehouse_name": "FCA",
    "default_lakehouse_workspace_id": "b5e66845-fd73-4d5d-b28c-da7dbc2d17ec",
    "known_lakehouses": [
     {
      "id": "e3e0ae4e-208c-4a38-bf39-9671d2e2e0e6"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
