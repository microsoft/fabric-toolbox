{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f8607-f28f-495b-bf22-189c501245f8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T22:09:56.515426Z",
       "execution_start_time": "2025-10-23T22:09:56.1465729Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d6250bdb-2662-4ab4-8637-0e948cb0493f",
       "queued_time": "2025-10-23T22:09:56.145347Z",
       "session_id": "7a41f39c-5db6-4d40-b74d-84e74bd6c6b9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 7a41f39c-5db6-4d40-b74d-84e74bd6c6b9, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "from notebookutils import mssparkutils\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, when, from_json, date_format, lit, row_number,max, lower, to_date, regexp_replace, coalesce\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import os\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\") # needed for automatic schema evolution in merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6592d0-c5e1-403f-8c61-6f2fcf74e2a8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T22:09:56.8176565Z",
       "execution_start_time": "2025-10-23T22:09:56.5173696Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "fb801230-d4d9-48c3-b36a-c99ed038e413",
       "queued_time": "2025-10-23T22:09:56.2255077Z",
       "session_id": "7a41f39c-5db6-4d40-b74d-84e74bd6c6b9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 7a41f39c-5db6-4d40-b74d-84e74bd6c6b9, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fromMonth = -2 #-1, -2,... from datenow -1 day\n",
    "toMonth = -1 #-1, -2,... from datenow -1 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b5c39e-976c-4518-b242-1ba0c8852b87",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T22:09:57.1750431Z",
       "execution_start_time": "2025-10-23T22:09:56.8196786Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b7ddb054-89b0-431c-9305-a6b3001d6fda",
       "queued_time": "2025-10-23T22:09:56.4313037Z",
       "session_id": "7a41f39c-5db6-4d40-b74d-84e74bd6c6b9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, 7a41f39c-5db6-4d40-b74d-84e74bd6c6b9, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rawSourcePath = \"Files/reservation-transactions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41ada1-ad51-4b9e-85f5-d2b8e94ca50b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T22:09:57.5319709Z",
       "execution_start_time": "2025-10-23T22:09:57.1771078Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "3bdb85f6-7aa0-4124-99b7-751f868d43be",
       "queued_time": "2025-10-23T22:09:56.6190718Z",
       "session_id": "7a41f39c-5db6-4d40-b74d-84e74bd6c6b9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, 7a41f39c-5db6-4d40-b74d-84e74bd6c6b9, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_first_csv_file(path):\n",
    "    \"\"\"\n",
    "    Recursively search for the first .csv file in the given directory.\n",
    "    Args:\n",
    "        path (str): The root directory to start the search.\n",
    "    Returns:\n",
    "        str or None: The full path to the first .parquet file found, or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for entry in mssparkutils.fs.ls(path):\n",
    "            if entry.isFile and entry.name.endswith(\".csv\"):\n",
    "                return entry.path\n",
    "            elif entry.isDir:\n",
    "                result = find_first_csv_file(entry.path)\n",
    "                if result:\n",
    "                    return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def generate_wildcard_path(full_path: str, raw_source_path: str, snapshot_folder: str) -> str:\n",
    "    # Find the index where the raw source path starts\n",
    "    idx = full_path.find(raw_source_path)\n",
    "    if idx == -1:\n",
    "        raise ValueError(\"rawSourcePath not found in full path\")\n",
    "\n",
    "    # Extract the base URI before the raw source path\n",
    "    base_uri = full_path[:idx]\n",
    "    detailPath = structurePath[idx+len(rawSourcePath):]\n",
    "\n",
    "    startleveltoAdd = detailPath.count('/') - '/date-date/Guid/name.csv'.count('/')\n",
    "\n",
    "    # Construct the wildcard path\n",
    "    wildcard_path = f\"{base_uri}{raw_source_path}{'/*' * startleveltoAdd}/{snapshot_folder}/*/*.csv\"\n",
    "    return wildcard_path\n",
    "\n",
    "def generateArrayOFPeriod(from_Month: int, to_month: int):\n",
    "    # Get today's date\n",
    "    today = datetime.today()\n",
    "\n",
    "    # Subtract one day\n",
    "    yesterday = today - timedelta(days=1)\n",
    "    first_day = yesterday.replace(day=1)\n",
    "\n",
    "    periodToLoad = []\n",
    "    if from_Month == to_month:\n",
    "        periodDate = first_day + relativedelta(months=to_month)\n",
    "        periodToLoad.append(periodDate.date())\n",
    "    else:\n",
    "        for i in range(from_Month, to_month+1):\n",
    "            periodDate = first_day + relativedelta(months=i)\n",
    "            periodToLoad.append(periodDate.date())\n",
    "\n",
    "    return periodToLoad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc9f2a-905b-4f8c-9cf1-58d71df2111c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Step1: Load Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9287cb91-7eb3-4c5d-9cdc-6eafbb7d1bd1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T22:10:01.0409167Z",
       "execution_start_time": "2025-10-23T22:09:57.5341787Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f19bcae5-7de5-4157-8086-a6bfe5ce380e",
       "queued_time": "2025-10-23T22:09:56.8743132Z",
       "session_id": "7a41f39c-5db6-4d40-b74d-84e74bd6c6b9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, 7a41f39c-5db6-4d40-b74d-84e74bd6c6b9, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "structurePath = find_first_csv_file(rawSourcePath)\n",
    "periodsToLoad = generateArrayOFPeriod(fromMonth, toMonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f66a5c-a69d-46b7-a0f4-416acded1b99",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T22:10:47.18542Z",
       "execution_start_time": "2025-10-23T22:10:01.0431863Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "fcbc67ff-6de1-48a4-b424-8dfac3c992e2",
       "queued_time": "2025-10-23T22:09:57.0494134Z",
       "session_id": "7a41f39c-5db6-4d40-b74d-84e74bd6c6b9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, 7a41f39c-5db6-4d40-b74d-84e74bd6c6b9, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Period : 2025-08-01\n",
      "Used path to load data: abfss://57bceddc-a995-44a7-bfb5-1d5f11ad1e98@onelake.dfs.fabric.microsoft.com/897aa7f9-9a59-4b8a-ad72-551118dc1109/Files/reservation-transactions/20250801-20250831/*.csv\n",
      "Table exists, snapshot will be clean.\n",
      "Data loaded in silver.\n",
      "End Period : 2025-08-01\n",
      "Start Period : 2025-09-01\n",
      "Used path to load data: abfss://57bceddc-a995-44a7-bfb5-1d5f11ad1e98@onelake.dfs.fabric.microsoft.com/897aa7f9-9a59-4b8a-ad72-551118dc1109/Files/reservation-transactions/20250901-20250930/*.csv\n",
      "Path not found: abfss://57bceddc-a995-44a7-bfb5-1d5f11ad1e98@onelake.dfs.fabric.microsoft.com/897aa7f9-9a59-4b8a-ad72-551118dc1109/Files/reservation-transactions/20250901-20250930/*.csv\n",
      "End Period : 2025-09-01\n"
     ]
    }
   ],
   "source": [
    "for per in periodsToLoad:\n",
    "    print(\"Start Period : \" + per.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    #drop staging table if exists\n",
    "    spark.sql(\"DROP TABLE IF EXISTS dim_reservations_staging\")\n",
    "\n",
    "    #generate storage path date part\n",
    "    fromFormatedDate = per.strftime(\"%Y%m%d\")\n",
    "    toFormatedDate = (per + relativedelta(months=1) + relativedelta(days=-1)).strftime(\"%Y%m%d\")\n",
    "    snapshot_folder = fromFormatedDate + \"-\" + toFormatedDate\n",
    "\n",
    "    wildcard = generate_wildcard_path(structurePath, rawSourcePath, snapshot_folder)\n",
    "    print(\"Used path to load data: \" + wildcard)\n",
    "\n",
    "    try:\n",
    "        df = spark.read.csv(wildcard, header=True, inferSchema=True)\n",
    "        df= df.withColumn(\"PeriodLoaded\",to_date(date_format(\"EventDate\", \"yyyy-MM-01\")) )\n",
    "        df.write.format('delta').saveAsTable(\"dim_reservations_staging\")\n",
    "\n",
    "        #identify period loaded\n",
    "        df = spark.sql(\"SELECT PeriodLoaded FROM dim_reservations_staging LIMIT 1\")\n",
    "        value = df.first()['PeriodLoaded']\n",
    "\n",
    "        #clean existing data in silver\n",
    "        if spark.catalog.tableExists('dim_reservations_silver'):\n",
    "            print(\"Table exists, snapshot will be clean.\")\n",
    "            spark.sql(f\"DELETE FROM dim_reservations_silver WHERE PeriodLoaded = '{value}'\")\n",
    "\n",
    "        #Load data in silver\n",
    "        dim_reservations_staging_df = DeltaTable.forPath(spark,\"Tables/dim_reservations_staging\").toDF()\n",
    "        dim_reservations_staging_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"dim_reservations_silver\")\n",
    "        print(\"Data loaded in silver.\")\n",
    "\n",
    "    except AnalysisException as e:\n",
    "        if \"PATH_NOT_FOUND\" in str(e):\n",
    "            print(f\"Path not found: {wildcard}\")\n",
    "        else:\n",
    "            raise # re-raise if it's a different AnalysisException\n",
    "\n",
    "    print(\"End Period : \" + per.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074a9edd-518e-43f3-9d99-c58b61795204",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Step2: Load to Gold Dim Reservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8197a2e6-7a06-4a9e-8079-afa356f1a51d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T22:10:50.2030453Z",
       "execution_start_time": "2025-10-23T22:10:47.1877979Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6d2c0954-03b5-40d3-8738-9c4d8ad6b7ce",
       "queued_time": "2025-10-23T22:09:57.3252837Z",
       "session_id": "7a41f39c-5db6-4d40-b74d-84e74bd6c6b9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": [
       "StatementMeta(, 7a41f39c-5db6-4d40-b74d-84e74bd6c6b9, 15, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_df = DeltaTable.forPath(spark,\"Tables/dim_reservations_silver\").toDF()\n",
    "source_df = source_df.where(\"lower(ArmSkuName) = 'fabric_capacity_cu_hour'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1649ea-bccf-4b8c-860f-cb7839dbcf71",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-23T22:11:00.1975652Z",
       "execution_start_time": "2025-10-23T22:10:50.2051679Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "8fa584e4-12f0-46e0-bb67-5d6d1d8cb5b2",
       "queued_time": "2025-10-23T22:09:57.6309297Z",
       "session_id": "7a41f39c-5db6-4d40-b74d-84e74bd6c6b9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 16,
       "statement_ids": [
        16
       ]
      },
      "text/plain": [
       "StatementMeta(, 7a41f39c-5db6-4d40-b74d-84e74bd6c6b9, 16, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table reservations creation started\n",
      "Table reservations creation Ended\n"
     ]
    }
   ],
   "source": [
    "tableName = \"reservations\"\n",
    "logicalKeyColumn = \"ReservationOrderId\"\n",
    "technicalKeyColumn = \"ReservationOrderKey\"\n",
    "tableAlreadyExists = spark.catalog.tableExists(tableName)\n",
    "\n",
    "\n",
    "source_merge_df = source_df\\\n",
    "                 .select(\"Amount\",\"BillingFrequency\",\"Currency\",\"Description\",\"EventDate\",\"PurchasingSubscriptionGuid\",\"PurchasingSubscriptionName\",\"Quantity\",\"Region\",\"ReservationOrderId\",\"ReservationOrderName\")\\\n",
    "                 .distinct()\n",
    "\n",
    "if tableAlreadyExists:\n",
    "    #Merge to table\n",
    "\n",
    "    print(f\"Merge Data for {tableName} table Started\")\n",
    "\n",
    "    target_table = DeltaTable.forPath(spark, f\"Tables/{tableName}\")\n",
    "    target_df = target_table.toDF()\n",
    "    target_df = target_df.select(logicalKeyColumn,technicalKeyColumn)\n",
    "\n",
    "\n",
    "    max_key = target_df.agg(coalesce(max(technicalKeyColumn),lit(0))).collect()[0][0]\n",
    "\n",
    "    combined_df = source_merge_df.join(target_df,logicalKeyColumn,\"leftouter\")\n",
    "    existingRows_df = combined_df.where(combined_df[technicalKeyColumn].isNotNull())\n",
    "    newRows_df = combined_df.where(combined_df[technicalKeyColumn].isNull())\n",
    "    window_spec = Window.orderBy(logicalKeyColumn)\n",
    "    newRows_df = newRows_df.withColumn(technicalKeyColumn, row_number().over(window_spec) + max_key )\n",
    "\n",
    "    Src_Merge_df = existingRows_df.union(newRows_df)\n",
    "\n",
    "\n",
    "    merge = (target_table.alias(\"target\")\n",
    "        .merge(\n",
    "            Src_Merge_df.alias(\"source\"),\n",
    "            f\"target.{technicalKeyColumn} = source.{technicalKeyColumn}\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        )\n",
    "    merge.execute()\n",
    "\n",
    "    print(f\"Merge Data for {tableName} Ended\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(f\"Table {tableName} creation started\")\n",
    "    window_spec = Window.orderBy(logicalKeyColumn)\n",
    "    source_merge_df = source_merge_df.withColumn(technicalKeyColumn, row_number().over(window_spec))\n",
    "    source_merge_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(tableName)\n",
    "    print(f\"Table {tableName} creation Ended\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "897aa7f9-9a59-4b8a-ad72-551118dc1109",
    "default_lakehouse_name": "FCA",
    "default_lakehouse_workspace_id": "57bceddc-a995-44a7-bfb5-1d5f11ad1e98",
    "known_lakehouses": [
     {
      "id": "897aa7f9-9a59-4b8a-ad72-551118dc1109"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
