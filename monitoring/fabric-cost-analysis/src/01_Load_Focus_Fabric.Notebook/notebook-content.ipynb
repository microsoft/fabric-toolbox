{"cells":[{"cell_type":"code","execution_count":null,"id":"ca1690e2-4c4e-4eb4-ab99-c303119f9c6a","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["from delta.tables import *\n","#from notebookutils import mssparkutils\n","from datetime import datetime, timedelta\n","from dateutil.relativedelta import relativedelta\n","#from pyspark.sql.utils import AnalysisException\n","from pyspark.sql.functions import col, when, from_json, date_format, lit, row_number,max, lower\n","from pyspark.sql.types import StructType, StringType\n","from pyspark.sql.window import Window"]},{"cell_type":"code","execution_count":null,"id":"e55c40ac-00e9-42f9-8c10-502e81702d51","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"outputs":[],"source":["fromMonth = 0 #-1, -2,... from datenow -1 day\n","toMonth = 0 #-1, -2,... from datenow -1 day"]},{"cell_type":"markdown","id":"6b716fa9-6b0f-4159-bcea-e22256a8052d","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## FUNCTIONS"]},{"cell_type":"code","execution_count":null,"id":"eb8b57c4-a4eb-48b5-baa5-861a8444c023","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["def generateArrayOFPeriod(from_Month: int, to_month: int):\n","    # Get today's date\n","    today = datetime.today()\n","\n","    # Subtract one day\n","    yesterday = today - timedelta(days=1)\n","    first_day = yesterday.replace(day=1)\n","\n","    periodToLoad = []\n","    if from_Month == to_month:\n","        periodDate = first_day + relativedelta(months=to_month)\n","        periodToLoad.append(periodDate.date())\n","    else:\n","        for i in range(from_Month, to_month+1):\n","            periodDate = first_day + relativedelta(months=i)\n","            periodToLoad.append(periodDate.date())\n","\n","    return periodToLoad\n","\n","def AddBillingTypeColumn(dfsource):\n","    # Define the schema for the JSON structure. In this version, only for Fabric billingtype\n","    schema = StructType().add(\"BillingType\", StringType())\n","\n","    df_parsed = dfsource.withColumn(\"parsed_json\", from_json(col(\"x_SkuDetails\"), schema))\n","\n","    # Create the new column based on the condition\n","    df_transformed = df_parsed.withColumn(\"BillingType\", col(\"parsed_json.BillingType\"))\n","\n","    # Optionally drop the intermediate parsed column\n","    df_final = df_transformed.drop(\"parsed_json\")\n","\n","    return df_final\n","\n"]},{"cell_type":"code","execution_count":null,"id":"8ae9886d-ddc1-4555-a762-add719fc4130","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["#Schema Evolution Function\n","def updateExistingPastData():\n","    needToOverwrite = False\n","\n","    focus_fabric_df = DeltaTable.forPath(spark,\"Tables/focus_fabric\").toDF()\n","    if not 'SubscriptionKey' in focus_fabric_df.columns:\n","        print(\"Missing SubscriptionKey ---> Adjustment needed\")\n","        subscription_df = DeltaTable.forPath(spark,\"Tables/subscriptions\").toDF()\n","        subscription_df = subscription_df.select(\"SubAccountId\",\"SubscriptionKey\")\n","        # Add a new column\n","        focus_fabric_df = focus_fabric_df.join(subscription_df,\"SubAccountId\", \"leftouter\").drop(\"SubAccountId\",\"SubAccountName\",\"SubAccountType\") #retrieve SubscriptionKey\n","        needToOverwrite = True\n","\n","    if not 'ResourceKey' in focus_fabric_df.columns:\n","        print(\"Missing ResourceKey ---> Adjustment needed\")\n","        resources_df = DeltaTable.forPath(spark,\"Tables/resources\").toDF()\n","        resources_df = resources_df.select(\"ResourceId\",\"ResourceKey\")\n","        # Add a new column\n","        focus_fabric_df = focus_fabric_df.join(resources_df,\"ResourceId\", \"leftouter\").drop('ResourceId','ResourceName','ResourceType','ServiceCategory','ServiceName','RegionId') #retrieve ResourceKey\n","        needToOverwrite = True\n","\n","    if 'RegionName' in focus_fabric_df.columns:\n","        print(\"RegionName still exists ---> Adjustment needed\")\n","        focus_fabric_df = focus_fabric_df.drop(\"RegionName\")\n","        needToOverwrite = True\n","\n","    if 'x_ResourceGroupName' in focus_fabric_df.columns:\n","        print(\"x_ResourceGroupName still exists ---> Adjustment needed\")\n","        focus_fabric_df = focus_fabric_df.drop(\"x_ResourceGroupName\")\n","        needToOverwrite = True\n","\n","    if needToOverwrite:\n","        print(\"Overwrite the focus_fabric table\")\n","        focus_fabric_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"focus_fabric\")\n","        print(\"Overwrite done for the focus_fabric table\")\n"]},{"cell_type":"markdown","id":"1841f3f2-1ac9-4aee-bc4f-e4d9095b6b99","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## STEP 2 Load Gold:\n","\n","- Filter on Fabric data only\n","- Remove x_ column\n","- Add FabricPause column"]},{"cell_type":"code","execution_count":null,"id":"81b24d57-58a2-4bb6-a1cf-0c4f28e6a7df","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["periodsToLoad = generateArrayOFPeriod(fromMonth, toMonth)"]},{"cell_type":"code","execution_count":null,"id":"65e36181-2317-4838-9c95-5ab2ba4761fc","metadata":{"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["# Preparation of predicate for all ingested periods\n","date_condition = \", \".join([f\"'{date.strftime('%Y-%m-%d')} 00:00:00'\" for date in periodsToLoad])\n","\n","print(\"Start loading Period(s) in \" + date_condition)\n","\n","#clean existing data in silver\n","if spark.catalog.tableExists(\"focus_fabric\"):\n","    print(\"Table exists, snapshot will be clean.\")\n","    Delete_sql_query = f\"\"\"DELETE FROM focus_fabric WHERE BillingPeriodStart IN ({date_condition})\"\"\"\n","    spark.sql(Delete_sql_query)\n","    print(\"Clean performed\")\n","    updateExistingPastData()\n","\n","x_columns_to_keep = ['x_SkuMeterName','x_SkuMeterSubcategory']\n","compute_columns_to_keep = [\"BillingType\",\"ChargePeriodStart_DateKey\",\"MeterKey\",\"CommitmentSavings\",\"SubscriptionKey\",\"ResourceKey\"]\n","columns_replace_per_dimensions = ['SubAccountId','SubAccountName','SubAccountType','RegionName','ResourceId','ResourceName','ResourceType','ServiceCategory','ServiceName','RegionId']\n","\n","focus_df = DeltaTable.forPath(spark,\"Tables/focus\").toDF()\n","Meters_df = DeltaTable.forPath(spark,\"Tables/meters\").toDF().select(\"Name_Lower\",\"MeterKey\")\n","max_key = Meters_df.agg(max(\"MeterKey\")).collect()[0][0]\n","\n","# Get all column names that do NOT start with 'x_'\n","columns_to_keep = [col for col in focus_df.columns if not col.startswith(\"x_\") and col not in columns_replace_per_dimensions]\n","columns_to_keep = columns_to_keep + x_columns_to_keep + compute_columns_to_keep\n","\n","focus_df = focus_df.where(f\"\"\"ServiceName = 'Microsoft.Fabric' and BillingPeriodStart IN ({date_condition})\"\"\")\n","focus_df = AddBillingTypeColumn(focus_df)\n","\n","#identify missing Meters in the referencial\n","missing_Meters_df = focus_df.select(\"x_SkuMeterName\",\"ChargeDescription\",\"BillingType\") \\\n","                            .withColumn(\"Name_Lower\",lower(\"x_SkuMeterName\")) \\\n","                            .drop(\"x_SkuMeterName\") \\\n","                            .dropDuplicates([\"Name_Lower\"]) \\\n","                            .withColumn(\"Category\",when(col(\"ChargeDescription\") == \"Fabric Cap\",\"Fabric CU\").otherwise(col(\"ChargeDescription\"))) \\\n","                            .drop(\"ChargeDescription\") \\\n","                            .withColumn(\"State\",lit(\"Unknow\")) \\\n","                            .withColumn(\"Main\",lit(\"FALSE\"))  \\\n","                            .withColumn(\"Included\",when(col(\"BillingType\").isNotNull() & (col(\"BillingType\") != \"Capacity Pause/Delete Surcharge\"),lit(\"TRUE\")).otherwise(lit(\"FALSE\"))) \\\n","                            .drop(\"BillingType\")\n","\n","missing_Meters_df = missing_Meters_df.join(Meters_df,\"Name_Lower\",\"leftouter\")\n","missing_Meters_df = missing_Meters_df.where(missing_Meters_df.MeterKey.isNull())\n","window_spec = Window.orderBy(\"Name_Lower\")\n","missing_Meters_df = missing_Meters_df.withColumn(\"MeterKey\", row_number().over(window_spec) + max_key)\n","\n","missing_Meters_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"Meters\")\n","\n","\n","Meters_df = DeltaTable.forPath(spark,\"Tables/meters\").toDF()\n","Meters_df = Meters_df.select(\"Name_Lower\",\"MeterKey\")\n","focus_df = focus_df.withColumn(\"Name_Lower\", lower(\"x_SkuMeterName\")).join(Meters_df,\"Name_Lower\", \"leftouter\").drop(\"Name_Lower\") #retrieve MeterKey\n","focus_df = focus_df.withColumn(\"ChargePeriodStart_DateKey\", date_format(\"ChargePeriodStart\", \"yyyyMMdd\").cast(\"int\"))\n","\n","subscription_df = DeltaTable.forPath(spark,\"Tables/subscriptions\").toDF()\n","subscription_df = subscription_df.select(\"SubAccountId\",\"SubscriptionKey\")\n","focus_df = focus_df.join(subscription_df,\"SubAccountId\", \"leftouter\") #retrieve SubscriptionKey\n","\n","resources_df = DeltaTable.forPath(spark,\"Tables/resources\").toDF()\n","resources_df = resources_df.select(\"ResourceId\",\"ResourceKey\")\n","focus_df = focus_df.join(resources_df,\"ResourceId\", \"leftouter\") #retrieve SubscriptionKey\n","\n","focus_df = focus_df.withColumn(\"CommitmentSavings\", col(\"ContractedCost\") - col(\"EffectiveCost\"))\n","focus_df = focus_df.select(columns_to_keep)\n","\n","focus_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"focus_fabric\")\n","\n","print(\"End Loading\")\n"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"897aa7f9-9a59-4b8a-ad72-551118dc1109","default_lakehouse_name":"FCA","default_lakehouse_workspace_id":"57bceddc-a995-44a7-bfb5-1d5f11ad1e98","known_lakehouses":[{"id":"897aa7f9-9a59-4b8a-ad72-551118dc1109"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"state":{},"version":"0.1"}},"nbformat":4,"nbformat_minor":5}