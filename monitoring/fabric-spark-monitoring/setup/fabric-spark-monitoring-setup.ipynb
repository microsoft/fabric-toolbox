{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105114aa-44e8-475d-8749-6fb54f4b5f48",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install ms-fabric-cli==1.1.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540e0fc-525a-4370-a36a-650547da2eec",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Variables - UPDATE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ac3a0-4444-46fb-bc4a-7b1b87bfefd5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Vairables to Replace\n",
    "\n",
    "- Add the target Workspace in workspace_name. It will use the target Workspace or create it.\n",
    "- Add the Capacity name in capacity_name. It will be used only if the workspace doesn´t exist and for the capacity assignment to that workspace.\n",
    "- Add the target Eventhouse name in eventhouse_name. Can be an existing Eventhouse. If blank will use detault name.\n",
    "- Add the list of environments you want to update in a structure approach in environments. It should be an array of object with the following format {\"workspace_id\": \"<guid>\", \"environment_id\": \"<guid>\"}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9ff3e-254a-4ea4-b21a-f0cfbfcae6f9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "workspace_name = \"Fabric Spark Monitoring\"\n",
    "capacity_name= \"\"\n",
    "eventhouse_name = \"fabric-spark-monitoring\"\n",
    "\n",
    "environments = [\n",
    "    {\"workspace_id\": \"\", \"environment_id\": \"\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0de0d-570e-494a-afae-09307e4789a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Source Git Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269dcc88-e062-4f58-8388-0409f58261d0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "##### DO NET CHANGE UNLESS SPECIFIED OTHERWISE ####\n",
    "repo_owner = \"ecotte\" \n",
    "repo_name = \"fabric-toolbox\" \n",
    "branch = \"fabric-spark-monitoring\"\n",
    "folder_prefix = \"monitoring/fabric-spark-monitoring\" \n",
    "github_token = \"\"\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ddc1aa-758d-4bc5-a0ab-2208a779c900",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe5dc1-b895-47b5-a09d-6a6f72840033",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## ****Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf74ba2-0f23-4242-834a-43c143be866c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import sempy.fabric as fabric\n",
    "\n",
    "class FabDeployCLI:\n",
    "    src_workspace_id = \"\"\n",
    "    src_workspace_name = \"\"\n",
    "    trg_workspace_id = \"\"\n",
    "    trg_workspace_name = \"\"\n",
    "    deployment_order = []\n",
    "    mapping_table =  []\n",
    "    workspace_name = \"\"\n",
    "    capacity_name = \"\"\n",
    "    eventhouse_name = \"\"\n",
    "    repo_owner = \"\"\n",
    "    repo_name = \"\"\n",
    "    branch = \"\"\n",
    "    folder_prefix = \"\"\n",
    "    github_token = \"\"\n",
    "    pipeline_parameters = {}\n",
    "\n",
    "    def __download_folder_as_zip(self, repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"src\",  remove_folder_prefix = \"\", github_token = None):\n",
    "        # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "        url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "        headers = None\n",
    "\n",
    "        if github_token is not None and github_token != \"\":\n",
    "        # Replace with your actual GitHub token\n",
    "            headers = {\n",
    "                \"Authorization\": f\"token {github_token}\",\n",
    "                \"Accept\": \"application/vnd.github.v3+json\"\n",
    "            }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        folder_to_extract = f\"/{folder_to_extract}\" if folder_to_extract[0] != \"/\" else folder_to_extract\n",
    "        \n",
    "        # Ensure the directory for the output zip file exists\n",
    "        os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "        \n",
    "        # Create a zip file in memory\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "            with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "                for file_info in zipf.infolist():\n",
    "                    parts = file_info.filename.split('/')\n",
    "                    if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n",
    "                        # Extract only the specified folder\n",
    "                        file_data = zipf.read(file_info.filename)  \n",
    "                        if folder_prefix != \"\":\n",
    "                            for remove_folder_prefix_folder in remove_folder_prefix.split('/'):\n",
    "                                parts.remove(remove_folder_prefix_folder)\n",
    "                        output_zipf.writestr(('/'.join(parts[1:])), file_data)\n",
    "\n",
    "    def __uncompress_zip_to_folder(self, zip_path, extract_to):\n",
    "        # Ensure the directory for extraction exists\n",
    "        os.makedirs(extract_to, exist_ok=True)\n",
    "        \n",
    "        # Uncompress all files from the zip into the specified folder\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        \n",
    "        # Delete the original zip file\n",
    "        os.remove(zip_path)\n",
    "\n",
    "    def __run_fab_command(self, command, capture_output: bool = False, silently_continue: bool = False, raw_output: bool = False):\n",
    "        result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "        if (not(silently_continue) and (result.returncode > 0 or result.stderr)):\n",
    "            raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result}'\")    \n",
    "        if (capture_output and not raw_output): \n",
    "            output = result.stdout.strip()\n",
    "            return output\n",
    "        elif (capture_output and raw_output):\n",
    "            return result\n",
    "\n",
    "    def __fab_get_workspace_id(self, name):\n",
    "        result = self.__run_fab_command(f\"get /{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "        return result\n",
    "\n",
    "    def __fab_workspace_exists(self, name):\n",
    "        id = self.__run_fab_command(f\"get /{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "        return(id)\n",
    "\n",
    "    def __fab_get_id(self, name):\n",
    "        id = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "        return(id)\n",
    "\n",
    "    def __fab_get_item(self, name):\n",
    "        item = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name}\" , capture_output = True, silently_continue= True)\n",
    "        return(item)\n",
    "\n",
    "    def fab_get_eventstream_connection_string(self, name, connection_name):\n",
    "        connection_id = \"\"\n",
    "        item_id = self.__fab_get_id(name)\n",
    "\n",
    "        item = self.__run_fab_command(f\"api -X get /workspaces/{self.trg_workspace_id}/eventstreams/{item_id}/topology\" , capture_output = True, silently_continue= True)\n",
    "        topology = json.loads(item)\n",
    "\n",
    "        sources = topology.get(\"text\",{}).get(\"sources\",[])\n",
    "        source_id = list(filter(lambda source: source[\"name\"] == connection_name, sources))\n",
    "        if len(source_id):\n",
    "            connection_id = source_id[0].get(\"id\")\n",
    "\n",
    "        destinations = topology.get(\"text\",{}).get(\"destinations\",[])\n",
    "        destination_id = list(filter(lambda destination: destination[\"name\"] == connection_name, destinations))\n",
    "        if len(destination_id):\n",
    "            connection_id = destination_id[0].get(\"id\")\n",
    "\n",
    "        connection = self.__run_fab_command(f\"api -X get /workspaces/{self.trg_workspace_id}/eventstreams/{item_id}/sources/{connection_id}/connection\" , capture_output = True, silently_continue= True)\n",
    "        connection = json.loads(connection)\n",
    "        connection = connection.get(\"text\",{}).get(\"accessKeys\",{}).get(\"primaryConnectionString\")\n",
    "        return(connection)\n",
    "\n",
    "    def __fab_get_display_name(self, name):\n",
    "        display_name = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name} -q displayName\" , capture_output = True, silently_continue= True)\n",
    "        return(display_name)\n",
    "\n",
    "    def __fab_get_kusto_query_uri(self, name):\n",
    "        connection = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name} -q properties.queryServiceUri -f\", capture_output = True, silently_continue= True)\n",
    "        return(connection)\n",
    "\n",
    "    def __fab_get_kusto_ingest_uri(self, name):\n",
    "        connection = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name} -q properties.ingestionServiceUri -f\", capture_output = True, silently_continue= True)\n",
    "        return(connection)\n",
    "\n",
    "    def __fab_get_folders(self):\n",
    "        response = self.__run_fab_command(f\"api workspaces/{self.trg_workspace_id}/folders\", capture_output = True, silently_continue= True)\n",
    "        return(json.loads(response).get('text',{}).get('value',[]))\n",
    "\n",
    "    def __fab_add_schedule(self, name):\n",
    "        item = self.__run_fab_command(f\"get /{self.trg_workspace_name}/{name} -q schedules\" , capture_output = True, silently_continue= True)\n",
    "\n",
    "        if len(json.loads(item)) == 0:\n",
    "            schedule = self.__get_schedule_by_name(name)\n",
    "\n",
    "            return self.__run_fab_command(f\"job run-sch /{self.trg_workspace_name}/{name} -i {json.dumps(schedule)}\" , capture_output = True, silently_continue=True)\n",
    "\n",
    "        return f\"\"\"Job schedule for '{name}' already exists...\n",
    "    * Job schedule {item}\"\"\" \n",
    "\n",
    "    def __get_id_by_name(self, name):\n",
    "        for it in self.deployment_order:\n",
    "            if it.get(\"name\") == name:\n",
    "                    return it.get(\"id\")\n",
    "        return None\n",
    "\n",
    "    def __get_schedule_by_name(self, name):\n",
    "        for it in self.deployment_order:\n",
    "            if it.get(\"name\") == name:\n",
    "                    return it.get(\"schedule\")\n",
    "        return None\n",
    "\n",
    "    def __copy_to_tmp(self, name,child=None,type=None):\n",
    "        child_path = \"\" if child is None else f\".children/{child}/\"\n",
    "        type_path = \"\" if type is None else f\"{type}/\"\n",
    "        shutil.rmtree(\"./builtin/tmp\",  ignore_errors=True)\n",
    "        path2zip = \"./builtin/src/src.zip\"\n",
    "        with  ZipFile(path2zip) as archive:\n",
    "            for file in archive.namelist():\n",
    "                if file.startswith(f'src/{type_path}{name}/{child_path}'):\n",
    "                    archive.extract(file, './builtin/tmp')\n",
    "        return(f\"./builtin/tmp/src/{type_path}{name}/{child_path}\" )\n",
    "\n",
    "    def __get_mapping_table_new_from_type(self, type):\n",
    "        result = \"\"\n",
    "        filtered_data = list(filter(lambda item: item['Type'] == type, self.mapping_table))\n",
    "        if len(filtered_data) > 0:\n",
    "            result=filtered_data[0][\"new\"]\n",
    "        return result\n",
    "\n",
    "    def __get_mapping_table_new_from_old(self, old):\n",
    "        result = \"\"\n",
    "        filtered_data = list(filter(lambda item: item['old'] == old, self.mapping_table))\n",
    "        if len(filtered_data) > 0:\n",
    "            result=filtered_data[0][\"new\"]\n",
    "        return result\n",
    "\n",
    "    def __get_mapping_table_new_from_type_item(self, type,item):\n",
    "        result = \"\"\n",
    "        filtered_data = list(filter(lambda table: table[\"Type\"] == type and table[\"Item\"] == item, self.mapping_table))\n",
    "        if len(filtered_data) > 0:\n",
    "            result=filtered_data[0][\"new\"]\n",
    "        return result\n",
    "\n",
    "    def __get_mapping_table_parent_type(self, type,item,parent_type):\n",
    "        parent_item = self.__get_mapping_table_new_from_type_item(type,item)\n",
    "        result = self.__get_mapping_table_new_from_type_item(parent_type,parent_item)\n",
    "        return result\n",
    "\n",
    "    def __replace_ids_in_folder(self, folder_path, mapping_table):\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file_name in files:\n",
    "                if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb', '.py', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "                    file_path = os.path.join(root, file_name)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        content = file.read()\n",
    "                        for mapping in mapping_table:  \n",
    "                            content = content.replace(mapping[\"old\"], mapping[\"new\"])\n",
    "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                        file.write(content)\n",
    "\n",
    "    def __replace_kqldb_parent_eventhouse(self, folder_path,parent_eventhouse):\n",
    "        property_file = f\"{folder_path}/DatabaseProperties.json\"\n",
    "        with open(property_file, 'r', encoding='utf-8') as file:\n",
    "            content = json.load(file)\n",
    "            content[\"parentEventhouseItemId\"] = self.__fab_get_id(parent_eventhouse)\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(content,file,indent=4)\n",
    "\n",
    "    def __replace_eventstream_destination(self, folder_path,it_destinations):\n",
    "        property_file = f\"{folder_path}/eventstream.json\"\n",
    "        with open(property_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = json.load(file)\n",
    "            destinations = content.get(\"destinations\",[])\n",
    "            for destination in destinations:\n",
    "                if destination.get(\"type\") != \"CustomEndpoint\":\n",
    "                    filtered_data = list(filter(lambda table: table[\"name\"] == destination.get(\"name\") and table[\"type\"] == destination.get(\"type\"), it_destinations))\n",
    "                    if len(filtered_data) > 0:        \n",
    "                        destination[\"properties\"][\"workspaceId\"] = self.__get_mapping_table_new_from_type_item(\"Workspace Id\",self.trg_workspace_name)\n",
    "                        destination[\"properties\"][\"itemId\"] = self.__get_mapping_table_new_from_type_item(\"KQL DB ID\",filtered_data[0].get(\"itemName\"))\n",
    "                        if destination.get(\"properties\",{}).get(\"databaseName\") is not None:\n",
    "                            destination[\"properties\"][\"databaseName\"] = self.__get_mapping_table_new_from_type_item(\"KQL DB Name\",filtered_data[0].get(\"itemName\"))\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(content,file,indent=4)\n",
    "\n",
    "    def __replace_kqldashboard_datasources(self, folder_path,it_datasources):\n",
    "        property_file = f\"{folder_path}/RealTimeDashboard.json\"\n",
    "        with open(property_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = json.load(file)\n",
    "            datasources = content.get(\"dataSources\",[])\n",
    "            for datasource in datasources:\n",
    "                filtered_data = list(filter(lambda table: table[\"name\"] == datasource.get(\"name\"), it_datasources))\n",
    "                if len(filtered_data) > 0:        \n",
    "                    datasource[\"workspace\"] = self.__get_mapping_table_new_from_type_item(\"Workspace Id\",self.trg_workspace_name)\n",
    "                    datasource[\"database\"] = self.__get_mapping_table_new_from_type_item(\"KQL DB ID\",filtered_data[0].get(\"itemName\"))\n",
    "                    datasource[\"clusterUri\"] = self.__get_mapping_table_parent_type(\"KQL DB Eventhouse\",filtered_data[0].get(\"itemName\"),\"Kusto Query Uri\")\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(content,file,indent=4)\n",
    "\n",
    "    def __replace_kqlqueryset_datasources(self, folder_path,it_datasources):\n",
    "        property_file = f\"{folder_path}/RealTimeQueryset.json\"\n",
    "        with open(property_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = json.load(file)\n",
    "            datasources = content.get(\"queryset\",{}).get(\"dataSources\",[])\n",
    "            for datasource in datasources:\n",
    "                filtered_data = list(filter(lambda table: str(table[\"itemName\"]).replace(\".KQLDatabase\",\"\") == datasource.get(\"databaseItemName\"), it_datasources))\n",
    "                if len(filtered_data) > 0:        \n",
    "                    datasource[\"databaseItemId\"] = self.__get_mapping_table_new_from_type_item(\"KQL DB ID\",filtered_data[0].get(\"itemName\"))\n",
    "                    datasource[\"clusterUri\"] = self.__get_mapping_table_parent_type(\"KQL DB Eventhouse\",filtered_data[0].get(\"itemName\"),\"Kusto Query Uri\")\n",
    "                    print(content.get(\"queryset\",{}).get(\"dataSources\",[]))\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(content,file,indent=4)\n",
    "\n",
    "    def __replace_pbi_report_definition(self, folder_path,datasource):\n",
    "        property_file = f\"{folder_path}/definition.pbir\"\n",
    "        sm_name = datasource.replace(\".SemanticModel\",\"\")\n",
    "        ws_id = self.__get_mapping_table_new_from_type(\"Workspace Id\")\n",
    "        sm_id = self.__get_mapping_table_new_from_type_item(\"Semantic Model ID\",datasource)\n",
    "        pbir_definition = {\n",
    "            \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/item/report/definitionProperties/1.0.0/schema.json\",\n",
    "            \"version\": \"4.0\",\n",
    "            \"datasetReference\": {\n",
    "                \"byPath\": None,\n",
    "                \"byConnection\": {\n",
    "                \"connectionString\": f\"Data Source=powerbi://api.powerbi.com/v1.0/myorg/{ws_id};Initial Catalog={sm_name};Integrated Security=ClaimsToken\",\n",
    "                \"pbiServiceModelId\": None,\n",
    "                \"pbiModelVirtualServerName\": \"sobe_wowvirtualserver\",\n",
    "                \"pbiModelDatabaseName\": sm_id,\n",
    "                \"connectionType\": \"pbiServiceXmlaStyleLive\",\n",
    "                \"name\": \"EntityDataSource\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(pbir_definition,file,indent=4)\n",
    "\n",
    "    def __replace_pipeline_parameter(self, folder_path, it_parameters):\n",
    "        property_file = f\"{folder_path}/pipeline-content.json\"\n",
    "        with open(property_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = json.load(file)\n",
    "            properties = content.get(\"properties\",{}).get(\"parameters\",{})\n",
    "            for parameter in it_parameters:\n",
    "                if parameter[\"type\"] == \"kusto_query_uri\":\n",
    "                    pipeline_parameter = properties.get(parameter[\"name\"],{})\n",
    "                    pipeline_parameter[\"defaultValue\"] = self.__get_mapping_table_new_from_type_item(\"Kusto Query Uri\",parameter[\"source\"] if self.eventhouse_name == \"\" or self.eventhouse_name is None else f\"{self.eventhouse_name}.Eventhouse\")\n",
    "                elif parameter[\"type\"] == \"kusto_database\":\n",
    "                    pipeline_parameter = properties.get(parameter[\"name\"],{})\n",
    "                    pipeline_parameter[\"defaultValue\"] = str(parameter[\"source\"]).replace(\".KQLDatabase\",\"\")\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(content,file,indent=4)    \n",
    "\n",
    "    def __replace_pipeline_activities(self, folder_path, it_acitivities):\n",
    "        property_file = f\"{folder_path}/pipeline-content.json\"\n",
    "        with open(property_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = json.load(file)\n",
    "            activities = content.get(\"properties\",{}).get(\"activities\",[])\n",
    "            for activity in activities:\n",
    "                if activity[\"type\"] == \"TridentNotebook\":\n",
    "                    filtered_data = list(filter(lambda act: act[\"name\"] == activity.get(\"name\"), it_acitivities))\n",
    "                    activity[\"typeProperties\"][\"workspaceId\"] = self.__get_mapping_table_new_from_type_item(\"Workspace Id\",self.trg_workspace_name)\n",
    "                    activity[\"typeProperties\"][\"notebookId\"] = self.__get_mapping_table_new_from_type_item(\"Notebook ID\",filtered_data[0].get(\"itemName\"))\n",
    "        with open(property_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(content,file,indent=4)       \n",
    "\n",
    "    def __deploy_item(self, name,child=None,it=None):\n",
    "        parent = \"\"\n",
    "        cli_parameter = \"\"\n",
    "\n",
    "        # Copy and replace IDs in the item\n",
    "        tmp_path = self.__copy_to_tmp(name,child,it.get(\"type\"))\n",
    "        \n",
    "        if child is not None:\n",
    "            parent = name\n",
    "            name = child     \n",
    "\n",
    "        if \".KQLDatabase\" in name:\n",
    "            if child is not None:\n",
    "                parent = parent if self.eventhouse_name == \"\" or self.eventhouse_name is None else f\"{self.eventhouse_name}.Eventhouse\"\n",
    "            if it[\"parent\"] is not None:\n",
    "                parent = it[\"parent\"] if self.eventhouse_name == \"\" or self.eventhouse_name is None else f\"{self.eventhouse_name}.Eventhouse\"\n",
    "            self.mapping_table.append({\"Type\": \"KQL DB Eventhouse\", \"Item\": name, \"old\": it[\"parent\"], \"new\": parent })  \n",
    "            self.__replace_kqldb_parent_eventhouse(tmp_path,parent)\n",
    "        elif \".Eventhouse\" in name:\n",
    "            name = name if self.eventhouse_name == \"\" or self.eventhouse_name is None else f\"{self.eventhouse_name}.Eventhouse\"\n",
    "        elif \".Eventstream\" in name:\n",
    "            self.__replace_eventstream_destination(tmp_path,it[\"destinations\"]) \n",
    "        elif \".Notebook\" in name:\n",
    "            cli_parameter = cli_parameter + \" --format .py\"\n",
    "            self.__replace_ids_in_folder(tmp_path, self.mapping_table)  \n",
    "        elif \".DataPipeline\" in name: \n",
    "            self.__replace_pipeline_parameter(tmp_path,it[\"parameters\"])\n",
    "            self.__replace_pipeline_activities(tmp_path,it[\"acitivities\"])\n",
    "        elif \".SemanticModel\" in name:\n",
    "            self.__replace_ids_in_folder(tmp_path, self.mapping_table)\n",
    "        elif \".KQLDashboard\" in name:\n",
    "            self.__replace_kqldashboard_datasources(tmp_path, it[\"datasources\"])\n",
    "        elif \".KQLQueryset\" in name:\n",
    "            self.__replace_kqlqueryset_datasources(tmp_path, it[\"datasources\"])\n",
    "        elif \".Report\" in name:\n",
    "            self.__replace_pbi_report_definition(tmp_path,it[\"datasource\"])\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"#############################################\")\n",
    "        print(f\"Deploying {name}\")      \n",
    "        \n",
    "        self.__run_fab_command(f\"import  /{self.trg_workspace_name}/{name} -i {tmp_path} -f {cli_parameter} \", silently_continue= True)\n",
    "\n",
    "        new_id = self.__fab_get_id(name)\n",
    "\n",
    "        if \".KQLDatabase\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"KQL DB ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".Eventhouse\" in name:\n",
    "            query_uri = self.__fab_get_kusto_query_uri(name)\n",
    "            ingest_uri = self.__fab_get_kusto_ingest_uri(name)\n",
    "            self.mapping_table.append({\"Type\": \"Kusto Query Uri\", \"Item\": name, \"old\": it[\"kustoQueryUri\"], \"new\": query_uri })        \n",
    "            self.mapping_table.append({\"Type\": \"Kusto Ingest Uri\", \"Item\": name, \"old\": it[\"kustoIngestUri\"], \"new\": ingest_uri })\n",
    "            self.mapping_table.append({\"Type\": \"Eventhouse ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".Eventstream\" in name:\n",
    "            if it.get(\"ConnectionStringCustomEndpoint\") is not None:\n",
    "                self.mapping_table.append({\"Type\": \"Connection String Evenstream\", \"Item\": name, \"old\": it[\"ConnectionStringCustomEndpoint\"], \"new\": self.fab_get_eventstreamConnectionString(name,it[\"ConnectionStringCustomEndpoint\"]) })\n",
    "            self.mapping_table.append({\"Type\": \"Eventstream ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".Notebook\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"Notebook ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".DataPipeline\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"Pipeline ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".Report\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"Report ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".SemanticModel\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"Semantic Model ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        elif \".KQLDashboard\" in name:\n",
    "            self.mapping_table.append({\"Type\": \"KQLDashboard ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "\n",
    "    def __init__(self, repo_owner, repo_name, branch, folder_prefix, github_token):\n",
    "        \n",
    "        # Set environment parameters for Fabric CLI\n",
    "        token = notebookutils.credentials.getToken('pbi')\n",
    "        os.environ['FAB_TOKEN'] = token\n",
    "        os.environ['FAB_TOKEN_ONELAKE'] = token  \n",
    "\n",
    "        self.repo_owner = repo_owner\n",
    "        self.repo_name = repo_name\n",
    "        self.branch = branch\n",
    "        self.folder_prefix = folder_prefix\n",
    "        self.github_token = github_token   \n",
    "      \n",
    "\n",
    "    def run(self, workspace_name, capacity_name= \"\", eventhouse_name = \"\", exclude = [], type_exclude = [], pipeline_parameters = {}):\n",
    "\n",
    "        self.__download_folder_as_zip(self.repo_owner, self.repo_name, output_zip = \"./builtin/src/src.zip\", branch = self.branch, folder_to_extract= f\"{self.folder_prefix}/src\", remove_folder_prefix = f\"{self.folder_prefix}\", github_token=self.github_token)\n",
    "        self.__download_folder_as_zip(self.repo_owner, self.repo_name, output_zip = \"./builtin/config/config.zip\", branch = self.branch, folder_to_extract= f\"{self.folder_prefix}/config\" , remove_folder_prefix = f\"{self.folder_prefix}\", github_token=self.github_token)\n",
    "        self.__uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")\n",
    "\n",
    "        base_path = './builtin/'\n",
    "\n",
    "        self.eventhouse_name = eventhouse_name\n",
    "        self.pipeline_parameters = pipeline_parameters\n",
    "\n",
    "        deploy_order_path = os.path.join(base_path, 'config/deployment_order.json')\n",
    "        with open(deploy_order_path, 'r') as file:\n",
    "                self.deployment_order = json.load(file)\n",
    "\n",
    "        #deploy workspace idempotent\n",
    "        if \"NotFound\" in self.__fab_workspace_exists(f\"{workspace_name}.Workspace\"):\n",
    "            if capacity_name == \"\" or capacity_name is None:\n",
    "                raise \"Workspace doesn´t exist and capacity_name not provided\"\n",
    "            self.__run_fab_command(f\"mkdir {workspace_name}.Workspace -P capacityname={capacity_name}.Capacity\")\n",
    "            print(f\"New Workspace Create\")\n",
    "\n",
    "        self.src_workspace_name = \"Workspace.src\"\n",
    "        self.src_workspace_id = self.__get_id_by_name(self.src_workspace_name)\n",
    "\n",
    "        self.trg_workspace_id = self.__fab_get_workspace_id(f\"{workspace_name}.Workspace\")\n",
    "        self.trg_workspace_name = f\"{workspace_name}.Workspace\"\n",
    "\n",
    "        print(f\"Target Workspace Id: {self.trg_workspace_id}\")\n",
    "        print(f\"Target Workspace Name: {self.trg_workspace_name}\")\n",
    "\n",
    "        self.mapping_table.append({\"Type\": \"Workspace Id\", \"Item\": self.trg_workspace_name, \"old\": self.__get_id_by_name(self.src_workspace_name), \"new\": self.trg_workspace_id })\n",
    "        self.mapping_table.append({\"Type\": \"Workspace Blank Id\", \"Item\": self.trg_workspace_name, \"old\": \"00000000-0000-0000-0000-000000000000\", \"new\": self.trg_workspace_id })\n",
    "        self.mapping_table.append({\"Type\": \"Workspace Name\", \"Item\": self.trg_workspace_name, \"old\": self.src_workspace_name, \"new\": self.trg_workspace_name.replace(\".Workspace\", \"\") })\n",
    "\n",
    "        exclude = exclude + [self.src_workspace_name]\n",
    "\n",
    "        for it in self.deployment_order:\n",
    "            new_id = None            \n",
    "            name = it[\"name\"]\n",
    "            type = it.get(\"type\")\n",
    "\n",
    "            if name in exclude:\n",
    "                continue    \n",
    "            \n",
    "            if type in type_exclude:\n",
    "                continue\n",
    "\n",
    "            self.__deploy_item(name,None,it)\n",
    "\n",
    "            for child in it.get(\"children\",[]):\n",
    "                child_name = child[\"name\"]\n",
    "                self.__deploy_item(name,child_name,child)\n",
    "\n",
    "    def fab_update_environments_spark_monitor(self,evironments, workspace_name, eventstream_name, connection_name):\n",
    "        \n",
    "        for environment in evironments:\n",
    "            workspace = environment.get(\"workspace_id\")            \n",
    "            environment = environment.get(\"environment_id\")\n",
    "            print(\"################### UPDATING ENVIRONMENT\")\n",
    "            print(\"workspace: \" + workspace)\n",
    "            print(\"Environment: \" + environment)\n",
    "            \n",
    "            self.trg_workspace_id = self.__fab_get_workspace_id(f\"{workspace_name}.Workspace\")\n",
    "            self.trg_workspace_name = f\"{workspace_name}.Workspace\"\n",
    "            eventstream_name = f\"{eventstream_name}.Eventstream\" if \".Eventstream\" not in eventstream_name else eventstream_name\n",
    "            connection_string = self.fab_get_eventstream_connection_string(eventstream_name,connection_name)\n",
    "            StringSparkProperties = json.dumps(\n",
    "                {\n",
    "                    \"sparkProperties\":\n",
    "                        {\n",
    "                            \"spark.synapse.diagnostic.emitters\": \"SparkEmitter\",\n",
    "                            \"spark.synapse.diagnostic.emitter.SparkEmitter.type\": \"AzureEventHub\",\n",
    "                            \"spark.synapse.diagnostic.emitter.SparkEmitter.secret\": connection_string,\n",
    "                            \"spark.fabric.pools.skipStarterPools\": \"true\"\n",
    "                        }\n",
    "                }\n",
    "            )\n",
    "            response = self.__run_fab_command(f\"api -X patch /workspaces/{workspace}/environments/{environment}/staging/sparkcompute -i  {StringSparkProperties}\", capture_output = True, silently_continue= True)\n",
    "            response = self.__run_fab_command(f\"api -X post /workspaces/{workspace}/environments/{environment}/staging/publish \", capture_output = True, silently_continue= True)\n",
    "            print(\"################### FINISH UPDATING ENVIRONMENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888efa78-8050-4857-b08b-8e715cf4fec6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## ****Deployment Worder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6592e-b3c7-475b-943e-a6f650a72c7f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "exclude = []\n",
    "type_exclude = []\n",
    "\n",
    "fabDeployCLI = FabDeployCLI(repo_owner, repo_name, branch, folder_prefix, github_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f1b2a3-5919-4424-9631-5ebdb5690dbb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "fabDeployCLI.run(exclude=exclude, type_exclude=type_exclude, workspace_name=workspace_name, capacity_name=capacity_name, eventhouse_name=eventhouse_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13874a3-64c8-42f2-85ac-81e5d732c030",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "fabDeployCLI.fab_update_environments_spark_monitor(environments, workspace_name, \"SparkMonitoring\",\"IngestionEndpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f882c26-3c2f-4f25-a5bb-8c6307b12bf7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "display(fabDeployCLI.mapping_table)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
