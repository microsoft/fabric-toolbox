{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fed6147-755d-461a-8a56-75e6aaab935b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Welcome to FUAM Deployment\n",
    "\n",
    "This notebook deployes the latest FUAM version in the specified workspace. It works for initial deployment and for the upgrade process of FUAM.\n",
    "\n",
    "**End-to-end documenation on fabric-toolbox:**\n",
    "\n",
    "[Visit - How to deploy and configure FUAM](https://github.com/microsoft/fabric-toolbox/blob/main/monitoring/fabric-unified-admin-monitoring/how-to/How_to_deploy_FUAM.md)\n",
    "\n",
    "**What is happening in this notebook?**\n",
    " - The notebook checks the two cloud connections for FUAM (if initial deployment, connections will be created, otherwise check only)\n",
    " - It downloads the latest FUAM src files from Github\n",
    " - It deploys/updates the Fabric items in the current workspace\n",
    " - It creates all needed tables automatically, so reports work also with some data missing\n",
    "\n",
    "**Next steps**\n",
    "- (Optional) Change connection names, only if needed\n",
    "- Run this notebook\n",
    "\n",
    "If you **deploy** FUAM in this workspace at the **first time**:\n",
    "- Navigate to the cloud connections\n",
    "- Search under cloud connection for **fuam fabric-service-api admin** and for **fuam pbi-service-api admin** \n",
    "- Add the credentials of your service principal to these connections\n",
    "\n",
    "If you **update** your existing FUAM workspace:\n",
    "- After the notebooks has been executed, you are **done**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439a740-1fc5-4e3c-a47e-de324036912a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install ms-fabric-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e022b8-0d0d-4236-857c-6d253c78122d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "pbi_connection_name = 'fuam pbi-service-api admin'\n",
    "fabric_connection_name = 'fuam fabric-service-api admin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb77f4b-4cc8-49c6-83cd-231680a1d618",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Import of needed libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e6feb-ec15-4bb6-b111-5558df98fea0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import sempy.fabric as fabric\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44471f8d-0f6f-4d86-9bef-20a0afccd13e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Download of source & config files\n",
    "This part downloads all source and config files of FUAM needed for the deployment into the ressources of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfa16b-c718-48d6-81b1-00f456ccc80d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "def download_folder_as_zip(repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"src\",  remove_folder_prefix = \"\"):\n",
    "    # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "    \n",
    "    # Make a request to the GitHub API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Ensure the directory for the output zip file exists\n",
    "    os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "    \n",
    "    # Create a zip file in memory\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "        with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "            for file_info in zipf.infolist():\n",
    "                parts = file_info.filename.split('/')\n",
    "                if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n",
    "                    # Extract only the specified folder\n",
    "                    file_data = zipf.read(file_info.filename)\n",
    "                    output_zipf.writestr(('/'.join(parts[1:]).replace(remove_folder_prefix, \"\")), file_data)\n",
    "\n",
    "def uncompress_zip_to_folder(zip_path, extract_to):\n",
    "    # Ensure the directory for extraction exists\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    # Uncompress all files from the zip into the specified folder\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    # Delete the original zip file\n",
    "    os.remove(zip_path)\n",
    "\n",
    "repo_owner = \"Microsoft\"\n",
    "repo_name = \"fabric-toolbox\"\n",
    "branch = \"main\"\n",
    "folder_prefix = \"monitoring/fabric-unified-admin-monitoring\"\n",
    "\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/src/src.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/src\", remove_folder_prefix = f\"{folder_prefix}/\")\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/config/config.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/config\" , remove_folder_prefix = folder_prefix)\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/data/data.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/data\" , remove_folder_prefix = folder_prefix)\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/data/data.zip\", extract_to= \"./builtin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef159d-0f52-43a1-a895-5c3b293ffc61",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "base_path = './builtin/'\n",
    "config_path = os.path.join(base_path, 'config/deployment_config.yaml')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/deployment_order.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        deployment_order = json.load(file)\n",
    "\n",
    "src_workspace_name = config['workspace']\n",
    "src_pbi_connection = config['connections']['pbi_connection']\n",
    "src_fabric_connection = config['connections']['fabric_connection']\n",
    "\n",
    "semantic_model_connect_to_lakehouse = config['fuam_lakehouse_semantic_models']\n",
    "\n",
    "mapping_table=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b4640-8237-433b-ac46-8986531f28ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Definition of deployment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba6a51-107d-4c10-9a41-5cb2a9e07c27",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "def run_fab_command(command, capture_output: bool = False, silently_continue: bool = False, timeout: int = 300):\n",
    "    \"\"\"Run fabric CLI command with timeout protection (default 5 minutes).\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"fab\", \"-c\", command], \n",
    "            capture_output=capture_output, \n",
    "            text=True, \n",
    "            timeout=timeout\n",
    "        )\n",
    "        if (not(silently_continue) and (result.returncode > 0 or result.stderr)):\n",
    "            raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result.stderr}'\")    \n",
    "        if (capture_output): \n",
    "            output = result.stdout.strip()\n",
    "            return output\n",
    "    except subprocess.TimeoutExpired:\n",
    "        raise Exception(f\"Command timed out after {timeout} seconds: {command}\")\n",
    "\n",
    "def fab_get_id(name):\n",
    "    id = run_fab_command(f\"get /{trg_workspace_name}.Workspace/{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "    return(id)\n",
    "\n",
    "def get_id_by_name(name):\n",
    "    for it in deployment_order:\n",
    "        if it.get(\"name\") == name:\n",
    "                return it.get(\"fuam_id\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def copy_to_tmp(name):\n",
    "    \"\"\"Extract files from zip to memory (handles nested folders at any depth).\"\"\"\n",
    "    path2zip = \"./builtin/src/src.zip\"\n",
    "    file_contents = {}  # Store file paths and their content in memory\n",
    "    \n",
    "    with ZipFile(path2zip) as archive:\n",
    "        for file in archive.namelist():\n",
    "            # Skip directory entries (ending with /) but include all files at any nesting level\n",
    "            # This handles: src/name/file.txt, src/name/subfolder/file.txt, src/name/a/b/c/file.txt, etc.\n",
    "            if file.startswith(f'src/{name}/') and not file.endswith('/'):\n",
    "                # Read file content into memory instead of extracting to disk\n",
    "                file_contents[file] = archive.read(file)\n",
    "    \n",
    "    return file_contents\n",
    "\n",
    "\n",
    "def replace_ids_in_memory(file_contents, mapping_table):\n",
    "    \"\"\"Replace IDs in memory-stored files.\"\"\"\n",
    "    updated_contents = {}\n",
    "    \n",
    "    for file_path, content_bytes in file_contents.items():\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        # Decode bytes to string\n",
    "        try:\n",
    "            content = content_bytes.decode('utf-8')\n",
    "        except:\n",
    "            # If decoding fails, keep as binary\n",
    "            updated_contents[file_path] = content_bytes\n",
    "            continue\n",
    "        \n",
    "        if file_name.endswith('.ipynb'):\n",
    "            notebook_json = json.loads(content)\n",
    "            dependencies = notebook_json.get('metadata', {}).get('dependencies', {})\n",
    "            depend = json.dumps(dependencies)\n",
    "            for mapping in mapping_table:  \n",
    "                depend = depend.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "            notebook_json['metadata']['dependencies'] = json.loads(depend)\n",
    "            content = json.dumps(notebook_json)\n",
    "            \n",
    "        elif file_name.endswith(('.py', '.json', '.pbir', '.platform', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "            for mapping in mapping_table:  \n",
    "                content = content.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "        \n",
    "        updated_contents[file_path] = content.encode('utf-8')\n",
    "    \n",
    "    return updated_contents\n",
    "\n",
    "def write_memory_to_temp(file_contents, temp_dir):\n",
    "    \"\"\"Write in-memory files to temporary directory (system temp, not builtin storage).\"\"\"\n",
    "    for file_path, content_bytes in file_contents.items():\n",
    "        full_path = os.path.join(temp_dir, file_path)\n",
    "        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "        with open(full_path, 'wb') as f:\n",
    "            f.write(content_bytes)\n",
    "    return temp_dir\n",
    "\n",
    "def get_semantic_model_id_from_memory(file_contents, name):\n",
    "    \"\"\"Get semantic model ID from in-memory report definition.\"\"\"\n",
    "    definition_path = f'src/{name}/definition.pbir'\n",
    "    if definition_path in file_contents:\n",
    "        content = json.loads(file_contents[definition_path].decode('utf-8'))\n",
    "        semantic_model_id = content.get('datasetReference', {}).get('byConnection', {}).get('pbiModelDatabaseName')\n",
    "        if semantic_model_id:\n",
    "            return semantic_model_id\n",
    "    return None\n",
    "\n",
    "def get_semantic_model_id(report_folder):\n",
    "    definition_file = os.path.join(report_folder, 'definition.pbir')\n",
    "    if os.path.exists(definition_file):\n",
    "        with open(definition_file, 'r', encoding='utf-8') as file:\n",
    "            content = json.load(file)\n",
    "            semantic_model_id = content.get('datasetReference', {}).get('byConnection', {}).get('pbiModelDatabaseName')\n",
    "            if semantic_model_id:\n",
    "                return semantic_model_id\n",
    "    return None\n",
    "\n",
    "def update_sm_connection_to_fuam_lakehouse_in_memory(file_contents, name):\n",
    "    \"\"\"Update semantic model connection to FUAM lakehouse in memory.\"\"\"\n",
    "    new_sm_db = run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.connectionString\", capture_output=True, silently_continue=True)\n",
    "    new_lakehouse_sql_id = run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.id\", capture_output=True, silently_continue=True)\n",
    "    \n",
    "    expressions_path = f'src/{name}/definition/expressions.tmdl'\n",
    "    if expressions_path in file_contents:\n",
    "        content = file_contents[expressions_path].decode('utf-8')\n",
    "        match = re.search(r'Sql\\.Database\\(\"([^\"]+)\",\\s*\"([^\"]+)\"\\)', content)\n",
    "        if match:\n",
    "            old_sm_db, old_lakehouse_sql_id = match.group(1), match.group(2)\n",
    "            content = content.replace(old_sm_db, new_sm_db).replace(old_lakehouse_sql_id, new_lakehouse_sql_id)\n",
    "            file_contents[expressions_path] = content.encode('utf-8')\n",
    "    return file_contents\n",
    "\n",
    "def update_sm_connection_to_fuam_lakehouse(semantic_model_folder):\n",
    "    new_sm_db= run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.connectionString\", capture_output = True, silently_continue=True)\n",
    "    new_lakehouse_sql_id= run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.id\", capture_output = True, silently_continue=True)\n",
    "        \n",
    "    expressions_file = os.path.join(semantic_model_folder, 'definition', 'expressions.tmdl')\n",
    "    if os.path.exists(expressions_file):\n",
    "        with open(expressions_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            match = re.search(r'Sql\\.Database\\(\"([^\"]+)\",\\s*\"([^\"]+)\"\\)', content)\n",
    "            if match:\n",
    "                old_sm_db, old_lakehouse_sql_id = match.group(1), match.group(2)\n",
    "                content = content.replace(old_sm_db, new_sm_db).replace(old_lakehouse_sql_id, new_lakehouse_sql_id)\n",
    "                with open(expressions_file, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "\n",
    "def update_report_definition_in_memory(file_contents, name):\n",
    "    \"\"\"Update report definition in memory to reference semantic model by ID.\"\"\"\n",
    "    semantic_model_id = get_semantic_model_id_from_memory(file_contents, name)\n",
    "    definition_path = f\"src/{name}/definition.pbir\"\n",
    "    \n",
    "    if definition_path in file_contents and semantic_model_id:\n",
    "        report_definition = json.loads(file_contents[definition_path].decode('utf-8'))\n",
    "        \n",
    "        # Update connection string to reference the semantic model by ID\n",
    "        # Format: Data Source=powerbi://api.powerbi.com/v1.0/myorg/{workspace};initial catalog={model};semanticmodelid={id}\n",
    "        connection_string = f\"Data Source=powerbi://api.powerbi.com/v1.0/myorg/{trg_workspace_name};initial catalog={{MODEL_NAME}};integrated security=ClaimsToken;semanticmodelid={semantic_model_id}\"\n",
    "        \n",
    "        # Ensure datasetReference structure exists\n",
    "        if \"datasetReference\" not in report_definition:\n",
    "            report_definition[\"datasetReference\"] = {}\n",
    "        \n",
    "        # Clear byPath if it exists\n",
    "        if \"byPath\" in report_definition[\"datasetReference\"]:\n",
    "            del report_definition[\"datasetReference\"][\"byPath\"]\n",
    "        \n",
    "        # Set byConnection with only the connectionString property\n",
    "        report_definition[\"datasetReference\"][\"byConnection\"] = {\n",
    "            \"connectionString\": connection_string\n",
    "        }\n",
    "        \n",
    "        file_contents[definition_path] = json.dumps(report_definition, indent=4).encode('utf-8')\n",
    "    \n",
    "    return file_contents\n",
    "\n",
    "def update_report_definition(path): \n",
    "    \"\"\"Update report definition to reference semantic model by ID (legacy disk-based version).\"\"\"\n",
    "    semantic_model_id = get_semantic_model_id(path)\n",
    "    definition_path = os.path.join(path, \"definition.pbir\")\n",
    "   \n",
    "    if semantic_model_id:\n",
    "        with open(definition_path, \"r\", encoding=\"utf8\") as file:\n",
    "            report_definition = json.load(file)\n",
    "\n",
    "        # Update connection string to reference the semantic model by ID\n",
    "        connection_string = f\"Data Source=powerbi://api.powerbi.com/v1.0/myorg/{trg_workspace_name};initial catalog={{MODEL_NAME}};integrated security=ClaimsToken;semanticmodelid={semantic_model_id}\"\n",
    "        \n",
    "        # Ensure datasetReference structure exists\n",
    "        if \"datasetReference\" not in report_definition:\n",
    "            report_definition[\"datasetReference\"] = {}\n",
    "        \n",
    "        # Clear byPath if it exists\n",
    "        if \"byPath\" in report_definition[\"datasetReference\"]:\n",
    "            del report_definition[\"datasetReference\"][\"byPath\"]\n",
    "        \n",
    "        # Set byConnection with only the connectionString property\n",
    "        report_definition[\"datasetReference\"][\"byConnection\"] = {\n",
    "            \"connectionString\": connection_string\n",
    "        }\n",
    "\n",
    "        with open(definition_path, \"w\") as file:\n",
    "            json.dump(report_definition, file, indent=4)\n",
    "\n",
    "def print_color(text, state):\n",
    "    red  = '\\033[91m'\n",
    "    yellow = '\\033[93m'  \n",
    "    green = '\\033[92m'   \n",
    "    white = '\\033[0m'  \n",
    "    if state == \"error\":\n",
    "        print(red, text, white)\n",
    "    elif state == \"warning\":\n",
    "        print(yellow, text, white)\n",
    "    elif state == \"success\":\n",
    "        print(green, text, white)\n",
    "    else:\n",
    "        print(\"\", text)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a24f9d-acfc-41a4-ae24-772e5200f74a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Creation of connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328d01d-a5ef-4828-b2cf-dcb0d7482e53",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def create_or_get_connection(name, baseUrl, audience):\n",
    "    try:\n",
    "        run_fab_command(f\"\"\"create .connections/{name}.connection \n",
    "            -P connectionDetails.type=WebForPipeline,connectionDetails.creationMethod=WebForPipeline.Contents,connectionDetails.parameters.baseUrl={baseUrl},connectionDetails.parameters.audience={audience},credentialDetails.type=Anonymous\"\"\")\n",
    "        print_color(\"New connection created. Enter service principal credentials\", \"success\")\n",
    "    except Exception as ex:\n",
    "        print_color(\"Connection already exists\", \"warning\")\n",
    "\n",
    "    conn_id = run_fab_command(f\"get .connections/{name}.Connection -q id\", silently_continue= True, capture_output= True)\n",
    "    print(\"Connection ID:\" + conn_id)\n",
    "    \n",
    "    \n",
    "    return(conn_id)\n",
    "    \n",
    "conn_pbi_service_api_admin = create_or_get_connection(pbi_connection_name, \"https://api.powerbi.com/v1.0/myorg/admin\", \"https://analysis.windows.net/powerbi/api\" )\n",
    "conn_fabric_service_api_admin = create_or_get_connection(fabric_connection_name, \"https://api.fabric.microsoft.com/v1/admin\", \"\thttps://api.fabric.microsoft.com\" )\n",
    "\n",
    "mapping_table.append({ \"old_id\": get_id_by_name(src_pbi_connection), \"new_id\": conn_pbi_service_api_admin })\n",
    "mapping_table.append({ \"old_id\": get_id_by_name(src_fabric_connection), \"new_id\": conn_fabric_service_api_admin })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473b6db-df34-4a72-bd1a-05d5ddefa78e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get current Workspace\n",
    "This cell gets the current workspace to deploy FUAM automatically inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54542c-1e34-434b-bada-d73fbe7c2535",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "trg_workspace_id = fabric.get_notebook_workspace_id()\n",
    "res = run_fab_command(f\"api -X get workspaces/{trg_workspace_id}\" , capture_output = True, silently_continue=True)\n",
    "trg_workspace_name = json.loads(res)[\"text\"][\"displayName\"]\n",
    "\n",
    "print(f\"Current workspace: {trg_workspace_name}\")\n",
    "print(f\"Current workspace ID: {trg_workspace_id}\")\n",
    "\n",
    "\n",
    "mapping_table.append({ \"old_id\": get_id_by_name(src_workspace_name + \".Workspace\"), \"new_id\": trg_workspace_id })\n",
    "mapping_table.append({ \"old_id\": \"00000000-0000-0000-0000-000000000000\", \"new_id\": trg_workspace_id })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b5e53-bb86-4305-b3f2-62d259d8ad67",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "mapping_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8628a42-1d8a-4192-8e75-4754e626fede",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deployment Logic\n",
    "This part iterates through all the items, gets the respective source code, replaces all IDs dynamically and deploys the new item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ef8af-ff5a-4323-869c-7198e910c0bf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "exclude = [src_workspace_name + \".Workspace\", src_pbi_connection, src_fabric_connection]\n",
    "\n",
    "for it in deployment_order:\n",
    "\n",
    "    new_id = None\n",
    "    \n",
    "    name = it[\"name\"]\n",
    "    \n",
    "    if name in exclude:\n",
    "            continue\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"Deploying {name}\")\n",
    "\n",
    "    # Copy to memory and replace IDs in-memory\n",
    "    file_contents = copy_to_tmp(name)\n",
    "    file_contents = replace_ids_in_memory(file_contents, mapping_table)\n",
    "\n",
    "    cli_parameter = ''\n",
    "    if \"Notebook\" in name:\n",
    "        cli_parameter = cli_parameter + \" --format .ipynb\"\n",
    "    elif \"Lakehouse\" in name:\n",
    "        run_fab_command(f\"create /{trg_workspace_name}.Workspace/{name}  -P enableSchemas=False\" , silently_continue=True )\n",
    "        new_id = fab_get_id(name)\n",
    "        mapping_table.append({ \"old_id\": get_id_by_name(name), \"new_id\": new_id })\n",
    "        \n",
    "        continue\n",
    "    elif \"Report\" in name:\n",
    "        file_contents = update_report_definition_in_memory(file_contents, name)\n",
    "    elif name in semantic_model_connect_to_lakehouse:\n",
    "        file_contents = update_sm_connection_to_fuam_lakehouse_in_memory(file_contents, name)\n",
    "    \n",
    "    # Use system temp directory (often RAM-based) instead of builtin storage\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        try:\n",
    "            write_memory_to_temp(file_contents, temp_dir)\n",
    "            item_path = os.path.join(temp_dir, f\"src/{name}\")\n",
    "            \n",
    "            print(f\"Importing {name}...\")\n",
    "            run_fab_command(f\"import  /{trg_workspace_name}.Workspace/{name} -i {item_path} -f {cli_parameter} \", silently_continue= True, timeout=600)\n",
    "            new_id= fab_get_id(name)\n",
    "            mapping_table.append({ \"old_id\": it[\"fuam_id\"], \"new_id\": new_id })\n",
    "            print_color(f\"✓ Successfully deployed {name}\", \"success\")\n",
    "        except subprocess.TimeoutExpired as e:\n",
    "            print_color(f\"✗ Timeout importing {name}: {str(e)}\", \"error\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print_color(f\"✗ Error deploying {name}: {str(e)}\", \"error\")\n",
    "            raise\n",
    "        # temp_dir automatically cleaned up when context exits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6a7c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Move items into folders\n",
    "The items will be moved into the respective folders. Definition is done in the deployment_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0437a0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "items_in_ws =  json.loads(run_fab_command(f'api /workspaces/{trg_workspace_id}/items', capture_output= True))['text']['value']\n",
    "\n",
    "\n",
    "def find_existing_item_id(item_name):\n",
    "    for item in items_in_ws:\n",
    "        if item_name == item['displayName'] + '.' + item['type']:\n",
    "            return item['id']\n",
    "\n",
    "\n",
    "for folder in config['folders']:\n",
    "    print(folder['name'])\n",
    "    folder_name = folder['name']\n",
    "\n",
    "    folder_exists = run_fab_command(f'exists /{trg_workspace_name}.Workspace/{folder_name}.Folder', capture_output= True)\n",
    "    print(folder_exists)\n",
    "    if 'false' in folder_exists:\n",
    "        print(f'Create folder {folder_name}')\n",
    "        run_fab_command(f'create /{trg_workspace_name}.Workspace/{folder_name}.Folder')\n",
    "    \n",
    "    folder_id = run_fab_command(f'get {trg_workspace_name}.Workspace/{folder_name}.Folder -q id',  capture_output= True) \n",
    "    print(f'Move items into folder: {folder_name}')  \n",
    "    item_ids = []\n",
    "    for item in folder['items']:\n",
    "        found_it = find_existing_item_id(item)\n",
    "        if found_it is not None:\n",
    "            item_ids.append(found_it)\n",
    "    it = str(item_ids).replace(\"'\", '\"')\n",
    "    res = run_fab_command(f' api -X post workspaces/{trg_workspace_id}/items/bulkmove  -i \\'{{\"targetFolderId\": \"{folder_id}\", \"items\": {it} }}\\' ', capture_output = True)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8b0d2-a877-41e8-928a-65855b12bd1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Post-Deployment logic\n",
    "In this separate notebook, all needed tables for FUAM are automatically deployed. Addtionally new columns will be added to lakehouse tables in order to be available for the semantic model. This notebook has been deployed from the source code in the step before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb1703-82b2-4257-99c9-9b8d8b9e0ab6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%%configure -f \n",
    "{   \"defaultLakehouse\": { \"name\": \"FUAM_Config_Lakehouse\" } }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e172d2-c00c-42d1-8c6a-3d993247a79e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install ms-fabric-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e3268-556c-4d44-8736-b8a4cdf00d23",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import sempy.fabric as fabric\n",
    "import time\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a3eaf4-dada-45f9-9dcb-7f68ac376d08",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "def run_fab_command( command, capture_output: bool = False, silently_continue: bool = False):\n",
    "    result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "    if (not(silently_continue) and (result.returncode > 0 or result.stderr)):\n",
    "       raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result.stderr}'\")    \n",
    "    if (capture_output): \n",
    "        output = result.stdout.strip()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7873582-a4e7-4e67-83ff-750b0c7acdce",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "trg_workspace_id = fabric.get_notebook_workspace_id()\n",
    "trg_workspace_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12fe1d-d26b-411d-9b87-726536df4b9c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "trg_workspace_id = fabric.get_notebook_workspace_id()\n",
    "res = run_fab_command(f\"api -X get workspaces/{trg_workspace_id}\" , capture_output = True)\n",
    "trg_workspace_name = json.loads(res)[\"text\"][\"displayName\"]\n",
    "\n",
    "print(f\"Current workspace: {trg_workspace_name}\")\n",
    "print(f\"Current workspace ID: {trg_workspace_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3257437-01fc-4149-a9e6-210678eba47b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "src_file_path = \"./builtin/data/table_definitions.snappy.parquet\"\n",
    "with open(src_file_path, 'rb') as file:\n",
    "                    content = file.read()\n",
    "trg_lakehouse_folder_path = notebookutils.fs.getMountPath('/default') + \"/Files/table_definitions/\" \n",
    "notebookutils.fs.mkdirs(f\"file://\" +trg_lakehouse_folder_path)\n",
    "with open(trg_lakehouse_folder_path + \"table_definitions.snappy.parquet\", \"wb\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09edbdcc-4c4f-418f-9714-48ff2018c880",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "notebookutils.lakehouse.loadTable(\n",
    "    {\n",
    "        \"relativePath\": f\"Files/table_definitions/table_definitions.snappy.parquet\",\n",
    "        \"pathType\": \"File\",\n",
    "        \"mode\": \"Overwrite\",\n",
    "        \"recursive\": False,\n",
    "        \"formatOptions\": {\n",
    "            \"format\": \"Parquet\"\n",
    "        }\n",
    "    }, \"FUAM_Table_Definitions\", \"FUAM_Config_Lakehouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad141615-240e-436f-a47e-599c09c1bd58",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "In case the last step fails, please try to run it again or go to the Init_FUAM_Lakehouse_Tables Notebook and run it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d2c3c-6d5a-43e6-b950-bf70dd1de35d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Refresh SQL Endpoint for Config_Lakehouse\n",
    "items = run_fab_command(f'api -X get -A fabric /workspaces/{trg_workspace_id}/items' , capture_output = True)\n",
    "for it in json.loads(items)['text']['value']:\n",
    "    if (it['displayName'] == 'FUAM_Config_Lakehouse' ) & (it['type'] =='SQLEndpoint' ):\n",
    "        config_sql_endpoint = it['id']\n",
    "    if (it['displayName'] == 'FUAM_Lakehouse' ) & (it['type'] =='SQLEndpoint' ):\n",
    "        lh_sql_endpoint = it['id']\n",
    "print(f\"FUAM_Lakehouse SQL Endpoint ID: {lh_sql_endpoint}\")\n",
    "print(f\"FUAM_Config_Lakehouse SQL Endpoint ID: {config_sql_endpoint}\")\n",
    "\n",
    "try:\n",
    "    run_fab_command(f'api -A fabric -X post workspaces/{trg_workspace_id}/sqlEndpoints/{config_sql_endpoint}/refreshMetadata?preview=True -i {{}} ', capture_output=True)\n",
    "except:\n",
    "    print(\"SQL Endpoint Refresh API failed, it is still in Preview, so there can be changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8316de-e9e4-4d7a-a59d-d7c71cd2424e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Fill default tables\n",
    "time.sleep(10)\n",
    "run_fab_command('job run ' + trg_workspace_name + '.Workspace/Init_FUAM_Lakehouse_Tables.Notebook -i {\"parameters\": {\"_inlineInstallationEnabled\": {\"type\": \"Bool\", \"value\": \"True\"} } }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caead30-e459-4633-90c1-436737e395c1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Refresh of SQL Endpoint to make sure all tables are available\n",
    "try:\n",
    "    run_fab_command(f'api -A fabric -X post workspaces/{trg_workspace_id}/sqlEndpoints/{lh_sql_endpoint}/refreshMetadata?preview=True -i {{}} ', capture_output=True)\n",
    "    print(\"Refresh FUAM_Lakehouse_SQL_Endpoint\")\n",
    "except:\n",
    "    print(\"SQL Endpoint Refresh API failed, it is still in Preview, so there can be changes\")\n",
    "# Refresh Semantic Models on top of lakehouse\n",
    "base_path = './builtin/'\n",
    "config_path = os.path.join(base_path, 'config/deployment_config.yaml')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "semantic_model_connect_to_lakehouse = config['fuam_lakehouse_semantic_models']\n",
    "\n",
    "for sm in semantic_model_connect_to_lakehouse:\n",
    "    sm_id = run_fab_command(f\"get /{trg_workspace_name}.Workspace/{sm} -q id\" , capture_output = True, silently_continue= True)\n",
    "    run_fab_command(f'api -A powerbi -X post datasets/{sm_id}/refreshes -i  {{ \"retryCount\":\"3\" }} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15a725-70f2-4fc6-af8b-109ea6f4ce1f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
