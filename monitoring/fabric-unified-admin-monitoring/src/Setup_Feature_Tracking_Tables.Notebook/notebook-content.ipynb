{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d1ec5e",
   "metadata": {},
   "source": [
    "# Setup Feature Tracking Tables\n",
    "\n",
    "**Purpose**: One-time setup to create Delta tables for Feature Release Tracking\n",
    "\n",
    "**Tables Created**:\n",
    "1. `feature_releases` - Microsoft Fabric feature releases from What's New\n",
    "2. `preview_features_active` - Preview features currently activated in tenant\n",
    "3. `feature_alerts` - Alerts for preview feature activations\n",
    "\n",
    "**Run Once**: This notebook only needs to be executed once during initial setup\n",
    "\n",
    "**Prerequisites**: \n",
    "- FUAM Lakehouse must exist\n",
    "- `tenant_settings` table should already be created by FUAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bef748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b4ddc",
   "metadata": {},
   "source": [
    "## Step 1: Create `feature_releases` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d499ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Creating table: feature_releases\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "schema_feature_releases = StructType([\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"release_date\", TimestampType(), False),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"is_preview\", BooleanType(), False),\n",
    "    StructField(\"source_url\", StringType(), True),\n",
    "    StructField(\"extracted_date\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Create empty DataFrame\n",
    "df_feature_releases = spark.createDataFrame([], schema_feature_releases)\n",
    "\n",
    "# Write to Delta\n",
    "table_path = \"Tables/feature_releases\"\n",
    "df_feature_releases.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "\n",
    "print(\"‚úÖ Table created: feature_releases\")\n",
    "print(f\"   Location: {table_path}\")\n",
    "print(\"\\n   Schema:\")\n",
    "for field in schema_feature_releases.fields:\n",
    "    print(f\"     - {field.name}: {field.dataType.simpleString()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352b549",
   "metadata": {},
   "source": [
    "## Step 2: Create `preview_features_active` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8603d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Creating table: preview_features_active\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "schema_preview_active = StructType([\n",
    "    StructField(\"setting_name\", StringType(), False),\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"similarity_score\", DoubleType(), False),\n",
    "    StructField(\"is_enabled\", BooleanType(), False),\n",
    "    StructField(\"delegate_to_tenant\", BooleanType(), True),\n",
    "    StructField(\"detected_date\", TimestampType(), False),\n",
    "    StructField(\"release_date\", TimestampType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"source_url\", StringType(), True),\n",
    "    StructField(\"days_since_release\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create empty DataFrame\n",
    "df_preview_active = spark.createDataFrame([], schema_preview_active)\n",
    "\n",
    "# Write to Delta\n",
    "table_path = \"Tables/preview_features_active\"\n",
    "df_preview_active.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "\n",
    "print(\"‚úÖ Table created: preview_features_active\")\n",
    "print(f\"   Location: {table_path}\")\n",
    "print(\"\\n   Schema:\")\n",
    "for field in schema_preview_active.fields:\n",
    "    print(f\"     - {field.name}: {field.dataType.simpleString()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05db54",
   "metadata": {},
   "source": [
    "## Step 3: Create `feature_alerts` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21331f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Creating table: feature_alerts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "schema_alerts = StructType([\n",
    "    StructField(\"alert_id\", StringType(), False),\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"alert_type\", StringType(), False),\n",
    "    StructField(\"severity\", StringType(), False),\n",
    "    StructField(\"message\", StringType(), False),\n",
    "    StructField(\"days_since_release\", IntegerType(), True),\n",
    "    StructField(\"similarity_score\", DoubleType(), True),\n",
    "    StructField(\"alert_date\", TimestampType(), False),\n",
    "    StructField(\"acknowledged\", BooleanType(), False)\n",
    "])\n",
    "\n",
    "# Create empty DataFrame\n",
    "df_alerts = spark.createDataFrame([], schema_alerts)\n",
    "\n",
    "# Write to Delta\n",
    "table_path = \"Tables/feature_alerts\"\n",
    "df_alerts.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "\n",
    "print(\"‚úÖ Table created: feature_alerts\")\n",
    "print(f\"   Location: {table_path}\")\n",
    "print(\"\\n   Schema:\")\n",
    "for field in schema_alerts.fields:\n",
    "    print(f\"     - {field.name}: {field.dataType.simpleString()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f748b",
   "metadata": {},
   "source": [
    "## Step 4: Verify Tables Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3396868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Verification Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tables_to_verify = [\n",
    "    \"feature_releases\",\n",
    "    \"preview_features_active\", \n",
    "    \"feature_alerts\"\n",
    "]\n",
    "\n",
    "all_tables_exist = True\n",
    "\n",
    "for table_name in tables_to_verify:\n",
    "    table_path = f\"Tables/{table_name}\"\n",
    "    try:\n",
    "        from delta.tables import DeltaTable\n",
    "        \n",
    "        if DeltaTable.isDeltaTable(spark, table_path):\n",
    "            df = spark.read.format(\"delta\").load(table_path)\n",
    "            count = df.count()\n",
    "            print(f\"‚úÖ {table_name}: EXISTS (rows={count})\")\n",
    "        else:\n",
    "            print(f\"‚ùå {table_name}: NOT A DELTA TABLE\")\n",
    "            all_tables_exist = False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {table_name}: ERROR - {e}\")\n",
    "        all_tables_exist = False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if all_tables_exist:\n",
    "    print(\"\\nüéâ SUCCESS! All tables created successfully\")\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"1. Run '01_Transfer_Feature_Releases_Unit' to populate feature_releases\")\n",
    "    print(\"2. Run '02_Transfer_Preview_Features_Unit' to detect activated previews\")\n",
    "    print(\"3. Run '03_Transfer_Feature_Alerts_Unit' to generate alerts\")\n",
    "    print(\"\\nOR\")\n",
    "    print(\"‚Üí Run 'Load_Feature_Tracking_E2E' pipeline to execute all steps\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Some tables failed to create. Review errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be40c2",
   "metadata": {},
   "source": [
    "## Step 5: Create Sample Queries View (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Creating helper views for SQL Endpoint...\")\n",
    "\n",
    "# View 1: Active Preview Features (for quick querying)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW vw_active_preview_features AS\n",
    "    SELECT \n",
    "        feature_name,\n",
    "        workload,\n",
    "        setting_name,\n",
    "        days_since_release,\n",
    "        similarity_score,\n",
    "        release_date,\n",
    "        detected_date\n",
    "    FROM preview_features_active\n",
    "    WHERE is_enabled = true\n",
    "    ORDER BY detected_date DESC\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created view: vw_active_preview_features\")\n",
    "\n",
    "# View 2: Unacknowledged Critical Alerts\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW vw_critical_alerts AS\n",
    "    SELECT \n",
    "        alert_id,\n",
    "        feature_name,\n",
    "        workload,\n",
    "        alert_type,\n",
    "        severity,\n",
    "        message,\n",
    "        alert_date\n",
    "    FROM feature_alerts\n",
    "    WHERE acknowledged = false \n",
    "      AND severity IN ('Critical', 'Warning')\n",
    "    ORDER BY alert_date DESC\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created view: vw_critical_alerts\")\n",
    "\n",
    "# View 3: Feature Release Timeline\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW vw_feature_timeline AS\n",
    "    SELECT \n",
    "        feature_name,\n",
    "        workload,\n",
    "        status,\n",
    "        is_preview,\n",
    "        release_date,\n",
    "        DATEDIFF(CURRENT_DATE(), release_date) as days_since_release\n",
    "    FROM feature_releases\n",
    "    ORDER BY release_date DESC\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created view: vw_feature_timeline\")\n",
    "\n",
    "print(\"\\n‚úÖ All views created successfully\")\n",
    "print(\"   ‚Üí These views are accessible via SQL Endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1630563f",
   "metadata": {},
   "source": [
    "## ‚úÖ Setup Complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ FEATURE TRACKING SETUP COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìã Tables Created:\")\n",
    "print(\"   ‚úÖ feature_releases\")\n",
    "print(\"   ‚úÖ preview_features_active\")\n",
    "print(\"   ‚úÖ feature_alerts\")\n",
    "\n",
    "print(\"\\nüìã Views Created:\")\n",
    "print(\"   ‚úÖ vw_active_preview_features\")\n",
    "print(\"   ‚úÖ vw_critical_alerts\")\n",
    "print(\"   ‚úÖ vw_feature_timeline\")\n",
    "\n",
    "print(\"\\nüöÄ Ready to Run:\")\n",
    "print(\"   ‚Üí Execute 'Load_Feature_Tracking_E2E' pipeline\")\n",
    "print(\"   ‚Üí Or run individual Unit notebooks\")\n",
    "\n",
    "print(\"\\nüìä Query Examples:\")\n",
    "print(\"   SELECT * FROM vw_active_preview_features;\")\n",
    "print(\"   SELECT * FROM vw_critical_alerts;\")\n",
    "print(\"   SELECT * FROM vw_feature_timeline WHERE is_preview = true;\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
