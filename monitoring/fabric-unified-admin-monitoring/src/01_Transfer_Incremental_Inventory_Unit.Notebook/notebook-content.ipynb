{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "### Tenant Scan - Inventory data\n",
                "\n",
                "This notebook requests a tenant scan for **last modified workspaces** and **loads the results to FUAM Lakehouse**.\n",
                "\n",
                "##### Data ingestion strategy:\n",
                "<mark style=\"background: #88D5FF;\">**REPLACE**</mark>\n",
                "\n",
                "##### Related pipeline:\n",
                "\n",
                "**Load_Inventory_E2E**\n",
                "\n",
                "##### Source:\n",
                "\n",
                "**Files** from FUAM_Lakehouse folder **bronze_file_location** variable\n",
                "\n",
                "##### Target:\n",
                "\n",
                "**Different delta tables** in FUAM_Lakehouse"
            ],
            "metadata": {},
            "id": "a09f7363-0c1d-4d2f-8d24-56c2288ff6b1"
        },
        {
            "cell_type": "code",
            "source": [
                "import sempy.fabric as fabric\n",
                "from pyspark.sql.functions import col, explode\n",
                "import pyspark.sql.functions as f\n",
                "from delta.tables import *\n",
                "import pandas as pd\n",
                "import json\n",
                "from pyspark.sql.types import *\n",
                "import datetime\n",
                "import time\n",
                "pd.options.mode.chained_assignment = None # This option is used to suppress a warning "
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":\"2025-04-04T14:28:03.3445155Z\",\"execution_start_time\":\"2025-04-04T14:28:17.5691589Z\",\"execution_finish_time\":\"2025-04-04T14:28:21.1384747Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "8caaa26f-51a6-4f56-afc7-0e624a624ddb"
        },
        {
            "cell_type": "markdown",
            "source": [
                "##### Parameters:\n",
                "----------------\n",
                "\n",
                "Tenant specific parameters\n",
                "- **has_tenant_domains**: Set to _True_, if your tenant uses Domains\n",
                "- **extract_powerbi_artifacts_only**: If _True_, the notebook extracts and writes data for Power BI artifacts only (datamart, semantic model, dataflow, report, dashboard)\n",
                "- **display_data**: If true, the notebook shows the results of important steps (eg. extracted workspaces dataframe)\n",
                "\n"
            ],
            "metadata": {},
            "id": "b53809dc-2db3-4f37-aeff-1c4cd8896ef8"
        },
        {
            "cell_type": "code",
            "source": [
                "## Parameters\n",
                "# Tenant specific parameters\n",
                "has_tenant_domains = False\n",
                "extract_powerbi_artifacts_only = False\n",
                "\n",
                "# Debug\n",
                "display_data = False\n",
                "\n",
                "# Optional\n",
                "# Key Vault details, this should be used to query the APIs via Service Principal instead of the user\n",
                "optional_keyvault_name = '' # Name of the Azure Key Vault\n",
                "# Names of the secrets saved in Azure Key Vault \n",
                "optional_keyvault_sp_tenantId_secret_name = ''   # Tenant ID secret name\n",
                "optional_keyvault_sp_clientId_secret_name = ''   # Name for Client ID of Service Principal\n",
                "optional_keyvault_sp_secret_secret_name = '' # Name for Client Secret of Service Principal"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ],
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:21.1405245Z\",\"execution_finish_time\":\"2025-04-04T14:28:21.4229159Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "143ec38c-7bac-4822-956e-13da7a7e08f9"
        },
        {
            "cell_type": "markdown",
            "source": [
                "##### Variables:\n",
                "\n",
                "--------------\n",
                "an API request specific parameters\n",
                "\n",
                "**workspaces_per_request**: This variable determines, how many workspaces will be requested in a single call. Currently a maximum of 100 workspaces can be requested in a single call\n",
                "**max_parallel_requests**: This variable determines, how many concurrent requests can be done towards the scanner API. Currently there is a maximum of 16 parallel requests\n",
                "\n",
                "--------------\n",
                "\n",
                "an API content granularity parameters\n",
                "**get_artifact_users**: If _True_, the artifact users will be included\n",
                "**lineage**: If _True_, the lineage information will be included to artifacts (like in _Workspace_--> _Lineage view_ on the UI)\n",
                "**datasource_details**: If _True_, additional datasource details will be included e.g. semantic model table sources\n",
                "**dataset_schema**: If _True_, semantic model data will include tables, columns, measures\n",
                "**dataset_expressions**: If _True_, additional DAX and M-expressions (Power Query M-language) will be included\n",
                "\n",
                "**Important:** \n",
                "r enhanced meta data scan the following features has to be enabled under 'Admin API settings':\n",
                "Enable _'Enhance admin APIs responses with detailed metadata'_ before set **dataset_schema** to _True_\n",
                "Enable '_Enhance admin APIs responses with DAX and mashup expressions'_ before set **dataset_expressions** to _True_\n",
                "\n",
                "--------------\n",
                "\n",
                "nant specific parameters\n",
                "**has_tenant_domains**: Set to _True_, if your tenant uses Domains\n",
                "**extract_powerbi_artifacts_only**: If _True_, the notebook extracts and writes data for Power BI artifacts only (datamart, semantic model, dataflow, report, dashboard)\n",
                "\n",
                "--------------\n",
                "\n",
                "ditional parameters\n",
                "**write_to_files**: If true, the JSON results will be written to the files of the lakehouse\n"
            ],
            "metadata": {},
            "id": "05c986d4-5a92-4555-b51d-ea3cbdb9d1bc"
        },
        {
            "cell_type": "code",
            "source": [
                "## Variables\n",
                "\n",
                "# Scan API request specific\n",
                "workspaces_per_request = 100\n",
                "max_parallel_requests = 16\n",
                "\n",
                "# Scan API content workspace\n",
                "exclude_personal_workspaces = True\n",
                "exclude_inactive_workspaces = True\n",
                "\n",
                "# Scan API content granularity\n",
                "get_artifact_users = True\n",
                "lineage = True\n",
                "datasource_details = True\n",
                "dataset_schema = False\n",
                "dataset_expressions = False\n",
                "\n",
                "# Additional\n",
                "write_to_files = True"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:21.4248792Z\",\"execution_finish_time\":\"2025-04-04T14:28:21.7196458Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "a8234e9e-18fd-401d-adce-355ffeb25f2c"
        },
        {
            "cell_type": "code",
            "source": [
                "## Logic intern variables\n",
                "\n",
                "# Array of scan results\n",
                "results = []\n",
                "\n",
                "# Array of appended tenant scan content\n",
                "write_list = []"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:21.721415Z\",\"execution_finish_time\":\"2025-04-04T14:28:22.0545508Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "60a34ca7-d3bb-4bf1-89c3-e1818830f0d3"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Requesting scans & fetching results"
            ],
            "metadata": {},
            "id": "e526dd21-498c-4f7d-a819-f3c06d56501d"
        },
        {
            "cell_type": "code",
            "source": [
                "# Init the client\n",
                "client = fabric.FabricRestClient()\n",
                "\n",
                "# Set date helpers\n",
                "current_time = datetime.datetime.now()\n",
                "date = current_time.date()"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:22.0566079Z\",\"execution_finish_time\":\"2025-04-04T14:28:22.4164595Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "990a7b62-7152-405a-982e-77ea97eca1de"
        },
        {
            "cell_type": "code",
            "source": [
                "# Check if key vault secrets are configured in the right way. If not the executing users identity is used\n",
                "try:\n",
                "    keyvault = f'https://{optional_keyvault_name}.vault.azure.net/'\n",
                "    tenant_id = mssparkutils.credentials.getSecret(keyvault, optional_keyvault_sp_tenantId_secret_name)\n",
                "    client_id = mssparkutils.credentials.getSecret(keyvault, optional_keyvault_sp_clientId_secret_name)\n",
                "    client_secret = mssparkutils.credentials.getSecret(keyvault, optional_keyvault_sp_secret_secret_name)\n",
                "    use_keyvault = True\n",
                "    print(\"Service Principal identity is used for API authentification\")\n",
                "except:\n",
                "    print(\"Configured Secrets not found in Key Vault. Script tries to use user identity instead\")\n",
                "    use_keyvault = False"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:22.4185943Z\",\"execution_finish_time\":\"2025-04-04T14:28:22.7362592Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "39637673-0528-44d1-8e44-1463d0ea810c"
        },
        {
            "cell_type": "code",
            "source": [
                "def GenerateHeader():\n",
                "    if use_keyvault:\n",
                "        url = f'https://login.microsoftonline.com/{tenant_id}/oauth2/token'\n",
                "        data = f'grant_type=client_credentials&client_id={client_id}&client_secret={client_secret}&resource=https://analysis.windows.net/powerbi/api'  \n",
                "        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
                "        response = client.post(url, headers=headers, data=data)\n",
                "        return     {'Content-Type': 'application/json', 'Authorization': f'Bearer {response.json()[\"access_token\"]}'}\n",
                "    else:    \n",
                "        return {'Content-Type': 'application/json'}\n",
                "    \n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:22.7383919Z\",\"execution_finish_time\":\"2025-04-04T14:28:23.1097288Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "2ec12b81-b0ae-4f70-bfc6-d3bf135e9976"
        },
        {
            "cell_type": "code",
            "source": [
                "def RequestWithRetry(method, url, data = {}, num_retries=3, success_list=[200, 202, 404], **kwargs):\n",
                "    headers = GenerateHeader()\n",
                "    response = None\n",
                "    for i in range(num_retries):\n",
                "        try:\n",
                "            if method == 'post':\n",
                "                response = client.post(url, json=data, headers=headers,**kwargs)\n",
                "            if method == 'get':\n",
                "                response = client.get(url, headers=headers,**kwargs)\n",
                "                \n",
                "            if response.status_code in success_list:\n",
                "                return response\n",
                "                \n",
                "            ## Captures too many requests error\n",
                "            if response.status_code == 429:\n",
                "                ## Captures the 500 requests in an hour limit\n",
                "                if response.headers.get('Retry-After',None) is not None:\n",
                "                    waitTime = int(response.headers['Retry-After'])\n",
                "                    print(f'Hit the 500 requests per hour rate limit - waiting {str(waitTime)} seconds until next retry')\n",
                "                    time.sleep(waitTime)\n",
                "                ## Captures the 16 simultaneous requests limit\n",
                "                elif response.headers.get('Retry-After',None) is None:\n",
                "                    waitTime = 120\n",
                "                    print(f'Hit the 16 simultaneous requests limit - waiting {str(waitTime)} seconds until next retry')\n",
                "                    time.sleep(waitTime)\n",
                "        except Exception as e:\n",
                "            print(e)\n",
                "            pass\n",
                "    return response"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:23.1119483Z\",\"execution_finish_time\":\"2025-04-04T14:28:23.4277724Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "7a4b4c30-8e58-4a73-a0dc-b105905e9f67"
        },
        {
            "cell_type": "code",
            "source": [
                "# Group the total number of workspaces into packages of 100 (workspaces_per_request)\n",
                "\n",
                "# Get API data\n",
                "response = RequestWithRetry('get', f\"v1.0/myorg/admin/workspaces/modified?excludePersonalWorkspaces={exclude_personal_workspaces}&excludeInActiveWorkspaces={exclude_inactive_workspaces}\")\n",
                "\n",
                "# Build array buckets \n",
                "modified_workspaces = pd.json_normalize(response.json())\n",
                "modified_workspaces[\"index\"] = pd.to_numeric(modified_workspaces.reset_index()[\"index\"])\n",
                "modified_workspaces[\"run\"] = modified_workspaces[\"index\"] // workspaces_per_request\n",
                "modified_workspaces = modified_workspaces.groupby('run')['id'].apply(list)\n",
                "\n",
                "# Init runs\n",
                "df_runs = pd.DataFrame(data = modified_workspaces)\n",
                "df_runs[\"status\"] = \"Not Started\""
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:23.4298034Z\",\"execution_finish_time\":\"2025-04-04T14:28:24.8849179Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "570d4bd4-4b75-4e0c-8d46-00ed894d07b4"
        },
        {
            "cell_type": "code",
            "source": [
                "if display_data:\n",
                "    print(modified_workspaces[0])"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:24.8866913Z\",\"execution_finish_time\":\"2025-04-04T14:28:25.1857285Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "67207761-87b8-4702-8a6f-f93da9cb5e1d"
        },
        {
            "cell_type": "code",
            "source": [
                "# Use getInfo API to request generation of meta data for all workspaces, \n",
                "# making sure maximal 16 (workspaces_per_request) requests are running in parallel\n",
                "\n",
                "df_runs_current = df_runs[df_runs[\"status\"].isin([\"Not Started\", \"Request sent\", \"Running\"])].head(max_parallel_requests)\n",
                "\n",
                "while df_runs_current.shape[0] > 0:\n",
                "    time.sleep(5)\n",
                "    for i, row in df_runs_current.iterrows():\n",
                "        if row[\"status\"] == \"Not Started\":\n",
                "            payload = {}\n",
                "            payload[\"workspaces\"] = row[\"id\"]\n",
                "           # api_uri = f\"/v1.0/myorg/admin/workspaces/getInfo?getArtifactUsers={get_artifact_users}&lineage={lineage}&datasourceDetails={datasource_details}&datasetSchema={dataset_schema}&datasetExpressions={dataset_expressions}\"\n",
                "            powerBIAPIBaseUri = 'https://api.powerbi.com/v1.0/myorg/'\n",
                "            api_uri = f'{powerBIAPIBaseUri}/admin/workspaces/getInfo?getArtifactUsers={get_artifact_users}&lineage={lineage}&datasourceDetails={datasource_details}&datasetSchema={dataset_schema}&datasetExpressions={dataset_expressions}'\n",
                "       \n",
                "          #  response = client.post(api_uri, json = payload)\n",
                "            response = RequestWithRetry(\"post\",api_uri,payload)\n",
                "            \n",
                "            id = pd.json_normalize(response.json())[\"id\"][0]\n",
                "            \n",
                "            df_runs.loc[i, \"status\"] = \"Request sent\"\n",
                "            df_runs.loc[i, \"run_id\"] = id\n",
                "\n",
                "        elif row[\"status\"] in [ \"Request sent\", \"Running\"]:\n",
                "            response = RequestWithRetry(\"get\",\"/v1.0/myorg/admin/workspaces/scanStatus/\" + row[\"run_id\"])\n",
                "\n",
                "            stat = pd.json_normalize(response.json())[\"status\"][0]\n",
                "            df_runs.loc[i, \"status\"] = stat\n",
                "            \n",
                "    df_runs_current = df_runs[df_runs[\"status\"].isin([\"Not Started\", \"Request sent\", \"Running\"])].head(max_parallel_requests)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:25.1875431Z\",\"execution_finish_time\":\"2025-04-04T14:28:36.8087063Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "970e1622-7c8e-452c-9845-b1ec62b81e65"
        },
        {
            "cell_type": "code",
            "source": [
                "df_runs"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:36.8107582Z\",\"execution_finish_time\":\"2025-04-04T14:28:37.1052727Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "52d25e0b-c726-4b44-b9b1-c781b37ee7c5"
        },
        {
            "cell_type": "code",
            "source": [
                "# Iterates through the scan runs and append the result to the 'result' variable\n",
                "# If write_to_files is true the JSON response will also be written into the files section\n",
                "for i, row in df_runs.iterrows():\n",
                "\n",
                "    if row[\"status\"] == \"Succeeded\":\n",
                "            response = RequestWithRetry('get', f\"/v1.0/myorg/admin/workspaces/scanResult/\" + row[\"run_id\"])\n",
                "            print(\"/v1.0/myorg/admin/workspaces/scanResult/\" + row[\"run_id\"])\n",
                "            results.append(response.json())\n",
                "\n",
                "            if write_to_files:\n",
                "                folder_path = mssparkutils.fs.getMountPath('/default') + \"/Files/history/inventory/\" + current_time.strftime(\"%Y/%m/%d\") + \"/\" +  current_time.strftime(\"%H-%M-%S\") + \"/\"\n",
                "                mssparkutils.fs.mkdirs(f\"file://\" +folder_path)\n",
                "\n",
                "                with open(folder_path + row[\"run_id\"] +\".json\", \"w\") as f:\n",
                "                    f.write(json.dumps(response.json()))"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:37.1072043Z\",\"execution_finish_time\":\"2025-04-04T14:28:39.5288161Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "f08955e0-ff38-475a-a06a-90d2858eb170"
        },
        {
            "cell_type": "code",
            "source": [
                "if display_data:\n",
                "    print(len(results))"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:39.5306638Z\",\"execution_finish_time\":\"2025-04-04T14:28:39.8101192Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "c8966320-914a-4dc5-863a-9ae374e58056"
        },
        {
            "cell_type": "markdown",
            "source": [
                "Function to parse result json into needed tables. If object type is not present in tenant, the function will return an empty df instead.\n",
                "The following parameters are expected:\n",
                "- df: Pandas DataFrame to be flattened\n",
                "- parent_id: ID to parent object, to be able to Link the data later on\n",
                "- rename_id (optional): New name of id column"
            ],
            "metadata": {},
            "id": "023a8216-0341-4950-9741-3029c61a0784"
        },
        {
            "cell_type": "code",
            "source": [
                "def get_details(df, parent_id, col, **kwargs ):\n",
                "    try:\n",
                "        rename_id = kwargs.get('rename_id' , None)\n",
                "        df_res = df[[parent_id, col]].explode(col, ignore_index = True)\n",
                "        df_res = df_res[[parent_id]].join(pd.json_normalize(df_res[col]))\n",
                "\n",
                "\n",
                "        # This check has been added in order to make it work for subsets which don't contain an id column\n",
                "        if 'id' in df_res.columns:\n",
                "            df_res = df_res.dropna(subset=['id'])\n",
                "        else:\n",
                "        # In case there is no id column, rows where all other values are empty will be droped\n",
                "            other_cols = df_res.columns.to_list()\n",
                "            other_cols.remove(parent_id)\n",
                "            df_res = df_res.dropna(subset = other_cols, how = 'all' )\n",
                "\n",
                "        if not(rename_id is None):\n",
                "            df_res = df_res.rename(columns = {'id' : rename_id})\n",
                "        \n",
                "        for column in df_res.columns:\n",
                "            if \"mixed\" in pd.api.types.infer_dtype(df_res[column]):\n",
                "                df_res[column] = df_res[column].astype(str)\n",
                "\n",
                "\n",
                "\n",
                "        return df_res\n",
                "    except:\n",
                "        return pd.DataFrame()"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:39.8119575Z\",\"execution_finish_time\":\"2025-04-04T14:28:40.1132024Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "collapsed": false
            },
            "id": "fecf0668-78fb-4832-a0db-2cb3cec05c39"
        },
        {
            "cell_type": "code",
            "source": [
                "def to_upper_if_exists(df, col):\n",
                "    if col in df.columns:\n",
                "        df[col] = df[col].str.upper() \n",
                "    return(df)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:40.1151854Z\",\"execution_finish_time\":\"2025-04-04T14:28:40.4094731Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "e7c074dc-6d16-41d6-b02c-f200bcfa4064"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Extracting results\n",
                "Parse the information from the result into data frames which can be written to lakehouse in later step. \n",
                "\n",
                "In case some object types are not existing in Fabric tenant, this might fail. \n",
                "\n",
                "In this case, the lakehouse tables will be created with an empty row"
            ],
            "metadata": {},
            "id": "5fb6f58b-6a3e-4313-865b-50707cfe221f"
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Workspaces"
            ],
            "metadata": {},
            "id": "298421df-b376-4d61-9dd7-faf31f93f03f"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Workspaces"
            ],
            "metadata": {},
            "id": "da71257d-8ddb-42ad-8686-ad0b870af97e"
        },
        {
            "cell_type": "code",
            "source": [
                "# Workspaces\n",
                "# [Info]: It can be defined statically\n",
                "\n",
                "# Check if workspace has domains\n",
                "workspace_columns = {'':''}\n",
                "if has_tenant_domains:\n",
                "    workspace_columns = {\n",
                "                        'id': 'WorkspaceId',\n",
                "                        'name': 'WorkspaceName',\n",
                "                        'type': 'Type',\n",
                "                        'state': 'State',\n",
                "                        'isOnDedicatedCapacity': 'IsOnDedicatedCapacity',\n",
                "                        'capacityId': 'CapacityId',\n",
                "                        'domainId': 'DomainId',\n",
                "                        'description': 'Description',\n",
                "                        'defaultDatasetStorageFormat': 'StorageFormat'\n",
                "                        }\n",
                "else:\n",
                "    workspace_columns = {\n",
                "                        'id': 'WorkspaceId',\n",
                "                        'name': 'WorkspaceName',\n",
                "                        'type': 'Type',\n",
                "                        'state': 'State',\n",
                "                        'isOnDedicatedCapacity': 'IsOnDedicatedCapacity',\n",
                "                        'capacityId': 'CapacityId',\n",
                "                        'description': 'Description',\n",
                "                        'defaultDatasetStorageFormat': 'StorageFormat'\n",
                "                        }\n",
                "\n",
                "df_workspaces = pd.json_normalize(pd.json_normalize(results).explode(\"workspaces\")[\"workspaces\"])\n",
                "\n",
                "# Rename columns\n",
                "df_workspaces = df_workspaces.rename(columns = workspace_columns)\n",
                "\n",
                "\n",
                "# Change id column values to upper case\n",
                "to_upper_if_exists(df_workspaces, 'WorkspaceId' )\n",
                "to_upper_if_exists(df_workspaces, 'CapacityId' )\n",
                "if has_tenant_domains:\n",
                "    to_upper_if_exists(df_workspaces, 'DomainId' )\n",
                "\n",
                "if display_data:\n",
                "    display(df_workspaces)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:40.4114533Z\",\"execution_finish_time\":\"2025-04-04T14:28:40.7018125Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "collapsed": false
            },
            "id": "ee5f5d54-2b81-4997-a864-664b31dbc5b5"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Workspace Users"
            ],
            "metadata": {},
            "id": "839b1a2a-dcaf-4f4a-9ddf-9cab5af4250d"
        },
        {
            "cell_type": "code",
            "source": [
                "# Workspace Users (within Workspaces)\n",
                "# [Info]: It can be defined statically\n",
                "df_ws_users = get_details(df_workspaces, \"WorkspaceId\", \"users\")\n",
                "\n",
                "if df_ws_users.empty:\n",
                "    print(\"df_ws_users dataframe is empty\")\n",
                "else:\n",
                "    # Remove columns\n",
                "    df_ws_users = df_ws_users.drop(columns=['emailAddress', 'displayName'], errors='ignore')\n",
                "\n",
                "    # Rename columns\n",
                "    df_ws_users = df_ws_users.rename(columns = {\n",
                "                                    'groupUserAccessRight': 'GroupUserAccessRight',\n",
                "                                    'identifier': 'Identifier',\n",
                "                                    'graphId': 'GraphId',\n",
                "                                    'principalType': 'PrincipalType',\n",
                "                                    'userType': 'UserType'\n",
                "                                    }\n",
                "                                )\n",
                "    # Add data to write array\n",
                "    write_list.append({\"df\": df_ws_users, \"name\" : \"workspaces_scanned_users\"})\n",
                "\n",
                "    if display_data:\n",
                "        display(df_ws_users)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:40.7039168Z\",\"execution_finish_time\":\"2025-04-04T14:28:41.0408201Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "1e8e24da-ae29-4e2f-9866-d9b941314d93"
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Power BI artifacts"
            ],
            "metadata": {},
            "id": "0a1ad05d-edeb-4dbc-bd0a-a807bd1f4dd7"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Datamarts"
            ],
            "metadata": {},
            "id": "e1071e4b-6840-49d6-bce6-c9ee42e70772"
        },
        {
            "cell_type": "code",
            "source": [
                "# Datamarts (within Workspaces)\n",
                "\n",
                "try:\n",
                "    df_datamarts = get_details(df_workspaces, \"WorkspaceId\" , \"datamarts\")\n",
                "\n",
                "    if df_datamarts.empty or 'id' not in df_datamarts.columns:\n",
                "        print(\"df_datamarts dataframe is empty\")\n",
                "    else:\n",
                "        # Remove null rows\n",
                "        df_datamarts = df_datamarts[df_datamarts['id'].notna()]\n",
                "\n",
                "        # Remove columns\n",
                "        df_datamarts = df_datamarts.drop(columns=['configuredBy', 'users', 'modifiedBy'], errors='ignore')\n",
                "\n",
                "        # Rename columns\n",
                "        df_datamarts = df_datamarts.rename(columns = {\n",
                "                                        'id': 'DatamartId',\n",
                "                                        'name': 'Name',\n",
                "                                        'configuredById': 'ConfiguredById',\n",
                "                                        'modifiedById': 'ModifiedById',\n",
                "                                        'modifiedDateTime': 'ModifiedDateTime',\n",
                "                                        'type': 'Type'\n",
                "                                        } , errors='ignore'\n",
                "                                    )\n",
                "\n",
                "        # Change id column values to upper case\n",
                "        to_upper_if_exists(df_datamarts, 'DatamartId' )\n",
                "        to_upper_if_exists(df_datamarts, 'ConfiguredById' ) \n",
                "        to_upper_if_exists(df_datamarts, 'ModifiedById' )\n",
                "\n",
                "        # Add calculated columns\n",
                "        df_datamarts['ModifiedDateTime'] = df_datamarts['ModifiedDateTime'].str.slice(0, 10)\n",
                "\n",
                "        # Add data to write array\n",
                "        write_list.append({\"df\": df_datamarts, \"name\" : \"datamarts\"})\n",
                "\n",
                "        if display_data:\n",
                "            display(df_datamarts)\n",
                "            \n",
                "except Exception as ex:\n",
                "    print(ex)\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:41.0429262Z\",\"execution_finish_time\":\"2025-04-04T14:28:41.3733628Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "f26c0ff9-3bd0-4171-afa5-ab49d77e9def"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Semantic Models"
            ],
            "metadata": {},
            "id": "12b0c4d1-5688-43de-98c1-285b107de064"
        },
        {
            "cell_type": "code",
            "source": [
                "# Semantic Models (within Workspaces)\n",
                "# [Info]: It must have a partially dynamic structure\n",
                "\n",
                "try:\n",
                "    df_semantic_models = get_details(df_workspaces, \"WorkspaceId\" , \"datasets\")\n",
                "\n",
                "    if df_semantic_models.empty or 'id' not in df_semantic_models.columns:\n",
                "        print(\"df_semantic_models dataframe is empty\")\n",
                "    else:\n",
                "        # Remove null rows\n",
                "        df_semantic_models = df_semantic_models[df_semantic_models['id'].notna()]\n",
                "\n",
                "        # Remove columns\n",
                "        df_semantic_models = df_semantic_models.drop(columns=\n",
                "                                    ['configuredBy', 'users', 'tables', 'datasourceUsages', \n",
                "                                    'relations'], errors='ignore')\n",
                "\n",
                "        try:\n",
                "            # Remove columns\n",
                "            df_semantic_models = df_semantic_models.drop(columns=\n",
                "                                    ['refreshSchedule.days','refreshSchedule.times'], errors='ignore')\n",
                "        except:\n",
                "            print(\"No refreshSchedule columns were found.\")\n",
                "\n",
                "        try:\n",
                "            # Remove columns\n",
                "            df_semantic_models = df_semantic_models.drop(columns=\n",
                "                                    ['directQueryRefreshSchedule.days', 'directQueryRefreshSchedule.times'], errors='ignore')\n",
                "        except:\n",
                "            print(\"No directQuerRefreshSchedule columns were found.\")\n",
                "\n",
                "        try:\n",
                "            # Remove columns\n",
                "            df_semantic_models = df_semantic_models.drop(columns= ['upstreamDatasets', 'upstreamDatamarts'], errors='ignore')\n",
                "\n",
                "        except:\n",
                "            print(\"No upstream(Dataset/Datamart) columns were found.\")\n",
                "\n",
                "        try:\n",
                "            # Remove columns\n",
                "            df_semantic_models = df_semantic_models.drop(columns= ['misconfiguredDatasourceUsages'], errors='ignore')\n",
                "        except:\n",
                "            print(\"No misconfiguredDatasourceUsages columns were found.\")\n",
                "\n",
                "        # Rename columns\n",
                "        df_semantic_models = df_semantic_models.rename(columns = {\n",
                "                                        'id': 'SemanticModelId',\n",
                "                                        'name': 'Name',\n",
                "                                        'configuredById': 'ConfiguredById',\n",
                "                                        'createdDate': 'CreatedDateTime',\n",
                "                                        'targetStorageMode': 'StorageMode',\n",
                "                                        'contentProviderType': 'ContentProviderType'\n",
                "                                        }, errors='ignore'\n",
                "                                    )\n",
                "\n",
                "\n",
                "        # Change id column values to upper case\n",
                "        to_upper_if_exists(df_semantic_models, 'SemanticModelId' ) \n",
                "        to_upper_if_exists(df_semantic_models, 'ConfiguredById' )\n",
                "\n",
                "        # Add calculated columns\n",
                "        df_semantic_models['CreatedDate'] = df_semantic_models['CreatedDateTime'].str.slice(0, 10)\n",
                "        \n",
                "        # Add data to write array\n",
                "        write_list.append({\"df\": df_semantic_models, \"name\" : \"semantic_models\"})\n",
                "\n",
                "        if display_data:\n",
                "            display(df_semantic_models)\n",
                "            \n",
                "except Exception as ex:\n",
                "    print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:41.3755284Z\",\"execution_finish_time\":\"2025-04-04T14:28:41.6846153Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "b784bb26-a22d-4f09-9c2f-1c74469d2bc3"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Dataflows"
            ],
            "metadata": {},
            "id": "00e900f7-81b0-41bd-a6c2-59fd7bf2705f"
        },
        {
            "cell_type": "code",
            "source": [
                "# Dataflows (within Workspaces)\n",
                "# [Info]: It can be defined statically\n",
                "\n",
                "try:\n",
                "    df_dataflows = get_details(df_workspaces, \"WorkspaceId\" , \"dataflows\")\n",
                "\n",
                "    if df_dataflows.empty or 'objectId' not in df_dataflows.columns:\n",
                "        print(\"df_dataflows dataframe is empty\")\n",
                "    else:\n",
                "        # Remove null rows\n",
                "        df_dataflows = df_dataflows[df_dataflows['objectId'].notna()]\n",
                "\n",
                "        # Remove columns\n",
                "        df_dataflows = df_dataflows.drop(columns=['configuredBy', 'modifiedBy', 'users'], errors='ignore')\n",
                "        try:\n",
                "            df_reports = df_reports.drop(columns=['datasourceUsages'], errors='ignore')\n",
                "        except:\n",
                "            print(\"No datasourceUsages\")\n",
                "\n",
                "        # Rename columns\n",
                "        df_dataflows = df_dataflows.rename(columns = {\n",
                "                                        'objectId': 'DataflowId',\n",
                "                                        'name': 'Name',\n",
                "                                        'modifiedDateTime': 'ModifiedDateTime',\n",
                "                                        'generation': 'Generation'\n",
                "                                        }, errors='ignore'\n",
                "                                    )\n",
                "\n",
                "        # Change id column values to upper case\n",
                "        to_upper_if_exists(df_dataflows, 'DataflowId' )\n",
                "\n",
                "        # Add data to write array\n",
                "        write_list.append({\"df\": df_dataflows, \"name\" : \"dataflows\"})\n",
                "\n",
                "        if display_data:\n",
                "            display(df_dataflows)\n",
                "            \n",
                "except Exception as ex:\n",
                "    print(ex)\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:41.6868699Z\",\"execution_finish_time\":\"2025-04-04T14:28:41.9884463Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "5271826f-aec2-4974-aaef-73dedd67c713"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Dataflow Sources"
            ],
            "metadata": {},
            "id": "945027fc-8f16-46d0-ae11-3ac620628919"
        },
        {
            "cell_type": "code",
            "source": [
                "# Dataflows (within Workspaces -> Dataflows)\n",
                "# [Info]: It can be defined statically\n",
                "\n",
                "try:\n",
                "    df_dataflow_base = get_details(df_workspaces, \"WorkspaceId\", \"dataflows\")\n",
                "    \n",
                "    # Remove null rows\n",
                "    df_dataflow_du = df_dataflow_base[df_dataflow_base['objectId'].notna()]\n",
                "\n",
                "    # Expand datasourceUsages\n",
                "    df_dataflow_du = get_details(df_dataflow_du, \"objectId\", \"datasourceUsages\")\n",
                "\n",
                "    # Remove null rows\n",
                "    df_dataflow_du = df_dataflow_du[df_dataflow_du['datasourceInstanceId'].notna()]\n",
                "\n",
                "    # Rename column\n",
                "    df_dataflow_du = df_dataflow_du.rename(columns = {'objectId': 'DataflowId'}, errors='ignore')\n",
                "\n",
                "    # Change id column values to upper case\n",
                "    to_upper_if_exists(df_dataflow_du, 'DataflowId')\n",
                "    to_upper_if_exists(df_dataflow_du, 'datasourceInstanceId')\n",
                "\n",
                "    # Add data to write array\n",
                "    write_list.append({\"df\": df_dataflow_du, \"name\" : \"dataflow_datasources\"})\n",
                "\n",
                "    if display_data:\n",
                "        display(df_dataflow_du)\n",
                "\n",
                "    del df_dataflow_base\n",
                "    del df_dataflow_du\n",
                "\n",
                "except Exception as ex:\n",
                "    print(ex)\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:41.9906012Z\",\"execution_finish_time\":\"2025-04-04T14:28:42.2766249Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "8ca13e56-8051-4d87-a100-44c7f3d55b29"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Reports"
            ],
            "metadata": {},
            "id": "9d9183da-f2d9-4cec-b448-3fba60dac7de"
        },
        {
            "cell_type": "code",
            "source": [
                "# Reports (within Workspaces)\n",
                "# [Info]: It can be defined statically\n",
                "\n",
                "try:\n",
                "    df_reports = get_details(df_workspaces, \"WorkspaceId\" , \"reports\")\n",
                "\n",
                "    if df_reports.empty or 'id' not in df_reports.columns:\n",
                "        print(\"DF is empty\")\n",
                "    else:\n",
                "        # Remove null rows\n",
                "        df_reports = df_reports[df_reports['id'].notna()]\n",
                "\n",
                "        # Remove columns\n",
                "        df_reports = df_reports.drop(columns=['createdBy', 'modifiedBy', 'relations'], errors='ignore')\n",
                "        try:\n",
                "            df_reports = df_reports.drop(columns=['datasourceUsages'], errors='ignore')\n",
                "        except:\n",
                "            print(\"No datasourceUsages\")\n",
                "\n",
                "        # Rename columns\n",
                "        df_reports = df_reports.rename(columns = {\n",
                "                                        'id': 'ReportId',\n",
                "                                        'datasetId': 'SemanticModelId',\n",
                "                                        'datasetWorkspaceId': 'SemanticModelWorkspaceId',\n",
                "                                        'reportType': 'ReportType',\n",
                "                                        'name': 'Name',\n",
                "                                        'createdDateTime': 'CreatedDateTime',\n",
                "                                        'modifiedDateTime': 'ModifiedDateTime',\n",
                "                                        'modifiedById': 'ModifiedById',\n",
                "                                        'createdById': 'CreatedById',\n",
                "                                        'originalReportObjectId': 'OriginalReportObjectId',\n",
                "                                        'appId': 'AppId',\n",
                "                                        }\n",
                "                                    )\n",
                "\n",
                "        # Change id column values to upper case\n",
                "        to_upper_if_exists(df_reports, 'ReportId')\n",
                "        to_upper_if_exists(df_reports, 'SemanticModelId')\n",
                "        to_upper_if_exists(df_reports, 'ModifiedById')\n",
                "        to_upper_if_exists(df_reports, 'CreatedById')\n",
                "        to_upper_if_exists(df_reports, 'SemanticModelWorkspaceId') \n",
                "        to_upper_if_exists(df_reports, 'OriginalReportObjectId')\n",
                "        to_upper_if_exists(df_reports, 'AppId')\n",
                "\n",
                "        # Add data to write array\n",
                "        write_list.append({\"df\": df_reports, \"name\" : \"reports\"})\n",
                "\n",
                "        if display_data:\n",
                "            display(df_reports)\n",
                "            \n",
                "except Exception as ex:\n",
                "    print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:42.2788783Z\",\"execution_finish_time\":\"2025-04-04T14:28:42.6287542Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "7ba42d2d-a4cb-405b-a6b5-d5d0a4706296"
        },
        {
            "cell_type": "code",
            "source": [
                "# Report datasourceUsages (within Workspaces -> Reports)\n",
                "# [Info]: It can be defined statically\n",
                "\n",
                "try:\n",
                "    df_report_base = get_details(df_workspaces, \"WorkspaceId\", \"reports\")\n",
                "    \n",
                "    # Remove null rows\n",
                "    df_report_du = df_report_base[df_report_base['id'].notna()]\n",
                "\n",
                "    # Expand datasourceUsages\n",
                "    df_report_du = get_details(df_report_du, \"id\", \"datasourceUsages\")\n",
                "\n",
                "    # Remove null rows\n",
                "    df_report_du = df_report_du[df_report_du['datasourceInstanceId'].notna()]\n",
                "\n",
                "    # Rename column\n",
                "    df_report_du = df_report_du.rename(columns = {'id': 'ReportId'}, errors='ignore')\n",
                "\n",
                "    # Change id column values to upper case\n",
                "    to_upper_if_exists(df_report_du, 'ReportId') \n",
                "    to_upper_if_exists(df_report_du, 'datasourceInstanceId')\n",
                "\n",
                "    # Add data to write array\n",
                "    write_list.append({\"df\": df_report_du, \"name\" : \"report_datasources\"})\n",
                "\n",
                "    if display_data:\n",
                "        display(df_report_du)\n",
                "\n",
                "    del df_report_base\n",
                "    del df_report_du\n",
                "\n",
                "except Exception as ex:\n",
                "    print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:42.6307445Z\",\"execution_finish_time\":\"2025-04-04T14:28:42.9185865Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "6f2e0e8f-3611-41b5-8402-ffa8a86ba399"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Dashboards"
            ],
            "metadata": {},
            "id": "fe4aeaa6-c1a4-4ec5-8833-9288c9c62405"
        },
        {
            "cell_type": "code",
            "source": [
                "# Dashboards (within Workspaces)\n",
                "# [Info]: It can be defined statically\n",
                "\n",
                "try:\n",
                "    df_dashboards = get_details(df_workspaces, \"WorkspaceId\" , \"dashboards\")\n",
                "\n",
                "    if df_dashboards.empty or 'id' not in df_dashboards.columns:\n",
                "        print(\"df_dashboards dataframe is empty\")\n",
                "    else:\n",
                "        # Remove null rows\n",
                "        df_dashboards = df_dashboards[df_dashboards['id'].notna()]\n",
                "\n",
                "        # Remove columns\n",
                "        df_dashboards = df_dashboards.drop(columns=['users', 'isReadOnly'], errors='ignore')\n",
                "\n",
                "        # Rename columns\n",
                "        df_dashboards = df_dashboards.rename(columns = {\n",
                "                                        'id': 'DashboardId',\n",
                "                                        'displayName': 'Name'\n",
                "                                        }, errors='ignore'\n",
                "                                    )\n",
                "\n",
                "\n",
                "        # Change id column values to upper case\n",
                "        to_upper_if_exists(df_dashboards, 'DashboardId')\n",
                "\n",
                "        # Add data to write array\n",
                "        write_list.append({\"df\": df_dashboards, \"name\" : \"dashboards\"})\n",
                "\n",
                "        if display_data:\n",
                "            display(df_dashboards)\n",
                "\n",
                "except Exception as ex:\n",
                "    print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:42.9207035Z\",\"execution_finish_time\":\"2025-04-04T14:28:43.2139347Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "3af5f408-654d-4c4a-be44-e91b02a8716d"
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Fabric artifacts"
            ],
            "metadata": {},
            "id": "39c592d1-8904-4041-8e0e-29478f559cc2"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Eventhouses"
            ],
            "metadata": {},
            "id": "59ffb364-0c62-45ee-9f64-f40ec3381223"
        },
        {
            "cell_type": "code",
            "source": [
                "# Eventhouses (within Workspaces)\n",
                "\n",
                "if extract_powerbi_artifacts_only == False:\n",
                "    try:\n",
                "        df_eventhouses = get_details(df_workspaces, \"WorkspaceId\" , \"Eventhouse\")\n",
                "\n",
                "        if df_eventhouses.empty or 'id' not in df_eventhouses.columns:\n",
                "            print(\"df_eventhouses dataframe is empty\")\n",
                "        else:\n",
                "            # Remove null rows\n",
                "            df_eventhouses = df_eventhouses[df_eventhouses['id'].notna()]\n",
                "\n",
                "            # Remove columns\n",
                "            df_eventhouses = df_eventhouses.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
                "\n",
                "            # Rename columns\n",
                "            df_eventhouses = df_eventhouses.rename(columns = {\n",
                "                                            'id': 'EventhouseId',\n",
                "                                            'name': 'Name',\n",
                "                                            'description': 'Description',\n",
                "                                            'state': 'State',\n",
                "                                            'createdById': 'CreatedById',\n",
                "                                            'modifiedById': 'ModifiedById',\n",
                "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
                "                                            'createdDate': 'CreatedDateTime'\n",
                "                                            }, errors='ignore'\n",
                "                                        )\n",
                "\n",
                "\n",
                "            # Change id column values to upper case\n",
                "            to_upper_if_exists(df_eventhouses, 'EventhouseId')\n",
                "            to_upper_if_exists(df_eventhouses, 'CreatedById')\n",
                "            to_upper_if_exists(df_eventhouses, 'ModifiedById') \n",
                "\n",
                "            # Add calculated columns\n",
                "            df_eventhouses['CreatedDate'] = df_eventhouses['CreatedDateTime'].str.slice(0, 10)\n",
                "            df_eventhouses['LastUpdatedDate'] = df_eventhouses['LastUpdatedDateTime'].str.slice(0, 10)\n",
                "            \n",
                "            # Add data to write array\n",
                "            write_list.append({\"df\": df_eventhouses, \"name\" : \"eventhouses\"})\n",
                "\n",
                "            if display_data:\n",
                "                display(df_eventhouses)\n",
                "    except Exception as ex:\n",
                "        print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:43.2159223Z\",\"execution_finish_time\":\"2025-04-04T14:28:43.4916284Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "b76c1b92-edae-472b-875a-2291ac52ebc5"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### KQL Databases"
            ],
            "metadata": {},
            "id": "225735ca-bb98-4164-a461-1756302f2c73"
        },
        {
            "cell_type": "code",
            "source": [
                "# KQL Databases (within Workspaces)\n",
                "\n",
                "if extract_powerbi_artifacts_only == False:\n",
                "    try:\n",
                "        df_kql = get_details(df_workspaces, \"WorkspaceId\" , \"KQLDatabase\")\n",
                "\n",
                "        if df_kql.empty or 'id' not in df_kql.columns:\n",
                "            print(\"df_kql dataframe is empty\")\n",
                "        else:\n",
                "            # Remove null rows\n",
                "            df_kql = df_kql[df_kql['id'].notna()]\n",
                "\n",
                "            # Remove columns\n",
                "            df_kql = df_kql.drop(columns=['users'], errors='ignore')\n",
                "\n",
                "            # Rename columns\n",
                "            df_kql = df_kql.rename(columns = {\n",
                "                                            'id': 'KQLDatabaseId',\n",
                "                                            'name': 'Name',\n",
                "                                            'description': 'Description',\n",
                "                                            'state': 'State'\n",
                "                                            }, errors='ignore'\n",
                "                                        )\n",
                "\n",
                "\n",
                "            # Change id column values to upper case\n",
                "            to_upper_if_exists(df_kql, 'KQLDatabaseId')  \n",
                "\n",
                "            # Add data to write array\n",
                "            write_list.append({\"df\": df_kql, \"name\" : \"kql_databases\"})\n",
                "\n",
                "            if display_data:\n",
                "                display(df_kql)\n",
                "    except Exception as ex:\n",
                "        print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:43.4938619Z\",\"execution_finish_time\":\"2025-04-04T14:28:43.810283Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "1f4b16cf-11fa-4a2f-b1ff-8b3c2e5d591d"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Lakehouses"
            ],
            "metadata": {},
            "id": "8ce75fdb-9f71-4478-bcdf-5a8bb7445932"
        },
        {
            "cell_type": "code",
            "source": [
                "# Lakehouses (within Workspaces)\n",
                "\n",
                "if extract_powerbi_artifacts_only == False:\n",
                "    try:\n",
                "        df_lakehouses = get_details(df_workspaces, \"WorkspaceId\" , \"Lakehouse\")\n",
                "\n",
                "        if df_lakehouses.empty or 'id' not in df_lakehouses.columns:\n",
                "            print(\"df_lakehouses dataframe is empty\")\n",
                "        else:\n",
                "            # Remove null rows\n",
                "            df_lakehouses = df_lakehouses[df_lakehouses['id'].notna()]\n",
                "\n",
                "            # Remove columns\n",
                "            df_lakehouses = df_lakehouses.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
                "\n",
                "            # Rename columns\n",
                "            df_lakehouses = df_lakehouses.rename(columns = {\n",
                "                                            'id': 'LakehouseId',\n",
                "                                            'name': 'Name',\n",
                "                                            'description': 'Description',\n",
                "                                            'state': 'State',\n",
                "                                            'createdById': 'CreatedById',\n",
                "                                            'modifiedById': 'ModifiedById',\n",
                "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
                "                                            'createdDate': 'CreatedDateTime'\n",
                "                                            }, errors='ignore'\n",
                "                                        )\n",
                "\n",
                "\n",
                "            # Change id column values to upper case\n",
                "            to_upper_if_exists(df_lakehouses, 'LakehouseId')\n",
                "            to_upper_if_exists(df_lakehouses, 'CreatedById') \n",
                "            to_upper_if_exists(df_lakehouses, 'ModifiedById') \n",
                "\n",
                "            # Add calculated columns\n",
                "            df_lakehouses['CreatedDate'] = df_lakehouses['CreatedDateTime'].str.slice(0, 10)\n",
                "            df_lakehouses['LastUpdatedDate'] = df_lakehouses['LastUpdatedDateTime'].str.slice(0, 10)\n",
                "\n",
                "            # Add data to write array\n",
                "            write_list.append({\"df\": df_lakehouses, \"name\" : \"lakehouses\"})\n",
                "\n",
                "            if display_data:\n",
                "                display(df_lakehouses)\n",
                "    except Exception as ex:\n",
                "        print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:43.8125121Z\",\"execution_finish_time\":\"2025-04-04T14:28:44.0971821Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "8d759825-699e-444b-88fd-384c9cc52e15"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Warehouses"
            ],
            "metadata": {},
            "id": "5fad04d3-165b-4195-b7dd-2ea76c791b83"
        },
        {
            "cell_type": "code",
            "source": [
                "# Warehouses (within Workspaces)\n",
                "\n",
                "if extract_powerbi_artifacts_only == False:\n",
                "    try:\n",
                "        df_warehouses = get_details(df_workspaces, \"WorkspaceId\" , \"warehouses\")\n",
                "\n",
                "        if df_warehouses.empty or 'id' not in df_warehouses.columns:\n",
                "            print(\"df_warehouses dataframe is empty\")\n",
                "        else:\n",
                "            # Remove null rows\n",
                "            df_warehouses = df_warehouses[df_warehouses['id'].notna()]\n",
                "\n",
                "            # Remove columns\n",
                "            df_warehouses = df_warehouses.drop(columns=['users', 'configuredBy', 'modifiedBy'], errors='ignore')\n",
                "\n",
                "            # Rename columns\n",
                "            df_warehouses = df_warehouses.rename(columns = {\n",
                "                                            'id': 'WarehouseId',\n",
                "                                            'name': 'Name',\n",
                "                                            'description': 'Description',\n",
                "                                            'state': 'State',\n",
                "                                            'configuredById': 'ConfiguredById',\n",
                "                                            'modifiedById': 'ModifiedById',\n",
                "                                            'modifiedDateTime': 'ModifiedDateTime'\n",
                "                                            }, errors='ignore'\n",
                "                                        )\n",
                "\n",
                "\n",
                "            # Change id column values to upper case\n",
                "            to_upper_if_exists(df_warehouses, 'WarehouseId') \n",
                "            to_upper_if_exists(df_warehouses, 'ConfiguredById') \n",
                "\n",
                "            # Add calculated columns\n",
                "            df_warehouses['ModifiedDate'] = df_warehouses['ModifiedDateTime'].str.slice(0, 10)\n",
                "\n",
                "            # Add data to write array\n",
                "            write_list.append({\"df\": df_warehouses, \"name\" : \"warehouses\"})\n",
                "\n",
                "            if display_data:\n",
                "                display(df_warehouses)\n",
                "    except Exception as ex:\n",
                "        print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:44.0990691Z\",\"execution_finish_time\":\"2025-04-04T14:28:44.4330425Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "b43bf5dc-6416-4fd1-81a0-0643b704a0b4"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Eventstream"
            ],
            "metadata": {},
            "id": "07d27dc8-a5e7-4fd7-a7d5-3c5212ba36a2"
        },
        {
            "cell_type": "code",
            "source": [
                "# Eventstreams (within Workspaces)\n",
                "\n",
                "if extract_powerbi_artifacts_only == False:\n",
                "    try:\n",
                "        df_eventstreams = get_details(df_workspaces, \"WorkspaceId\" , \"Eventstream\")\n",
                "\n",
                "        if df_eventstreams.empty or 'id' not in df_eventstreams.columns:\n",
                "            print(\"df_eventstreams dataframe  is empty\")\n",
                "        else:\n",
                "            # Remove null rows\n",
                "            df_eventstreams = df_eventstreams[df_eventstreams['id'].notna()]\n",
                "\n",
                "            # Remove columns\n",
                "            df_eventstreams = df_eventstreams.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
                "\n",
                "            # Rename columns\n",
                "            df_eventstreams = df_eventstreams.rename(columns = {\n",
                "                                            'id': 'EventstreamId',\n",
                "                                            'name': 'Name',\n",
                "                                            'description': 'Description',\n",
                "                                            'state': 'State',\n",
                "                                            'createdById': 'CreatedById',\n",
                "                                            'modifiedById': 'ModifiedById',\n",
                "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
                "                                            'createdDate': 'CreatedDateTime'\n",
                "                                            }, errors='ignore'\n",
                "                                        )\n",
                "\n",
                "\n",
                "            # Change id column values to upper case\n",
                "            to_upper_if_exists(df_eventstreams, 'EventstreamId')\n",
                "            to_upper_if_exists(df_eventstreams, 'CreatedById')  \n",
                "            to_upper_if_exists(df_eventstreams, 'ModifiedById')\n",
                "\n",
                "            # Add calculated columns\n",
                "            df_eventstreams['CreatedDate'] = df_eventstreams['CreatedDateTime'].str.slice(0, 10)\n",
                "            df_eventstreams['LastUpdatedDate'] = df_eventstreams['LastUpdatedDateTime'].str.slice(0, 10)\n",
                "\n",
                "            # Add data to write array\n",
                "            write_list.append({\"df\": df_eventstreams, \"name\" : \"eventstreams\"})\n",
                "\n",
                "            if display_data:\n",
                "                display(df_eventstreams)\n",
                "    except Exception as ex:\n",
                "        print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:44.4355294Z\",\"execution_finish_time\":\"2025-04-04T14:28:44.7340286Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "83e9b8c7-2c6f-455f-91d0-7e4930a75c6f"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Data Pipelines"
            ],
            "metadata": {},
            "id": "ac61e6d3-74d2-4956-b033-1670aeeadbf4"
        },
        {
            "cell_type": "code",
            "source": [
                "# Data Pipelines (within Workspaces)\n",
                "\n",
                "if extract_powerbi_artifacts_only == False:\n",
                "    try:\n",
                "        df_pipelines = get_details(df_workspaces, \"WorkspaceId\" , \"DataPipeline\")\n",
                "\n",
                "        if df_pipelines.empty or 'id' not in df_pipelines.columns:\n",
                "            print(\"df_pipelines dataframe is empty\")\n",
                "        else:\n",
                "            # Remove null rows\n",
                "            df_pipelines = df_pipelines[df_pipelines['id'].notna()]\n",
                "\n",
                "            # Remove columns\n",
                "            df_pipelines = df_pipelines.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
                "\n",
                "            # Rename columns\n",
                "            df_pipelines = df_pipelines.rename(columns = {\n",
                "                                            'id': 'PipelineId',\n",
                "                                            'name': 'Name',\n",
                "                                            'description': 'Description',\n",
                "                                            'state': 'State',\n",
                "                                            'createdById': 'CreatedById',\n",
                "                                            'modifiedById': 'ModifiedById',\n",
                "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
                "                                            'createdDate': 'CreatedDateTime'\n",
                "                                            }, errors='ignore'\n",
                "                                        )\n",
                "\n",
                "\n",
                "            # Change id column values to upper case\n",
                "            to_upper_if_exists(df_pipelines, 'PipelineId')\n",
                "            to_upper_if_exists(df_pipelines, 'CreatedById') \n",
                "            to_upper_if_exists(df_pipelines, 'ModifiedById') \n",
                "\n",
                "            # Add calculated columns\n",
                "            df_pipelines['CreatedDate'] = df_pipelines['CreatedDateTime'].str.slice(0, 10)\n",
                "            df_pipelines['LastUpdatedDate'] = df_pipelines['LastUpdatedDateTime'].str.slice(0, 10)\n",
                "\n",
                "            # Add data to write array\n",
                "            write_list.append({\"df\": df_pipelines, \"name\" : \"pipelines\"})\n",
                "\n",
                "            if display_data:\n",
                "                display(df_pipelines)\n",
                "    except Exception as ex:\n",
                "        print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:44.736151Z\",\"execution_finish_time\":\"2025-04-04T14:28:45.0527044Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "aa3708c3-666a-4458-82eb-49179deff619"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Notebooks"
            ],
            "metadata": {},
            "id": "fa059aec-0bad-45fd-9903-409042aed3d2"
        },
        {
            "cell_type": "code",
            "source": [
                "# Notebooks (within Workspaces)\n",
                "# [Info]: It must have a partially dynamic structure\n",
                "\n",
                "if extract_powerbi_artifacts_only == False:\n",
                "    try:\n",
                "        df_notebooks = get_details(df_workspaces, \"WorkspaceId\" , \"Notebook\")\n",
                "\n",
                "        if df_notebooks.empty or 'id' not in df_notebooks.columns:\n",
                "            print(\"df_notebooks dataframe is empty\")\n",
                "        else:\n",
                "            # Remove null rows\n",
                "            df_notebooks = df_notebooks[df_notebooks['id'].notna()]\n",
                "\n",
                "            # Remove columns\n",
                "            df_notebooks = df_notebooks.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
                "\n",
                "            # Rename columns\n",
                "            df_notebooks = df_notebooks.rename(columns = {\n",
                "                                            'id': 'NotebookId',\n",
                "                                            'name': 'Name',\n",
                "                                            'description': 'Description',\n",
                "                                            'state': 'State',\n",
                "                                            'createdById': 'CreatedById',\n",
                "                                            'modifiedById': 'ModifiedById',\n",
                "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
                "                                            'createdDate': 'CreatedDateTime'\n",
                "                                            }, errors='ignore'\n",
                "                                        )\n",
                "\n",
                "\n",
                "            # Change id column values to upper case\n",
                "            to_upper_if_exists(df_notebooks, 'NotebookId')  \n",
                "            to_upper_if_exists(df_notebooks, 'CreatedById')  \n",
                "            to_upper_if_exists(df_notebooks, 'ModifiedById') \n",
                "\n",
                "            # Add calculated columns\n",
                "            df_notebooks['CreatedDate'] = df_notebooks['CreatedDateTime'].str.slice(0, 10)\n",
                "            df_notebooks['LastUpdatedDate'] = df_notebooks['LastUpdatedDateTime'].str.slice(0, 10)\n",
                "\n",
                "            # Add data to write array\n",
                "            write_list.append({\"df\": df_notebooks, \"name\" : \"notebooks\"})\n",
                "\n",
                "            if display_data:\n",
                "                display(df_notebooks)\n",
                "    except Exception as ex:\n",
                "        print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:45.0548956Z\",\"execution_finish_time\":\"2025-04-04T14:28:45.3891527Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "aa4308c5-5115-4840-8594-28b00b7fa9ab"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Environments"
            ],
            "metadata": {},
            "id": "b72a4dcc-1b65-4a5d-9166-8a97a2450d4d"
        },
        {
            "cell_type": "code",
            "source": [
                "# Environments (within Workspaces)\n",
                "\n",
                "if extract_powerbi_artifacts_only == False:\n",
                "    try:\n",
                "        df_env = get_details(df_workspaces, \"WorkspaceId\" , \"Environment\")\n",
                "\n",
                "        if df_env.empty or 'id' not in df_env.columns:\n",
                "            print(\"df_env dataframe is empty\")\n",
                "        else:\n",
                "            # Remove null rows\n",
                "            df_env = df_env[df_env['id'].notna()]\n",
                "\n",
                "            # Remove columns\n",
                "            df_env = df_env.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
                "\n",
                "            # Rename columns\n",
                "            df_env = df_env.rename(columns = {\n",
                "                                            'id': 'EnvironmentId',\n",
                "                                            'name': 'Name',\n",
                "                                            'description': 'Description',\n",
                "                                            'state': 'State',\n",
                "                                            'createdById': 'CreatedById',\n",
                "                                            'modifiedById': 'ModifiedById',\n",
                "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
                "                                            'createdDate': 'CreatedDateTime'\n",
                "                                            }, errors='ignore'\n",
                "                                        )\n",
                "\n",
                "\n",
                "            # Change id column values to upper case\n",
                "            to_upper_if_exists(df_env, 'EnvironmentId') \n",
                "            to_upper_if_exists(df_env, 'CreatedById') \n",
                "            to_upper_if_exists(df_env, 'ModifiedById')  \n",
                "\n",
                "            # Add calculated columns\n",
                "            df_env['CreatedDate'] = df_env['CreatedDateTime'].str.slice(0, 10)\n",
                "            df_env['LastUpdatedDate'] = df_env['LastUpdatedDateTime'].str.slice(0, 10)\n",
                "\n",
                "            # Add data to write array\n",
                "            write_list.append({\"df\": df_env, \"name\" : \"environments\"})\n",
                "\n",
                "            if display_data:\n",
                "                display(df_env)\n",
                "    except Exception as ex:\n",
                "        print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:45.3915239Z\",\"execution_finish_time\":\"2025-04-04T14:28:45.6851433Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "87f73940-2ff5-4a90-a8d0-a5bc51108385"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### Reflex"
            ],
            "metadata": {},
            "id": "ad0d0bf4-1eb1-4be7-8c5a-b4688a7e71fb"
        },
        {
            "cell_type": "code",
            "source": [
                "# Reflex (within Workspaces)\n",
                "\n",
                "if extract_powerbi_artifacts_only == False:\n",
                "    try:\n",
                "        df_reflex = get_details(df_workspaces, \"WorkspaceId\" , \"Reflex\")\n",
                "\n",
                "        if df_reflex.empty or 'id' not in df_reflex.columns:\n",
                "            print(\"df_reflex dataframe is empty\")\n",
                "        else:\n",
                "            # Remove null rows\n",
                "            df_reflex = df_reflex[df_reflex['id'].notna()]\n",
                "\n",
                "            # Remove columns\n",
                "            df_reflex = df_reflex.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
                "\n",
                "            # Rename columns\n",
                "            df_reflex = df_reflex.rename(columns = {\n",
                "                                            'id': 'ReflexId',\n",
                "                                            'name': 'Name',\n",
                "                                            'description': 'Description',\n",
                "                                            'state': 'State',\n",
                "                                            'createdById': 'CreatedById',\n",
                "                                            'modifiedById': 'ModifiedById',\n",
                "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
                "                                            'createdDate': 'CreatedDateTime'\n",
                "                                            }, errors='ignore'\n",
                "                                        )\n",
                "\n",
                "\n",
                "            # Change id column values to upper case \n",
                "            to_upper_if_exists(df_reflex, 'ReflexId')  \n",
                "            to_upper_if_exists(df_reflex, 'CreatedById')   \n",
                "            to_upper_if_exists(df_reflex, 'ModifiedById')  \n",
                "\n",
                "            # Add calculated columns\n",
                "            df_reflex['CreatedDate'] = df_reflex['CreatedDateTime'].str.slice(0, 10)\n",
                "            df_reflex['LastUpdatedDate'] = df_reflex['LastUpdatedDateTime'].str.slice(0, 10)\n",
                "\n",
                "            # Add data to write array\n",
                "            write_list.append({\"df\": df_reflex, \"name\" : \"reflexes\"})\n",
                "\n",
                "            if display_data:\n",
                "                display(df_reflex)\n",
                "    except Exception as ex:\n",
                "        print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:45.6870475Z\",\"execution_finish_time\":\"2025-04-04T14:28:45.9702617Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "2ed9c2bd-5a8a-43c4-9dd1-a8220252d364"
        },
        {
            "cell_type": "markdown",
            "source": [
                "###### ML-Models"
            ],
            "metadata": {},
            "id": "2f6a49f9-e865-46fb-a26a-b46b36cabe6c"
        },
        {
            "cell_type": "code",
            "source": [
                "# ML-Models (within Workspaces)\n",
                "\n",
                "if extract_powerbi_artifacts_only == False:\n",
                "    try:\n",
                "        df_mlm = get_details(df_workspaces, \"WorkspaceId\" , \"MLModel\")\n",
                "\n",
                "        if df_mlm.empty or 'id' not in df_mlm.columns:\n",
                "            print(\"df_mlm dataframe is empty\")\n",
                "        else:\n",
                "            # Remove null rows\n",
                "            df_mlm = df_mlm[df_mlm['id'].notna()]\n",
                "\n",
                "            # Remove columns\n",
                "            df_mlm = df_mlm.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
                "\n",
                "            # Rename columns\n",
                "            df_mlm = df_mlm.rename(columns = {\n",
                "                                            'id': 'MLModelId',\n",
                "                                            'name': 'Name',\n",
                "                                            'description': 'Description',\n",
                "                                            'state': 'State',\n",
                "                                            'createdById': 'CreatedById',\n",
                "                                            'modifiedById': 'ModifiedById',\n",
                "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
                "                                            'createdDate': 'CreatedDateTime'\n",
                "                                            }, errors='ignore'\n",
                "                                        )\n",
                "\n",
                "\n",
                "            # Change id column values to upper case\n",
                "            to_upper_if_exists(df_mlm, 'MLModelId')  \n",
                "            to_upper_if_exists(df_mlm, 'CreatedById') \n",
                "            to_upper_if_exists(df_mlm, 'ModifiedById')  \n",
                "\n",
                "            # Add calculated columns\n",
                "            df_mlm['CreatedDate'] = df_mlm['CreatedDateTime'].str.slice(0, 10)\n",
                "            df_mlm['LastUpdatedDate'] = df_mlm['LastUpdatedDateTime'].str.slice(0, 10)\n",
                "\n",
                "            # Add data to write array\n",
                "            write_list.append({\"df\": df_mlm, \"name\" : \"ml_models\"})\n",
                "\n",
                "            if display_data:\n",
                "                display(df_mlm)\n",
                "    except Exception as ex:\n",
                "        print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:45.9725761Z\",\"execution_finish_time\":\"2025-04-04T14:28:46.3867588Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "8b649daa-f695-40a1-abc4-ef68870bbccd"
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Datasource Instances"
            ],
            "metadata": {},
            "id": "989b582d-4f18-43bc-a062-366c3e8a3b48"
        },
        {
            "cell_type": "code",
            "source": [
                "# Datasource Instances (same level as Workspaces)\n",
                "# [Info]: It must have a partially dynamic structure\n",
                "\n",
                "try:\n",
                "    df_dts_inst = pd.json_normalize(pd.json_normalize(results).explode(\"datasourceInstances\")[\"datasourceInstances\"])\n",
                "\n",
                "    if df_dts_inst.empty:\n",
                "        print(\"df_dts_inst dataframe is empty\")\n",
                "    else:\n",
                "        # Rename columns\n",
                "        df_dts_inst = df_dts_inst.rename(columns = {\n",
                "                                        'datasourceId' : 'DatasourceId',\n",
                "                                        'datasourceType': 'DatasourceType',\n",
                "                                        'gatewayId': 'GatewayId'\n",
                "                                        }, errors='ignore'\n",
                "                                    )\n",
                "\n",
                "        # Change id column values to upper case\n",
                "        to_upper_if_exists(df_dts_inst, 'DatasourceId') \n",
                "        to_upper_if_exists(df_dts_inst, 'GatewayId') \n",
                "\n",
                "        # Add data to write array\n",
                "        write_list.append({\"df\": df_dts_inst, \"name\" : \"datasource_instances\"}) \n",
                "\n",
                "        if display_data:\n",
                "            display(df_dts_inst)\n",
                "\n",
                "except Exception as ex:\n",
                "    print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:46.3885832Z\",\"execution_finish_time\":\"2025-04-04T14:28:46.6674075Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "038bab1d-559c-407d-b2f0-40960beb3195"
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Misconfigured Datasource Instances"
            ],
            "metadata": {},
            "id": "ca06d80c-e5de-4d33-a3c2-2da03e8ebf68"
        },
        {
            "cell_type": "code",
            "source": [
                "# Misconfigured Datasource Instances (same level as Workspaces)\n",
                "# [Info]: It must have a partially dynamic structure\n",
                "\n",
                "try:\n",
                "    df_mc_dts_inst = pd.json_normalize(pd.json_normalize(results).explode(\"misconfiguredDatasourceInstances\")[\"misconfiguredDatasourceInstances\"])\n",
                "\n",
                "    if df_mc_dts_inst.empty:\n",
                "        print(\"df_mc_dts_inst dataframe is empty\")\n",
                "    else:\n",
                "        # Rename columns\n",
                "        df_mc_dts_inst = df_mc_dts_inst.rename(columns = {\n",
                "                                        'datasourceId' : 'DatasourceId',\n",
                "                                        'datasourceType': 'DatasourceType'\n",
                "                                        }, errors='ignore'\n",
                "                                    )\n",
                "\n",
                "        # Change id column values to upper case\n",
                "        to_upper_if_exists(df_mc_dts_inst, 'DatasourceId')\n",
                "        \n",
                "        # Add data to write array\n",
                "        write_list.append({\"df\": df_mc_dts_inst, \"name\" : \"misconfigured_datasource_instances\"}) \n",
                "\n",
                "        if display_data:\n",
                "            display(df_mc_dts_inst)\n",
                "\n",
                "except Exception as ex:\n",
                "    print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:46.669329Z\",\"execution_finish_time\":\"2025-04-04T14:28:46.9878048Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "8bf77e02-39f5-464c-8dc4-fabef8017b6a"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Write data to Lakehouse\n",
                "\n",
                "This last sequence writes the extracted tenant meta data to FUAM Lakehouse\n",
                "\n",
                "The **write_list** array will be iterated, which writes the extracted data to tables"
            ],
            "metadata": {},
            "id": "a436990f-a6ff-46cc-b5c3-a0ed978ad119"
        },
        {
            "cell_type": "markdown",
            "source": [
                "Function to dynamically write different data frames to lakehouse. Depending on the keep_history variable data get added daily or overwritten in the respective delta tables. If the pandas dataframe is empty, no result will be written to Delta tables\n",
                "The following parameters need to be configured:\n",
                "- df: Pandas Dataframe containing the data to be written\n",
                "- table_name: Target table name"
            ],
            "metadata": {},
            "id": "cb14dc20-4070-4a9f-bf33-d3820f448ae1"
        },
        {
            "cell_type": "code",
            "source": [
                "def write_data_to_gold(df, table_name):\n",
                "    if df.empty:\n",
                "        print(\"No data for table \" + table_name + \" existing\")\n",
                "    else:\n",
                "        # Transfer pandas df to spark df\n",
                "        spark_df = spark.createDataFrame(df)\n",
                "        #spark_df.printSchema()\n",
                "        spark_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(table_name)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:46.9897003Z\",\"execution_finish_time\":\"2025-04-04T14:28:47.2882342Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "03c8c285-bfe3-4087-b735-6f25bb8f5cb3"
        },
        {
            "cell_type": "code",
            "source": [
                "for it in write_list:\n",
                "    try:\n",
                "        print(\"Artifact:\", it[\"name\"])\n",
                "        print(f\"Loading to\", it[\"name\"], \"table\")\n",
                "        print(\"--------\")\n",
                "        write_data_to_gold(it[\"df\"], it[\"name\"])\n",
                "    except Exception as ex:\n",
                "        print(ex)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:28:47.290064Z\",\"execution_finish_time\":\"2025-04-04T14:30:18.4517689Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "ab61b705-73ed-4c0c-a642-7225ed7b042c"
        },
        {
            "cell_type": "code",
            "source": [
                "def create_empty_table_if_missing (table_name, default_columns) : \n",
                "    if not spark.catalog.tableExists(table_name):\n",
                "        print(\"Create table \" + table_name)\n",
                "        records = []\n",
                "        dummy_record = {}\n",
                "        for col in default_columns:\n",
                "            dummy_record[col] = \"Dummy\"\n",
                "        records.append(dummy_record)\n",
                "        pdf = pd.DataFrame(   records )\n",
                "        spark_df = spark.createDataFrame( pdf)\n",
                "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(table_name)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:30:18.4543467Z\",\"execution_finish_time\":\"2025-04-04T14:30:18.7407582Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "2f5e4e73-9145-4cff-b678-dc07cc38cdf7"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Create missing tables\n",
                "Create empty tables in case there has been an error while creating the tables from API.\n",
                "\n",
                "This step is important to prevent breaking the semantic model in a case of error."
            ],
            "metadata": {},
            "id": "841b79e9-4182-4735-823a-3df9dd892cb7"
        },
        {
            "cell_type": "code",
            "source": [
                "create_empty_table_if_missing(\"reports\", [\"WorkspaceId\",\"ReportId\", \"SemanticModelId\", \"OriginalReportObjectId\", \"SemanticModelWorkspaceId\" ])\n",
                "create_empty_table_if_missing(\"semantic_models\", [\"WorkspaceId\",\"SemanticModelId\"])\n",
                "create_empty_table_if_missing(\"dataflows\", [\"WorkspaceId\",\"DataflowId\",\"Name\",\"description\", \"Generation\", \"ModifiedDateTime\"])\n",
                "create_empty_table_if_missing(\"dashboards\", [\"WorkspaceId\",\"DashboardId\"])\n",
                "create_empty_table_if_missing(\"datasource_instances\", [\"DatasourceId\",\"GatewayId\"])\n",
                "create_empty_table_if_missing(\"workspaces_scanned_users\", [\"WorkspaceId\",\"GraphId\", \"Identifier\", \"profile.id\"])\n",
                "create_empty_table_if_missing(\"eventhouses\", [\"WorkspaceId\",\"EventhouseId\", \"Name\"])\n",
                "create_empty_table_if_missing(\"pipelines\", [\"WorkspaceId\",\"EventhouseId\", \"Name\"])\n",
                "create_empty_table_if_missing(\"reflexes\", [\"WorkspaceId\", \"ReflexId\", \"Name\"])\n",
                "create_empty_table_if_missing(\"notebooks\", [\"WorkspaceId\",\"NotebookId\", \"Name\"])\n",
                "create_empty_table_if_missing(\"environments\", [\"WorkspaceId\", \"EnvironmentId\", \"Name\"])"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-04T14:30:18.7431461Z\",\"execution_finish_time\":\"2025-04-04T14:30:20.170062Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "7f961aa6-cee5-4d0b-a9e3-a106bec76e8b"
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "a365ComputeOptions": null,
        "sessionKeepAliveTimeout": 0,
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "widgets": {},
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "kernelspec": {
            "name": "synapse_pyspark",
            "language": "Python",
            "display_name": "Synapse PySpark"
        },
        "synapse_widget": {
            "version": "0.1",
            "state": {}
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "dependencies": {
            "lakehouse": {
                "default_lakehouse": "729eb8a2-8070-5ed8-ad43-dccbc00b32af",
                "default_lakehouse_name": "FUAM_Lakehouse",
                "default_lakehouse_workspace_id": "eb764c8b-cf3b-55be-adf4-348fe9233657"
            },
            "environment": {}
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}