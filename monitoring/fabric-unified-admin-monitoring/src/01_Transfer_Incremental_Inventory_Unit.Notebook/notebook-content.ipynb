{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Tenant Scan - Inventory data\n",
        "\n",
        "This notebook requests a tenant scan for **last modified workspaces** and **loads the results to FUAM Lakehouse**.\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: #88D5FF;\">**REPLACE**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_Inventory_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Files** from FUAM_Lakehouse folder **bronze_file_location** variable\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**Different delta tables** in FUAM_Lakehouse"
      ],
      "metadata": {},
      "id": "a09f7363-0c1d-4d2f-8d24-56c2288ff6b1"
    },
    {
      "cell_type": "code",
      "source": [
        "import sempy.fabric as fabric\n",
        "from pyspark.sql.functions import col, explode\n",
        "import pyspark.sql.functions as f\n",
        "from delta.tables import *\n",
        "import pandas as pd\n",
        "import json\n",
        "from pyspark.sql.types import *\n",
        "import datetime\n",
        "import time\n",
        "pd.options.mode.chained_assignment = None # This option is used to suppress a warning "
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "8caaa26f-51a6-4f56-afc7-0e624a624ddb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Parameters:\n",
        "----------------\n",
        "\n",
        "Tenant specific parameters\n",
        "- **has_tenant_domains**: Set to _True_, if your tenant uses Domains\n",
        "- **extract_powerbi_artifacts_only**: If _True_, the notebook extracts and writes data for Power BI artifacts only (datamart, semantic model, dataflow, report, dashboard)\n",
        "- **display_data**: If true, the notebook shows the results of important steps (eg. extracted workspaces dataframe)\n",
        "\n"
      ],
      "metadata": {},
      "id": "b53809dc-2db3-4f37-aeff-1c4cd8896ef8"
    },
    {
      "cell_type": "code",
      "source": [
        "## Parameters\n",
        "# Tenant specific parameters\n",
        "has_tenant_domains = False\n",
        "extract_powerbi_artifacts_only = False\n",
        "\n",
        "# Debug\n",
        "display_data = False\n",
        "\n",
        "# Optional\n",
        "# Key Vault details, this should be used to query the APIs via Service Principal instead of the user\n",
        "optional_keyvault_name = '' # Name of the Azure Key Vault\n",
        "# Names of the secrets saved in Azure Key Vault \n",
        "optional_keyvault_sp_tenantId_secret_name = ''   # Tenant ID secret name\n",
        "optional_keyvault_sp_clientId_secret_name = ''   # Name for Client ID of Service Principal\n",
        "optional_keyvault_sp_secret_secret_name = '' # Name for Client Secret of Service Principal"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "143ec38c-7bac-4822-956e-13da7a7e08f9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Variables:\n",
        "\n",
        "--------------\n",
        "an API request specific parameters\n",
        "\n",
        "**workspaces_per_request**: This variable determines, how many workspaces will be requested in a single call. Currently a maximum of 100 workspaces can be requested in a single call\n",
        "**max_parallel_requests**: This variable determines, how many concurrent requests can be done towards the scanner API. Currently there is a maximum of 16 parallel requests\n",
        "\n",
        "--------------\n",
        "\n",
        "an API content granularity parameters\n",
        "**get_artifact_users**: If _True_, the artifact users will be included\n",
        "**lineage**: If _True_, the lineage information will be included to artifacts (like in _Workspace_--> _Lineage view_ on the UI)\n",
        "**datasource_details**: If _True_, additional datasource details will be included e.g. semantic model table sources\n",
        "**dataset_schema**: If _True_, semantic model data will include tables, columns, measures\n",
        "**dataset_expressions**: If _True_, additional DAX and M-expressions (Power Query M-language) will be included\n",
        "\n",
        "**Important:** \n",
        "r enhanced meta data scan the following features has to be enabled under 'Admin API settings':\n",
        "Enable _'Enhance admin APIs responses with detailed metadata'_ before set **dataset_schema** to _True_\n",
        "Enable '_Enhance admin APIs responses with DAX and mashup expressions'_ before set **dataset_expressions** to _True_\n",
        "\n",
        "--------------\n",
        "\n",
        "nant specific parameters\n",
        "**has_tenant_domains**: Set to _True_, if your tenant uses Domains\n",
        "**extract_powerbi_artifacts_only**: If _True_, the notebook extracts and writes data for Power BI artifacts only (datamart, semantic model, dataflow, report, dashboard)\n",
        "\n",
        "--------------\n",
        "\n",
        "ditional parameters\n",
        "**write_to_files**: If true, the JSON results will be written to the files of the lakehouse\n"
      ],
      "metadata": {},
      "id": "05c986d4-5a92-4555-b51d-ea3cbdb9d1bc"
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables\n",
        "\n",
        "# Scan API request specific\n",
        "workspaces_per_request = 100\n",
        "max_parallel_requests = 16\n",
        "\n",
        "# Scan API content workspace\n",
        "exclude_personal_workspaces = True\n",
        "exclude_inactive_workspaces = True\n",
        "\n",
        "# Scan API content granularity\n",
        "get_artifact_users = True\n",
        "lineage = True\n",
        "datasource_details = True\n",
        "dataset_schema = False\n",
        "dataset_expressions = False\n",
        "\n",
        "# Additional\n",
        "write_to_files = True"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "a8234e9e-18fd-401d-adce-355ffeb25f2c"
    },
    {
      "cell_type": "code",
      "source": [
        "## Logic intern variables\n",
        "\n",
        "# Array of scan results\n",
        "results = []\n",
        "\n",
        "# Array of appended tenant scan content\n",
        "write_list = []"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "60a34ca7-d3bb-4bf1-89c3-e1818830f0d3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requesting scans & fetching results"
      ],
      "metadata": {},
      "id": "e526dd21-498c-4f7d-a819-f3c06d56501d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Init the client\n",
        "client = fabric.FabricRestClient()\n",
        "\n",
        "# Set date helpers\n",
        "current_time = datetime.datetime.now()\n",
        "date = current_time.date()"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "990a7b62-7152-405a-982e-77ea97eca1de"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if key vault secrets are configured in the right way. If not the executing users identity is used\n",
        "try:\n",
        "    keyvault = f'https://{optional_keyvault_name}.vault.azure.net/'\n",
        "    tenant_id = mssparkutils.credentials.getSecret(keyvault, optional_keyvault_sp_tenantId_secret_name)\n",
        "    client_id = mssparkutils.credentials.getSecret(keyvault, optional_keyvault_sp_clientId_secret_name)\n",
        "    client_secret = mssparkutils.credentials.getSecret(keyvault, optional_keyvault_sp_secret_secret_name)\n",
        "    use_keyvault = True\n",
        "    print(\"Service Principal identity is used for API authentification\")\n",
        "except:\n",
        "    print(\"Configured Secrets not found in Key Vault. Script tries to use user identity instead\")\n",
        "    use_keyvault = False"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "39637673-0528-44d1-8e44-1463d0ea810c"
    },
    {
      "cell_type": "code",
      "source": [
        "def GenerateHeader():\n",
        "    if use_keyvault:\n",
        "        url = f'https://login.microsoftonline.com/{tenant_id}/oauth2/token'\n",
        "        data = f'grant_type=client_credentials&client_id={client_id}&client_secret={client_secret}&resource=https://analysis.windows.net/powerbi/api'  \n",
        "        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
        "        response = client.post(url, headers=headers, data=data)\n",
        "        return     {'Content-Type': 'application/json', 'Authorization': f'Bearer {response.json()[\"access_token\"]}'}\n",
        "    else:    \n",
        "        return {'Content-Type': 'application/json'}\n",
        "    \n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "2ec12b81-b0ae-4f70-bfc6-d3bf135e9976"
    },
    {
      "cell_type": "code",
      "source": [
        "def RequestWithRetry(method, url, data = {}, num_retries=3, success_list=[200, 202, 404], **kwargs):\n",
        "    headers = GenerateHeader()\n",
        "    response = None\n",
        "    for i in range(num_retries):\n",
        "        try:\n",
        "            if method == 'post':\n",
        "                response = client.post(url, json=data, headers=headers,**kwargs)\n",
        "            if method == 'get':\n",
        "                response = client.get(url, headers=headers,**kwargs)\n",
        "                \n",
        "            if response.status_code in success_list:\n",
        "                return response\n",
        "                \n",
        "            ## Captures too many requests error\n",
        "            if response.status_code == 429:\n",
        "                ## Captures the 500 requests in an hour limit\n",
        "                if response.headers.get('Retry-After',None) is not None:\n",
        "                    waitTime = int(response.headers['Retry-After'])\n",
        "                    print(f'Hit the 500 requests per hour rate limit - waiting {str(waitTime)} seconds until next retry')\n",
        "                    time.sleep(waitTime)\n",
        "                ## Captures the 16 simultaneous requests limit\n",
        "                elif response.headers.get('Retry-After',None) is None:\n",
        "                    waitTime = 120\n",
        "                    print(f'Hit the 16 simultaneous requests limit - waiting {str(waitTime)} seconds until next retry')\n",
        "                    time.sleep(waitTime)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            pass\n",
        "    return response"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "7a4b4c30-8e58-4a73-a0dc-b105905e9f67"
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the total number of workspaces into packages of 100 (workspaces_per_request)\n",
        "\n",
        "# Get API data\n",
        "response = RequestWithRetry('get', f\"v1.0/myorg/admin/workspaces/modified?excludePersonalWorkspaces={exclude_personal_workspaces}&excludeInActiveWorkspaces={exclude_inactive_workspaces}\")\n",
        "\n",
        "# Build array buckets \n",
        "modified_workspaces = pd.json_normalize(response.json())\n",
        "modified_workspaces[\"index\"] = pd.to_numeric(modified_workspaces.reset_index()[\"index\"])\n",
        "modified_workspaces[\"run\"] = modified_workspaces[\"index\"] // workspaces_per_request\n",
        "modified_workspaces = modified_workspaces.groupby('run')['id'].apply(list)\n",
        "\n",
        "# Init runs\n",
        "df_runs = pd.DataFrame(data = modified_workspaces)\n",
        "df_runs[\"status\"] = \"Not Started\""
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "570d4bd4-4b75-4e0c-8d46-00ed894d07b4"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    print(modified_workspaces[0])"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "67207761-87b8-4702-8a6f-f93da9cb5e1d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Use getInfo API to request generation of meta data for all workspaces, \n",
        "# making sure maximal 16 (workspaces_per_request) requests are running in parallel\n",
        "\n",
        "df_runs_current = df_runs[df_runs[\"status\"].isin([\"Not Started\", \"Request sent\", \"Running\"])].head(max_parallel_requests)\n",
        "\n",
        "while df_runs_current.shape[0] > 0:\n",
        "    time.sleep(5)\n",
        "    for i, row in df_runs_current.iterrows():\n",
        "        if row[\"status\"] == \"Not Started\":\n",
        "            payload = {}\n",
        "            payload[\"workspaces\"] = row[\"id\"]\n",
        "           # api_uri = f\"/v1.0/myorg/admin/workspaces/getInfo?getArtifactUsers={get_artifact_users}&lineage={lineage}&datasourceDetails={datasource_details}&datasetSchema={dataset_schema}&datasetExpressions={dataset_expressions}\"\n",
        "            powerBIAPIBaseUri = 'https://api.powerbi.com/v1.0/myorg/'\n",
        "            api_uri = f'{powerBIAPIBaseUri}/admin/workspaces/getInfo?getArtifactUsers={get_artifact_users}&lineage={lineage}&datasourceDetails={datasource_details}&datasetSchema={dataset_schema}&datasetExpressions={dataset_expressions}'\n",
        "       \n",
        "          #  response = client.post(api_uri, json = payload)\n",
        "            response = RequestWithRetry(\"post\",api_uri,payload)\n",
        "            \n",
        "            id = pd.json_normalize(response.json())[\"id\"][0]\n",
        "            \n",
        "            df_runs.loc[i, \"status\"] = \"Request sent\"\n",
        "            df_runs.loc[i, \"run_id\"] = id\n",
        "\n",
        "        elif row[\"status\"] in [ \"Request sent\", \"Running\"]:\n",
        "            response = RequestWithRetry(\"get\",\"/v1.0/myorg/admin/workspaces/scanStatus/\" + row[\"run_id\"])\n",
        "\n",
        "            stat = pd.json_normalize(response.json())[\"status\"][0]\n",
        "            df_runs.loc[i, \"status\"] = stat\n",
        "            \n",
        "    df_runs_current = df_runs[df_runs[\"status\"].isin([\"Not Started\", \"Request sent\", \"Running\"])].head(max_parallel_requests)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "970e1622-7c8e-452c-9845-b1ec62b81e65"
    },
    {
      "cell_type": "code",
      "source": [
        "df_runs"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "52d25e0b-c726-4b44-b9b1-c781b37ee7c5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterates through the scan runs and append the result to the 'result' variable\n",
        "# If write_to_files is true the JSON response will also be written into the files section\n",
        "for i, row in df_runs.iterrows():\n",
        "\n",
        "    if row[\"status\"] == \"Succeeded\":\n",
        "            response = RequestWithRetry('get', f\"/v1.0/myorg/admin/workspaces/scanResult/\" + row[\"run_id\"])\n",
        "            print(\"/v1.0/myorg/admin/workspaces/scanResult/\" + row[\"run_id\"])\n",
        "            results.append(response.json())\n",
        "\n",
        "            if write_to_files:\n",
        "                folder_path = mssparkutils.fs.getMountPath('/default') + \"/Files/history/inventory/\" + current_time.strftime(\"%Y/%m/%d\") + \"/\" +  current_time.strftime(\"%H-%M-%S\") + \"/\"\n",
        "                mssparkutils.fs.mkdirs(f\"file://\" +folder_path)\n",
        "\n",
        "                with open(folder_path + row[\"run_id\"] +\".json\", \"w\") as f:\n",
        "                    f.write(json.dumps(response.json()))"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "f08955e0-ff38-475a-a06a-90d2858eb170"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    print(len(results))"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "c8966320-914a-4dc5-863a-9ae374e58056"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to parse result json into needed tables. If object type is not present in tenant, the function will return an empty df instead.\n",
        "The following parameters are expected:\n",
        "- df: Pandas DataFrame to be flattened\n",
        "- parent_id: ID to parent object, to be able to Link the data later on\n",
        "- rename_id (optional): New name of id column"
      ],
      "metadata": {},
      "id": "023a8216-0341-4950-9741-3029c61a0784"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_details(df, parent_id, col, **kwargs ):\n",
        "    try:\n",
        "        rename_id = kwargs.get('rename_id' , None)\n",
        "        df_res = df[[parent_id, col]].explode(col, ignore_index = True)\n",
        "        df_res = df_res[[parent_id]].join(pd.json_normalize(df_res[col]))\n",
        "\n",
        "\n",
        "        # This check has been added in order to make it work for subsets which don't contain an id column\n",
        "        if 'id' in df_res.columns:\n",
        "            df_res = df_res.dropna(subset=['id'])\n",
        "        else:\n",
        "        # In case there is no id column, rows where all other values are empty will be droped\n",
        "            other_cols = df_res.columns.to_list()\n",
        "            other_cols.remove(parent_id)\n",
        "            df_res = df_res.dropna(subset = other_cols, how = 'all' )\n",
        "\n",
        "        if not(rename_id is None):\n",
        "            df_res = df_res.rename(columns = {'id' : rename_id})\n",
        "        \n",
        "        for column in df_res.columns:\n",
        "            if \"mixed\" in pd.api.types.infer_dtype(df_res[column]):\n",
        "                df_res[column] = df_res[column].astype(str)\n",
        "\n",
        "\n",
        "\n",
        "        return df_res\n",
        "    except:\n",
        "        return pd.DataFrame()"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "id": "fecf0668-78fb-4832-a0db-2cb3cec05c39"
    },
    {
      "cell_type": "code",
      "source": [
        "def to_upper_if_exists(df, col):\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].str.upper() \n",
        "    return(df)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "e7c074dc-6d16-41d6-b02c-f200bcfa4064"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting results\n",
        "Parse the information from the result into data frames which can be written to lakehouse in later step. \n",
        "\n",
        "In case some object types are not existing in Fabric tenant, this might fail. \n",
        "\n",
        "In this case, the lakehouse tables will be created with an empty row"
      ],
      "metadata": {},
      "id": "5fb6f58b-6a3e-4313-865b-50707cfe221f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Workspaces"
      ],
      "metadata": {},
      "id": "298421df-b376-4d61-9dd7-faf31f93f03f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Workspaces"
      ],
      "metadata": {},
      "id": "da71257d-8ddb-42ad-8686-ad0b870af97e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Workspaces\n",
        "# [Info]: It can be defined statically\n",
        "\n",
        "# Check if workspace has domains\n",
        "workspace_columns = {'':''}\n",
        "if has_tenant_domains:\n",
        "    workspace_columns = {\n",
        "                        'id': 'WorkspaceId',\n",
        "                        'name': 'WorkspaceName',\n",
        "                        'type': 'Type',\n",
        "                        'state': 'State',\n",
        "                        'isOnDedicatedCapacity': 'IsOnDedicatedCapacity',\n",
        "                        'capacityId': 'CapacityId',\n",
        "                        'domainId': 'DomainId',\n",
        "                        'description': 'Description',\n",
        "                        'defaultDatasetStorageFormat': 'StorageFormat'\n",
        "                        }\n",
        "else:\n",
        "    workspace_columns = {\n",
        "                        'id': 'WorkspaceId',\n",
        "                        'name': 'WorkspaceName',\n",
        "                        'type': 'Type',\n",
        "                        'state': 'State',\n",
        "                        'isOnDedicatedCapacity': 'IsOnDedicatedCapacity',\n",
        "                        'capacityId': 'CapacityId',\n",
        "                        'description': 'Description',\n",
        "                        'defaultDatasetStorageFormat': 'StorageFormat'\n",
        "                        }\n",
        "\n",
        "df_workspaces = pd.json_normalize(pd.json_normalize(results).explode(\"workspaces\")[\"workspaces\"])\n",
        "\n",
        "# Rename columns\n",
        "df_workspaces = df_workspaces.rename(columns = workspace_columns)\n",
        "\n",
        "\n",
        "# Change id column values to upper case\n",
        "to_upper_if_exists(df_workspaces, 'WorkspaceId' )\n",
        "to_upper_if_exists(df_workspaces, 'CapacityId' )\n",
        "if has_tenant_domains:\n",
        "    to_upper_if_exists(df_workspaces, 'DomainId' )\n",
        "\n",
        "if display_data:\n",
        "    display(df_workspaces)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "id": "ee5f5d54-2b81-4997-a864-664b31dbc5b5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Workspace Users"
      ],
      "metadata": {},
      "id": "839b1a2a-dcaf-4f4a-9ddf-9cab5af4250d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Workspace Users (within Workspaces)\n",
        "# [Info]: It can be defined statically\n",
        "df_ws_users = get_details(df_workspaces, \"WorkspaceId\", \"users\")\n",
        "\n",
        "if df_ws_users.empty:\n",
        "    print(\"df_ws_users dataframe is empty\")\n",
        "else:\n",
        "    # Remove columns\n",
        "    df_ws_users = df_ws_users.drop(columns=['emailAddress', 'displayName'], errors='ignore')\n",
        "\n",
        "    # Rename columns\n",
        "    df_ws_users = df_ws_users.rename(columns = {\n",
        "                                    'groupUserAccessRight': 'GroupUserAccessRight',\n",
        "                                    'identifier': 'Identifier',\n",
        "                                    'graphId': 'GraphId',\n",
        "                                    'principalType': 'PrincipalType',\n",
        "                                    'userType': 'UserType'\n",
        "                                    }\n",
        "                                )\n",
        "    # Add data to write array\n",
        "    write_list.append({\"df\": df_ws_users, \"name\" : \"workspaces_scanned_users\"})\n",
        "\n",
        "    if display_data:\n",
        "        display(df_ws_users)"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "1e8e24da-ae29-4e2f-9866-d9b941314d93"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Power BI artifacts"
      ],
      "metadata": {},
      "id": "0a1ad05d-edeb-4dbc-bd0a-a807bd1f4dd7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Datamarts"
      ],
      "metadata": {},
      "id": "e1071e4b-6840-49d6-bce6-c9ee42e70772"
    },
    {
      "cell_type": "code",
      "source": [
        "# Datamarts (within Workspaces)\n",
        "\n",
        "try:\n",
        "    df_datamarts = get_details(df_workspaces, \"WorkspaceId\" , \"datamarts\")\n",
        "\n",
        "    if df_datamarts.empty or 'id' not in df_datamarts.columns:\n",
        "        print(\"df_datamarts dataframe is empty\")\n",
        "    else:\n",
        "        # Remove null rows\n",
        "        df_datamarts = df_datamarts[df_datamarts['id'].notna()]\n",
        "\n",
        "        # Remove columns\n",
        "        df_datamarts = df_datamarts.drop(columns=['configuredBy', 'users', 'modifiedBy'], errors='ignore')\n",
        "\n",
        "        # Rename columns\n",
        "        df_datamarts = df_datamarts.rename(columns = {\n",
        "                                        'id': 'DatamartId',\n",
        "                                        'name': 'Name',\n",
        "                                        'configuredById': 'ConfiguredById',\n",
        "                                        'modifiedById': 'ModifiedById',\n",
        "                                        'modifiedDateTime': 'ModifiedDateTime',\n",
        "                                        'type': 'Type'\n",
        "                                        } , errors='ignore'\n",
        "                                    )\n",
        "\n",
        "        # Change id column values to upper case\n",
        "        to_upper_if_exists(df_datamarts, 'DatamartId' )\n",
        "        to_upper_if_exists(df_datamarts, 'ConfiguredById' ) \n",
        "        to_upper_if_exists(df_datamarts, 'ModifiedById' )\n",
        "\n",
        "        # Add calculated columns\n",
        "        df_datamarts['ModifiedDateTime'] = df_datamarts['ModifiedDateTime'].str.slice(0, 10)\n",
        "\n",
        "        # Add data to write array\n",
        "        write_list.append({\"df\": df_datamarts, \"name\" : \"datamarts\"})\n",
        "\n",
        "        if display_data:\n",
        "            display(df_datamarts)\n",
        "            \n",
        "except Exception as ex:\n",
        "    print(ex)\n"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "f26c0ff9-3bd0-4171-afa5-ab49d77e9def"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Semantic Models"
      ],
      "metadata": {},
      "id": "12b0c4d1-5688-43de-98c1-285b107de064"
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic Models (within Workspaces)\n",
        "# [Info]: It must have a partially dynamic structure\n",
        "\n",
        "try:\n",
        "    df_semantic_models = get_details(df_workspaces, \"WorkspaceId\" , \"datasets\")\n",
        "\n",
        "    if df_semantic_models.empty or 'id' not in df_semantic_models.columns:\n",
        "        print(\"df_semantic_models dataframe is empty\")\n",
        "    else:\n",
        "        # Remove null rows\n",
        "        df_semantic_models = df_semantic_models[df_semantic_models['id'].notna()]\n",
        "\n",
        "        # Remove columns\n",
        "        df_semantic_models = df_semantic_models.drop(columns=\n",
        "                                    ['configuredBy', 'users', 'tables', 'datasourceUsages', \n",
        "                                    'relations'], errors='ignore')\n",
        "\n",
        "        try:\n",
        "            # Remove columns\n",
        "            df_semantic_models = df_semantic_models.drop(columns=\n",
        "                                    ['refreshSchedule.days','refreshSchedule.times'], errors='ignore')\n",
        "        except:\n",
        "            print(\"No refreshSchedule columns were found.\")\n",
        "\n",
        "        try:\n",
        "            # Remove columns\n",
        "            df_semantic_models = df_semantic_models.drop(columns=\n",
        "                                    ['directQueryRefreshSchedule.days', 'directQueryRefreshSchedule.times'], errors='ignore')\n",
        "        except:\n",
        "            print(\"No directQuerRefreshSchedule columns were found.\")\n",
        "\n",
        "        try:\n",
        "            # Remove columns\n",
        "            df_semantic_models = df_semantic_models.drop(columns= ['upstreamDatasets', 'upstreamDatamarts'], errors='ignore')\n",
        "\n",
        "        except:\n",
        "            print(\"No upstream(Dataset/Datamart) columns were found.\")\n",
        "\n",
        "        try:\n",
        "            # Remove columns\n",
        "            df_semantic_models = df_semantic_models.drop(columns= ['misconfiguredDatasourceUsages'], errors='ignore')\n",
        "        except:\n",
        "            print(\"No misconfiguredDatasourceUsages columns were found.\")\n",
        "\n",
        "        # Rename columns\n",
        "        df_semantic_models = df_semantic_models.rename(columns = {\n",
        "                                        'id': 'SemanticModelId',\n",
        "                                        'name': 'Name',\n",
        "                                        'configuredById': 'ConfiguredById',\n",
        "                                        'createdDate': 'CreatedDateTime',\n",
        "                                        'targetStorageMode': 'StorageMode',\n",
        "                                        'contentProviderType': 'ContentProviderType'\n",
        "                                        }, errors='ignore'\n",
        "                                    )\n",
        "\n",
        "\n",
        "        # Change id column values to upper case\n",
        "        to_upper_if_exists(df_semantic_models, 'SemanticModelId' ) \n",
        "        to_upper_if_exists(df_semantic_models, 'ConfiguredById' )\n",
        "\n",
        "        # Add calculated columns\n",
        "        df_semantic_models['CreatedDate'] = df_semantic_models['CreatedDateTime'].str.slice(0, 10)\n",
        "        \n",
        "        # Add data to write array\n",
        "        write_list.append({\"df\": df_semantic_models, \"name\" : \"semantic_models\"})\n",
        "\n",
        "        if display_data:\n",
        "            display(df_semantic_models)\n",
        "            \n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "b784bb26-a22d-4f09-9c2f-1c74469d2bc3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Dataflows"
      ],
      "metadata": {},
      "id": "00e900f7-81b0-41bd-a6c2-59fd7bf2705f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataflows (within Workspaces)\n",
        "# [Info]: It can be defined statically\n",
        "\n",
        "try:\n",
        "    df_dataflows = get_details(df_workspaces, \"WorkspaceId\" , \"dataflows\")\n",
        "\n",
        "    if df_dataflows.empty or 'objectId' not in df_dataflows.columns:\n",
        "        print(\"df_dataflows dataframe is empty\")\n",
        "    else:\n",
        "        # Remove null rows\n",
        "        df_dataflows = df_dataflows[df_dataflows['objectId'].notna()]\n",
        "\n",
        "        # Remove columns\n",
        "        df_dataflows = df_dataflows.drop(columns=['configuredBy', 'modifiedBy', 'users'], errors='ignore')\n",
        "        try:\n",
        "            df_reports = df_reports.drop(columns=['datasourceUsages'], errors='ignore')\n",
        "        except:\n",
        "            print(\"No datasourceUsages\")\n",
        "\n",
        "        # Rename columns\n",
        "        df_dataflows = df_dataflows.rename(columns = {\n",
        "                                        'objectId': 'DataflowId',\n",
        "                                        'name': 'Name',\n",
        "                                        'modifiedDateTime': 'ModifiedDateTime',\n",
        "                                        'generation': 'Generation'\n",
        "                                        }, errors='ignore'\n",
        "                                    )\n",
        "\n",
        "        # Change id column values to upper case\n",
        "        to_upper_if_exists(df_dataflows, 'DataflowId' )\n",
        "\n",
        "        # Add data to write array\n",
        "        write_list.append({\"df\": df_dataflows, \"name\" : \"dataflows\"})\n",
        "\n",
        "        if display_data:\n",
        "            display(df_dataflows)\n",
        "            \n",
        "except Exception as ex:\n",
        "    print(ex)\n"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "5271826f-aec2-4974-aaef-73dedd67c713"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Dataflow Sources"
      ],
      "metadata": {},
      "id": "945027fc-8f16-46d0-ae11-3ac620628919"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataflows (within Workspaces -> Dataflows)\n",
        "# [Info]: It can be defined statically\n",
        "\n",
        "try:\n",
        "    df_dataflow_base = get_details(df_workspaces, \"WorkspaceId\", \"dataflows\")\n",
        "    \n",
        "    # Remove null rows\n",
        "    df_dataflow_du = df_dataflow_base[df_dataflow_base['objectId'].notna()]\n",
        "\n",
        "    # Expand datasourceUsages\n",
        "    df_dataflow_du = get_details(df_dataflow_du, \"objectId\", \"datasourceUsages\")\n",
        "\n",
        "    # Remove null rows\n",
        "    df_dataflow_du = df_dataflow_du[df_dataflow_du['datasourceInstanceId'].notna()]\n",
        "\n",
        "    # Rename column\n",
        "    df_dataflow_du = df_dataflow_du.rename(columns = {'objectId': 'DataflowId'}, errors='ignore')\n",
        "\n",
        "    # Change id column values to upper case\n",
        "    to_upper_if_exists(df_dataflow_du, 'DataflowId')\n",
        "    to_upper_if_exists(df_dataflow_du, 'datasourceInstanceId')\n",
        "\n",
        "    # Add data to write array\n",
        "    write_list.append({\"df\": df_dataflow_du, \"name\" : \"dataflow_datasources\"})\n",
        "\n",
        "    if display_data:\n",
        "        display(df_dataflow_du)\n",
        "\n",
        "    del df_dataflow_base\n",
        "    del df_dataflow_du\n",
        "\n",
        "except Exception as ex:\n",
        "    print(ex)\n"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "8ca13e56-8051-4d87-a100-44c7f3d55b29"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Reports"
      ],
      "metadata": {},
      "id": "9d9183da-f2d9-4cec-b448-3fba60dac7de"
    },
    {
      "cell_type": "code",
      "source": [
        "# Reports (within Workspaces)\n",
        "# [Info]: It can be defined statically\n",
        "\n",
        "try:\n",
        "    df_reports = get_details(df_workspaces, \"WorkspaceId\" , \"reports\")\n",
        "\n",
        "    if df_reports.empty or 'id' not in df_reports.columns:\n",
        "        print(\"DF is empty\")\n",
        "    else:\n",
        "        # Remove null rows\n",
        "        df_reports = df_reports[df_reports['id'].notna()]\n",
        "\n",
        "        # Remove columns\n",
        "        df_reports = df_reports.drop(columns=['createdBy', 'modifiedBy', 'relations'], errors='ignore')\n",
        "        try:\n",
        "            df_reports = df_reports.drop(columns=['datasourceUsages'], errors='ignore')\n",
        "        except:\n",
        "            print(\"No datasourceUsages\")\n",
        "\n",
        "        # Rename columns\n",
        "        df_reports = df_reports.rename(columns = {\n",
        "                                        'id': 'ReportId',\n",
        "                                        'datasetId': 'SemanticModelId',\n",
        "                                        'datasetWorkspaceId': 'SemanticModelWorkspaceId',\n",
        "                                        'reportType': 'ReportType',\n",
        "                                        'name': 'Name',\n",
        "                                        'createdDateTime': 'CreatedDateTime',\n",
        "                                        'modifiedDateTime': 'ModifiedDateTime',\n",
        "                                        'modifiedById': 'ModifiedById',\n",
        "                                        'createdById': 'CreatedById',\n",
        "                                        'originalReportObjectId': 'OriginalReportObjectId',\n",
        "                                        'appId': 'AppId',\n",
        "                                        }\n",
        "                                    )\n",
        "\n",
        "        # Change id column values to upper case\n",
        "        to_upper_if_exists(df_reports, 'ReportId')\n",
        "        to_upper_if_exists(df_reports, 'SemanticModelId')\n",
        "        to_upper_if_exists(df_reports, 'ModifiedById')\n",
        "        to_upper_if_exists(df_reports, 'CreatedById')\n",
        "        to_upper_if_exists(df_reports, 'SemanticModelWorkspaceId') \n",
        "        to_upper_if_exists(df_reports, 'OriginalReportObjectId')\n",
        "        to_upper_if_exists(df_reports, 'AppId')\n",
        "\n",
        "        # Add data to write array\n",
        "        write_list.append({\"df\": df_reports, \"name\" : \"reports\"})\n",
        "\n",
        "        if display_data:\n",
        "            display(df_reports)\n",
        "            \n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "7ba42d2d-a4cb-405b-a6b5-d5d0a4706296"
    },
    {
      "cell_type": "code",
      "source": [
        "# Report datasourceUsages (within Workspaces -> Reports)\n",
        "# [Info]: It can be defined statically\n",
        "\n",
        "try:\n",
        "    df_report_base = get_details(df_workspaces, \"WorkspaceId\", \"reports\")\n",
        "    \n",
        "    # Remove null rows\n",
        "    df_report_du = df_report_base[df_report_base['id'].notna()]\n",
        "\n",
        "    # Expand datasourceUsages\n",
        "    df_report_du = get_details(df_report_du, \"id\", \"datasourceUsages\")\n",
        "\n",
        "    # Remove null rows\n",
        "    df_report_du = df_report_du[df_report_du['datasourceInstanceId'].notna()]\n",
        "\n",
        "    # Rename column\n",
        "    df_report_du = df_report_du.rename(columns = {'id': 'ReportId'}, errors='ignore')\n",
        "\n",
        "    # Change id column values to upper case\n",
        "    to_upper_if_exists(df_report_du, 'ReportId') \n",
        "    to_upper_if_exists(df_report_du, 'datasourceInstanceId')\n",
        "\n",
        "    # Add data to write array\n",
        "    write_list.append({\"df\": df_report_du, \"name\" : \"report_datasources\"})\n",
        "\n",
        "    if display_data:\n",
        "        display(df_report_du)\n",
        "\n",
        "    del df_report_base\n",
        "    del df_report_du\n",
        "\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "6f2e0e8f-3611-41b5-8402-ffa8a86ba399"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Dashboards"
      ],
      "metadata": {},
      "id": "fe4aeaa6-c1a4-4ec5-8833-9288c9c62405"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dashboards (within Workspaces)\n",
        "# [Info]: It can be defined statically\n",
        "\n",
        "try:\n",
        "    df_dashboards = get_details(df_workspaces, \"WorkspaceId\" , \"dashboards\")\n",
        "\n",
        "    if df_dashboards.empty or 'id' not in df_dashboards.columns:\n",
        "        print(\"df_dashboards dataframe is empty\")\n",
        "    else:\n",
        "        # Remove null rows\n",
        "        df_dashboards = df_dashboards[df_dashboards['id'].notna()]\n",
        "\n",
        "        # Remove columns\n",
        "        df_dashboards = df_dashboards.drop(columns=['users', 'isReadOnly'], errors='ignore')\n",
        "\n",
        "        # Rename columns\n",
        "        df_dashboards = df_dashboards.rename(columns = {\n",
        "                                        'id': 'DashboardId',\n",
        "                                        'displayName': 'Name'\n",
        "                                        }, errors='ignore'\n",
        "                                    )\n",
        "\n",
        "\n",
        "        # Change id column values to upper case\n",
        "        to_upper_if_exists(df_dashboards, 'DashboardId')\n",
        "\n",
        "        # Add data to write array\n",
        "        write_list.append({\"df\": df_dashboards, \"name\" : \"dashboards\"})\n",
        "\n",
        "        if display_data:\n",
        "            display(df_dashboards)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "3af5f408-654d-4c4a-be44-e91b02a8716d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fabric artifacts"
      ],
      "metadata": {},
      "id": "39c592d1-8904-4041-8e0e-29478f559cc2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Eventhouses"
      ],
      "metadata": {},
      "id": "59ffb364-0c62-45ee-9f64-f40ec3381223"
    },
    {
      "cell_type": "code",
      "source": [
        "# Eventhouses (within Workspaces)\n",
        "\n",
        "if extract_powerbi_artifacts_only == False:\n",
        "    try:\n",
        "        df_eventhouses = get_details(df_workspaces, \"WorkspaceId\" , \"Eventhouse\")\n",
        "\n",
        "        if df_eventhouses.empty or 'id' not in df_eventhouses.columns:\n",
        "            print(\"df_eventhouses dataframe is empty\")\n",
        "        else:\n",
        "            # Remove null rows\n",
        "            df_eventhouses = df_eventhouses[df_eventhouses['id'].notna()]\n",
        "\n",
        "            # Remove columns\n",
        "            df_eventhouses = df_eventhouses.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
        "\n",
        "            # Rename columns\n",
        "            df_eventhouses = df_eventhouses.rename(columns = {\n",
        "                                            'id': 'EventhouseId',\n",
        "                                            'name': 'Name',\n",
        "                                            'description': 'Description',\n",
        "                                            'state': 'State',\n",
        "                                            'createdById': 'CreatedById',\n",
        "                                            'modifiedById': 'ModifiedById',\n",
        "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
        "                                            'createdDate': 'CreatedDateTime'\n",
        "                                            }, errors='ignore'\n",
        "                                        )\n",
        "\n",
        "\n",
        "            # Change id column values to upper case\n",
        "            to_upper_if_exists(df_eventhouses, 'EventhouseId')\n",
        "            to_upper_if_exists(df_eventhouses, 'CreatedById')\n",
        "            to_upper_if_exists(df_eventhouses, 'ModifiedById') \n",
        "\n",
        "            # Add calculated columns\n",
        "            df_eventhouses['CreatedDate'] = df_eventhouses['CreatedDateTime'].str.slice(0, 10)\n",
        "            df_eventhouses['LastUpdatedDate'] = df_eventhouses['LastUpdatedDateTime'].str.slice(0, 10)\n",
        "            \n",
        "            # Add data to write array\n",
        "            write_list.append({\"df\": df_eventhouses, \"name\" : \"eventhouses\"})\n",
        "\n",
        "            if display_data:\n",
        "                display(df_eventhouses)\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "b76c1b92-edae-472b-875a-2291ac52ebc5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### KQL Databases"
      ],
      "metadata": {},
      "id": "225735ca-bb98-4164-a461-1756302f2c73"
    },
    {
      "cell_type": "code",
      "source": [
        "# KQL Databases (within Workspaces)\n",
        "\n",
        "if extract_powerbi_artifacts_only == False:\n",
        "    try:\n",
        "        df_kql = get_details(df_workspaces, \"WorkspaceId\" , \"KQLDatabase\")\n",
        "\n",
        "        if df_kql.empty or 'id' not in df_kql.columns:\n",
        "            print(\"df_kql dataframe is empty\")\n",
        "        else:\n",
        "            # Remove null rows\n",
        "            df_kql = df_kql[df_kql['id'].notna()]\n",
        "\n",
        "            # Remove columns\n",
        "            df_kql = df_kql.drop(columns=['users'], errors='ignore')\n",
        "\n",
        "            # Rename columns\n",
        "            df_kql = df_kql.rename(columns = {\n",
        "                                            'id': 'KQLDatabaseId',\n",
        "                                            'name': 'Name',\n",
        "                                            'description': 'Description',\n",
        "                                            'state': 'State'\n",
        "                                            }, errors='ignore'\n",
        "                                        )\n",
        "\n",
        "\n",
        "            # Change id column values to upper case\n",
        "            to_upper_if_exists(df_kql, 'KQLDatabaseId')  \n",
        "\n",
        "            # Add data to write array\n",
        "            write_list.append({\"df\": df_kql, \"name\" : \"kql_databases\"})\n",
        "\n",
        "            if display_data:\n",
        "                display(df_kql)\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "1f4b16cf-11fa-4a2f-b1ff-8b3c2e5d591d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Lakehouses"
      ],
      "metadata": {},
      "id": "8ce75fdb-9f71-4478-bcdf-5a8bb7445932"
    },
    {
      "cell_type": "code",
      "source": [
        "# Lakehouses (within Workspaces)\n",
        "\n",
        "if extract_powerbi_artifacts_only == False:\n",
        "    try:\n",
        "        df_lakehouses = get_details(df_workspaces, \"WorkspaceId\" , \"Lakehouse\")\n",
        "\n",
        "        if df_lakehouses.empty or 'id' not in df_lakehouses.columns:\n",
        "            print(\"df_lakehouses dataframe is empty\")\n",
        "        else:\n",
        "            # Remove null rows\n",
        "            df_lakehouses = df_lakehouses[df_lakehouses['id'].notna()]\n",
        "\n",
        "            # Remove columns\n",
        "            df_lakehouses = df_lakehouses.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
        "\n",
        "            # Rename columns\n",
        "            df_lakehouses = df_lakehouses.rename(columns = {\n",
        "                                            'id': 'LakehouseId',\n",
        "                                            'name': 'Name',\n",
        "                                            'description': 'Description',\n",
        "                                            'state': 'State',\n",
        "                                            'createdById': 'CreatedById',\n",
        "                                            'modifiedById': 'ModifiedById',\n",
        "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
        "                                            'createdDate': 'CreatedDateTime'\n",
        "                                            }, errors='ignore'\n",
        "                                        )\n",
        "\n",
        "\n",
        "            # Change id column values to upper case\n",
        "            to_upper_if_exists(df_lakehouses, 'LakehouseId')\n",
        "            to_upper_if_exists(df_lakehouses, 'CreatedById') \n",
        "            to_upper_if_exists(df_lakehouses, 'ModifiedById') \n",
        "\n",
        "            # Add calculated columns\n",
        "            df_lakehouses['CreatedDate'] = df_lakehouses['CreatedDateTime'].str.slice(0, 10)\n",
        "            df_lakehouses['LastUpdatedDate'] = df_lakehouses['LastUpdatedDateTime'].str.slice(0, 10)\n",
        "\n",
        "            # Add data to write array\n",
        "            write_list.append({\"df\": df_lakehouses, \"name\" : \"lakehouses\"})\n",
        "\n",
        "            if display_data:\n",
        "                display(df_lakehouses)\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "8d759825-699e-444b-88fd-384c9cc52e15"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Warehouses"
      ],
      "metadata": {},
      "id": "5fad04d3-165b-4195-b7dd-2ea76c791b83"
    },
    {
      "cell_type": "code",
      "source": [
        "# Warehouses (within Workspaces)\n",
        "\n",
        "if extract_powerbi_artifacts_only == False:\n",
        "    try:\n",
        "        df_warehouses = get_details(df_workspaces, \"WorkspaceId\" , \"warehouses\")\n",
        "\n",
        "        if df_warehouses.empty or 'id' not in df_warehouses.columns:\n",
        "            print(\"df_warehouses dataframe is empty\")\n",
        "        else:\n",
        "            # Remove null rows\n",
        "            df_warehouses = df_warehouses[df_warehouses['id'].notna()]\n",
        "\n",
        "            # Remove columns\n",
        "            df_warehouses = df_warehouses.drop(columns=['users', 'configuredBy', 'modifiedBy'], errors='ignore')\n",
        "\n",
        "            # Rename columns\n",
        "            df_warehouses = df_warehouses.rename(columns = {\n",
        "                                            'id': 'WarehouseId',\n",
        "                                            'name': 'Name',\n",
        "                                            'description': 'Description',\n",
        "                                            'state': 'State',\n",
        "                                            'configuredById': 'ConfiguredById',\n",
        "                                            'modifiedById': 'ModifiedById',\n",
        "                                            'modifiedDateTime': 'ModifiedDateTime'\n",
        "                                            }, errors='ignore'\n",
        "                                        )\n",
        "\n",
        "\n",
        "            # Change id column values to upper case\n",
        "            to_upper_if_exists(df_warehouses, 'WarehouseId') \n",
        "            to_upper_if_exists(df_warehouses, 'ConfiguredById') \n",
        "\n",
        "            # Add calculated columns\n",
        "            df_warehouses['ModifiedDate'] = df_warehouses['ModifiedDateTime'].str.slice(0, 10)\n",
        "\n",
        "            # Add data to write array\n",
        "            write_list.append({\"df\": df_warehouses, \"name\" : \"warehouses\"})\n",
        "\n",
        "            if display_data:\n",
        "                display(df_warehouses)\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "b43bf5dc-6416-4fd1-81a0-0643b704a0b4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Eventstream"
      ],
      "metadata": {},
      "id": "07d27dc8-a5e7-4fd7-a7d5-3c5212ba36a2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Eventstreams (within Workspaces)\n",
        "\n",
        "if extract_powerbi_artifacts_only == False:\n",
        "    try:\n",
        "        df_eventstreams = get_details(df_workspaces, \"WorkspaceId\" , \"Eventstream\")\n",
        "\n",
        "        if df_eventstreams.empty or 'id' not in df_eventstreams.columns:\n",
        "            print(\"df_eventstreams dataframe  is empty\")\n",
        "        else:\n",
        "            # Remove null rows\n",
        "            df_eventstreams = df_eventstreams[df_eventstreams['id'].notna()]\n",
        "\n",
        "            # Remove columns\n",
        "            df_eventstreams = df_eventstreams.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
        "\n",
        "            # Rename columns\n",
        "            df_eventstreams = df_eventstreams.rename(columns = {\n",
        "                                            'id': 'EventstreamId',\n",
        "                                            'name': 'Name',\n",
        "                                            'description': 'Description',\n",
        "                                            'state': 'State',\n",
        "                                            'createdById': 'CreatedById',\n",
        "                                            'modifiedById': 'ModifiedById',\n",
        "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
        "                                            'createdDate': 'CreatedDateTime'\n",
        "                                            }, errors='ignore'\n",
        "                                        )\n",
        "\n",
        "\n",
        "            # Change id column values to upper case\n",
        "            to_upper_if_exists(df_eventstreams, 'EventstreamId')\n",
        "            to_upper_if_exists(df_eventstreams, 'CreatedById')  \n",
        "            to_upper_if_exists(df_eventstreams, 'ModifiedById')\n",
        "\n",
        "            # Add calculated columns\n",
        "            df_eventstreams['CreatedDate'] = df_eventstreams['CreatedDateTime'].str.slice(0, 10)\n",
        "            df_eventstreams['LastUpdatedDate'] = df_eventstreams['LastUpdatedDateTime'].str.slice(0, 10)\n",
        "\n",
        "            # Add data to write array\n",
        "            write_list.append({\"df\": df_eventstreams, \"name\" : \"eventstreams\"})\n",
        "\n",
        "            if display_data:\n",
        "                display(df_eventstreams)\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "83e9b8c7-2c6f-455f-91d0-7e4930a75c6f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Data Pipelines"
      ],
      "metadata": {},
      "id": "ac61e6d3-74d2-4956-b033-1670aeeadbf4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Pipelines (within Workspaces)\n",
        "\n",
        "if extract_powerbi_artifacts_only == False:\n",
        "    try:\n",
        "        df_pipelines = get_details(df_workspaces, \"WorkspaceId\" , \"DataPipeline\")\n",
        "\n",
        "        if df_pipelines.empty or 'id' not in df_pipelines.columns:\n",
        "            print(\"df_pipelines dataframe is empty\")\n",
        "        else:\n",
        "            # Remove null rows\n",
        "            df_pipelines = df_pipelines[df_pipelines['id'].notna()]\n",
        "\n",
        "            # Remove columns\n",
        "            df_pipelines = df_pipelines.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
        "\n",
        "            # Rename columns\n",
        "            df_pipelines = df_pipelines.rename(columns = {\n",
        "                                            'id': 'PipelineId',\n",
        "                                            'name': 'Name',\n",
        "                                            'description': 'Description',\n",
        "                                            'state': 'State',\n",
        "                                            'createdById': 'CreatedById',\n",
        "                                            'modifiedById': 'ModifiedById',\n",
        "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
        "                                            'createdDate': 'CreatedDateTime'\n",
        "                                            }, errors='ignore'\n",
        "                                        )\n",
        "\n",
        "\n",
        "            # Change id column values to upper case\n",
        "            to_upper_if_exists(df_pipelines, 'PipelineId')\n",
        "            to_upper_if_exists(df_pipelines, 'CreatedById') \n",
        "            to_upper_if_exists(df_pipelines, 'ModifiedById') \n",
        "\n",
        "            # Add calculated columns\n",
        "            df_pipelines['CreatedDate'] = df_pipelines['CreatedDateTime'].str.slice(0, 10)\n",
        "            df_pipelines['LastUpdatedDate'] = df_pipelines['LastUpdatedDateTime'].str.slice(0, 10)\n",
        "\n",
        "            # Add data to write array\n",
        "            write_list.append({\"df\": df_pipelines, \"name\" : \"pipelines\"})\n",
        "\n",
        "            if display_data:\n",
        "                display(df_pipelines)\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "aa3708c3-666a-4458-82eb-49179deff619"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Notebooks"
      ],
      "metadata": {},
      "id": "fa059aec-0bad-45fd-9903-409042aed3d2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebooks (within Workspaces)\n",
        "# [Info]: It must have a partially dynamic structure\n",
        "\n",
        "if extract_powerbi_artifacts_only == False:\n",
        "    try:\n",
        "        df_notebooks = get_details(df_workspaces, \"WorkspaceId\" , \"Notebook\")\n",
        "\n",
        "        if df_notebooks.empty or 'id' not in df_notebooks.columns:\n",
        "            print(\"df_notebooks dataframe is empty\")\n",
        "        else:\n",
        "            # Remove null rows\n",
        "            df_notebooks = df_notebooks[df_notebooks['id'].notna()]\n",
        "\n",
        "            # Remove columns\n",
        "            df_notebooks = df_notebooks.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
        "\n",
        "            # Rename columns\n",
        "            df_notebooks = df_notebooks.rename(columns = {\n",
        "                                            'id': 'NotebookId',\n",
        "                                            'name': 'Name',\n",
        "                                            'description': 'Description',\n",
        "                                            'state': 'State',\n",
        "                                            'createdById': 'CreatedById',\n",
        "                                            'modifiedById': 'ModifiedById',\n",
        "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
        "                                            'createdDate': 'CreatedDateTime'\n",
        "                                            }, errors='ignore'\n",
        "                                        )\n",
        "\n",
        "\n",
        "            # Change id column values to upper case\n",
        "            to_upper_if_exists(df_notebooks, 'NotebookId')  \n",
        "            to_upper_if_exists(df_notebooks, 'CreatedById')  \n",
        "            to_upper_if_exists(df_notebooks, 'ModifiedById') \n",
        "\n",
        "            # Add calculated columns\n",
        "            df_notebooks['CreatedDate'] = df_notebooks['CreatedDateTime'].str.slice(0, 10)\n",
        "            df_notebooks['LastUpdatedDate'] = df_notebooks['LastUpdatedDateTime'].str.slice(0, 10)\n",
        "\n",
        "            # Add data to write array\n",
        "            write_list.append({\"df\": df_notebooks, \"name\" : \"notebooks\"})\n",
        "\n",
        "            if display_data:\n",
        "                display(df_notebooks)\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "aa4308c5-5115-4840-8594-28b00b7fa9ab"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Environments"
      ],
      "metadata": {},
      "id": "b72a4dcc-1b65-4a5d-9166-8a97a2450d4d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Environments (within Workspaces)\n",
        "\n",
        "if extract_powerbi_artifacts_only == False:\n",
        "    try:\n",
        "        df_env = get_details(df_workspaces, \"WorkspaceId\" , \"Environment\")\n",
        "\n",
        "        if df_env.empty or 'id' not in df_env.columns:\n",
        "            print(\"df_env dataframe is empty\")\n",
        "        else:\n",
        "            # Remove null rows\n",
        "            df_env = df_env[df_env['id'].notna()]\n",
        "\n",
        "            # Remove columns\n",
        "            df_env = df_env.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
        "\n",
        "            # Rename columns\n",
        "            df_env = df_env.rename(columns = {\n",
        "                                            'id': 'EnvironmentId',\n",
        "                                            'name': 'Name',\n",
        "                                            'description': 'Description',\n",
        "                                            'state': 'State',\n",
        "                                            'createdById': 'CreatedById',\n",
        "                                            'modifiedById': 'ModifiedById',\n",
        "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
        "                                            'createdDate': 'CreatedDateTime'\n",
        "                                            }, errors='ignore'\n",
        "                                        )\n",
        "\n",
        "\n",
        "            # Change id column values to upper case\n",
        "            to_upper_if_exists(df_env, 'EnvironmentId') \n",
        "            to_upper_if_exists(df_env, 'CreatedById') \n",
        "            to_upper_if_exists(df_env, 'ModifiedById')  \n",
        "\n",
        "            # Add calculated columns\n",
        "            df_env['CreatedDate'] = df_env['CreatedDateTime'].str.slice(0, 10)\n",
        "            df_env['LastUpdatedDate'] = df_env['LastUpdatedDateTime'].str.slice(0, 10)\n",
        "\n",
        "            # Add data to write array\n",
        "            write_list.append({\"df\": df_env, \"name\" : \"environments\"})\n",
        "\n",
        "            if display_data:\n",
        "                display(df_env)\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "87f73940-2ff5-4a90-a8d0-a5bc51108385"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Reflex"
      ],
      "metadata": {},
      "id": "ad0d0bf4-1eb1-4be7-8c5a-b4688a7e71fb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Reflex (within Workspaces)\n",
        "\n",
        "if extract_powerbi_artifacts_only == False:\n",
        "    try:\n",
        "        df_reflex = get_details(df_workspaces, \"WorkspaceId\" , \"Reflex\")\n",
        "\n",
        "        if df_reflex.empty or 'id' not in df_reflex.columns:\n",
        "            print(\"df_reflex dataframe is empty\")\n",
        "        else:\n",
        "            # Remove null rows\n",
        "            df_reflex = df_reflex[df_reflex['id'].notna()]\n",
        "\n",
        "            # Remove columns\n",
        "            df_reflex = df_reflex.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
        "\n",
        "            # Rename columns\n",
        "            df_reflex = df_reflex.rename(columns = {\n",
        "                                            'id': 'ReflexId',\n",
        "                                            'name': 'Name',\n",
        "                                            'description': 'Description',\n",
        "                                            'state': 'State',\n",
        "                                            'createdById': 'CreatedById',\n",
        "                                            'modifiedById': 'ModifiedById',\n",
        "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
        "                                            'createdDate': 'CreatedDateTime'\n",
        "                                            }, errors='ignore'\n",
        "                                        )\n",
        "\n",
        "\n",
        "            # Change id column values to upper case \n",
        "            to_upper_if_exists(df_reflex, 'ReflexId')  \n",
        "            to_upper_if_exists(df_reflex, 'CreatedById')   \n",
        "            to_upper_if_exists(df_reflex, 'ModifiedById')  \n",
        "\n",
        "            # Add calculated columns\n",
        "            df_reflex['CreatedDate'] = df_reflex['CreatedDateTime'].str.slice(0, 10)\n",
        "            df_reflex['LastUpdatedDate'] = df_reflex['LastUpdatedDateTime'].str.slice(0, 10)\n",
        "\n",
        "            # Add data to write array\n",
        "            write_list.append({\"df\": df_reflex, \"name\" : \"reflexes\"})\n",
        "\n",
        "            if display_data:\n",
        "                display(df_reflex)\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "2ed9c2bd-5a8a-43c4-9dd1-a8220252d364"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ML-Models"
      ],
      "metadata": {},
      "id": "2f6a49f9-e865-46fb-a26a-b46b36cabe6c"
    },
    {
      "cell_type": "code",
      "source": [
        "# ML-Models (within Workspaces)\n",
        "\n",
        "if extract_powerbi_artifacts_only == False:\n",
        "    try:\n",
        "        df_mlm = get_details(df_workspaces, \"WorkspaceId\" , \"MLModel\")\n",
        "\n",
        "        if df_mlm.empty or 'id' not in df_mlm.columns:\n",
        "            print(\"df_mlm dataframe is empty\")\n",
        "        else:\n",
        "            # Remove null rows\n",
        "            df_mlm = df_mlm[df_mlm['id'].notna()]\n",
        "\n",
        "            # Remove columns\n",
        "            df_mlm = df_mlm.drop(columns=['users', 'createdBy', 'modifiedBy'], errors='ignore')\n",
        "\n",
        "            # Rename columns\n",
        "            df_mlm = df_mlm.rename(columns = {\n",
        "                                            'id': 'MLModelId',\n",
        "                                            'name': 'Name',\n",
        "                                            'description': 'Description',\n",
        "                                            'state': 'State',\n",
        "                                            'createdById': 'CreatedById',\n",
        "                                            'modifiedById': 'ModifiedById',\n",
        "                                            'lastUpdatedDate': 'LastUpdatedDateTime',\n",
        "                                            'createdDate': 'CreatedDateTime'\n",
        "                                            }, errors='ignore'\n",
        "                                        )\n",
        "\n",
        "\n",
        "            # Change id column values to upper case\n",
        "            to_upper_if_exists(df_mlm, 'MLModelId')  \n",
        "            to_upper_if_exists(df_mlm, 'CreatedById') \n",
        "            to_upper_if_exists(df_mlm, 'ModifiedById')  \n",
        "\n",
        "            # Add calculated columns\n",
        "            df_mlm['CreatedDate'] = df_mlm['CreatedDateTime'].str.slice(0, 10)\n",
        "            df_mlm['LastUpdatedDate'] = df_mlm['LastUpdatedDateTime'].str.slice(0, 10)\n",
        "\n",
        "            # Add data to write array\n",
        "            write_list.append({\"df\": df_mlm, \"name\" : \"ml_models\"})\n",
        "\n",
        "            if display_data:\n",
        "                display(df_mlm)\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "8b649daa-f695-40a1-abc4-ef68870bbccd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datasource Instances"
      ],
      "metadata": {},
      "id": "989b582d-4f18-43bc-a062-366c3e8a3b48"
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasource Instances (same level as Workspaces)\n",
        "# [Info]: It must have a partially dynamic structure\n",
        "\n",
        "try:\n",
        "    df_dts_inst = pd.json_normalize(pd.json_normalize(results).explode(\"datasourceInstances\")[\"datasourceInstances\"])\n",
        "\n",
        "    if df_dts_inst.empty:\n",
        "        print(\"df_dts_inst dataframe is empty\")\n",
        "    else:\n",
        "        # Rename columns\n",
        "        df_dts_inst = df_dts_inst.rename(columns = {\n",
        "                                        'datasourceId' : 'DatasourceId',\n",
        "                                        'datasourceType': 'DatasourceType',\n",
        "                                        'gatewayId': 'GatewayId'\n",
        "                                        }, errors='ignore'\n",
        "                                    )\n",
        "\n",
        "        # Change id column values to upper case\n",
        "        to_upper_if_exists(df_dts_inst, 'DatasourceId') \n",
        "        to_upper_if_exists(df_dts_inst, 'GatewayId') \n",
        "\n",
        "        # Add data to write array\n",
        "        write_list.append({\"df\": df_dts_inst, \"name\" : \"datasource_instances\"}) \n",
        "\n",
        "        if display_data:\n",
        "            display(df_dts_inst)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "038bab1d-559c-407d-b2f0-40960beb3195"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Misconfigured Datasource Instances"
      ],
      "metadata": {},
      "id": "ca06d80c-e5de-4d33-a3c2-2da03e8ebf68"
    },
    {
      "cell_type": "code",
      "source": [
        "# Misconfigured Datasource Instances (same level as Workspaces)\n",
        "# [Info]: It must have a partially dynamic structure\n",
        "\n",
        "try:\n",
        "    df_mc_dts_inst = pd.json_normalize(pd.json_normalize(results).explode(\"misconfiguredDatasourceInstances\")[\"misconfiguredDatasourceInstances\"])\n",
        "\n",
        "    if df_mc_dts_inst.empty:\n",
        "        print(\"df_mc_dts_inst dataframe is empty\")\n",
        "    else:\n",
        "        # Rename columns\n",
        "        df_mc_dts_inst = df_mc_dts_inst.rename(columns = {\n",
        "                                        'datasourceId' : 'DatasourceId',\n",
        "                                        'datasourceType': 'DatasourceType'\n",
        "                                        }, errors='ignore'\n",
        "                                    )\n",
        "\n",
        "        # Change id column values to upper case\n",
        "        to_upper_if_exists(df_mc_dts_inst, 'DatasourceId')\n",
        "        \n",
        "        # Add data to write array\n",
        "        write_list.append({\"df\": df_mc_dts_inst, \"name\" : \"misconfigured_datasource_instances\"}) \n",
        "\n",
        "        if display_data:\n",
        "            display(df_mc_dts_inst)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "8bf77e02-39f5-464c-8dc4-fabef8017b6a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract Item Users\n",
        "Extract users access information from all object types (reports, datasets, lakehouse, notebooks, etc.) to create a consolidated item_users table"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "a960361d-1fff-41d5-962c-30f4545db244"
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_item_users(results):\n",
        "    \"\"\"\n",
        "    Extract all users information from different object types and create an item_users table.\n",
        "    \n",
        "    This function iterates through all workspaces and their contained objects to extract user access information.\n",
        "    It handles different object types including:\n",
        "    - Power BI artifacts: reports, datasets (semantic models), dashboards, dataflows, datamarts\n",
        "    - Fabric artifacts: Lakehouse, Notebook, DataPipeline, Eventstream, Environment, MLModel, Reflex, KQLDatabase, Eventhouse, warehouses\n",
        "    \n",
        "    Returns a pandas DataFrame with columns:\n",
        "    - WorkspaceId: The workspace containing the item\n",
        "    - ItemId: The ID of the item (report, dataset, etc.)\n",
        "    - ItemType: The type of the item (Report, Dataset, Lakehouse, etc.)\n",
        "    - ItemName: The name of the item\n",
        "    - AccessRight: The access right (varies by object type)\n",
        "    - EmailAddress: User's email address\n",
        "    - DisplayName: User's display name\n",
        "    - Identifier: User identifier\n",
        "    - GraphId: Azure AD Graph ID\n",
        "    - PrincipalType: Type of principal (User, Group, etc.)\n",
        "    - UserType: Type of user (Member, Guest, etc.)\n",
        "    \"\"\"\n",
        "    \n",
        "    item_users = []\n",
        "    \n",
        "    # Define object types and their corresponding access right field names\n",
        "    object_types = {\n",
        "        # Power BI artifacts\n",
        "        'reports': 'reportUserAccessRight',\n",
        "        'datasets': 'datasetUserAccessRight', \n",
        "        'dashboards': 'dashboardUserAccessRight',\n",
        "        'dataflows': 'dataflowUserAccessRight',\n",
        "        'datamarts': 'datamartUserAccessRight',\n",
        "        \n",
        "        # Fabric artifacts\n",
        "        'Lakehouse': 'artifactUserAccessRight',\n",
        "        'Notebook': 'artifactUserAccessRight',\n",
        "        'DataPipeline': 'artifactUserAccessRight',\n",
        "        'Eventstream': 'artifactUserAccessRight',\n",
        "        'Environment': 'artifactUserAccessRight',\n",
        "        'MLModel': 'artifactUserAccessRight',\n",
        "        'Reflex': 'artifactUserAccessRight',\n",
        "        'KQLDatabase': 'artifactUserAccessRight',\n",
        "        'Eventhouse': 'artifactUserAccessRight',\n",
        "        'warehouses': 'artifactUserAccessRight'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        # Process each workspace result\n",
        "        for result in results:\n",
        "            if 'workspaces' in result:\n",
        "                for workspace in result['workspaces']:\n",
        "                    workspace_id = workspace.get('id', '')\n",
        "                    \n",
        "                    # Process each object type in the workspace\n",
        "                    for object_type, access_right_field in object_types.items():\n",
        "                        if object_type in workspace:\n",
        "                            objects = workspace[object_type]\n",
        "                            \n",
        "                            # Handle both list and non-list objects\n",
        "                            if not isinstance(objects, list):\n",
        "                                continue\n",
        "                                \n",
        "                            for obj in objects:\n",
        "                                item_id = obj.get('id', obj.get('objectId', ''))  # Some objects use 'objectId' instead of 'id'\n",
        "                                item_name = obj.get('name', obj.get('displayName', ''))  # Some objects use 'displayName'\n",
        "                                \n",
        "                                # Extract users information\n",
        "                                if 'users' in obj and obj['users']:\n",
        "                                    for user in obj['users']:\n",
        "                                        access_right = user.get(access_right_field, '')\n",
        "                                        \n",
        "                                        # If the specific access right field doesn't exist, try common alternatives\n",
        "                                        if not access_right:\n",
        "                                            access_right = user.get('artifactUserAccessRight', \n",
        "                                                                   user.get('groupUserAccessRight', ''))\n",
        "                                        \n",
        "                                        item_user = {\n",
        "                                            'WorkspaceId': workspace_id.upper() if workspace_id else '',\n",
        "                                            'ItemId': item_id.upper() if item_id else '',\n",
        "                                            'ItemType': object_type,\n",
        "                                            'ItemName': item_name,\n",
        "                                            'AccessRight': access_right,\n",
        "                                            'EmailAddress': user.get('emailAddress', ''),\n",
        "                                            'DisplayName': user.get('displayName', ''),\n",
        "                                            'Identifier': user.get('identifier', ''),\n",
        "                                            'GraphId': user.get('graphId', '').upper() if user.get('graphId') else '',\n",
        "                                            'PrincipalType': user.get('principalType', ''),\n",
        "                                            'UserType': user.get('userType', '')\n",
        "                                        }\n",
        "                                        \n",
        "                                        item_users.append(item_user)\n",
        "    \n",
        "    except Exception as ex:\n",
        "        print(f\"Error extracting item users: {ex}\")\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df_item_users = pd.DataFrame(item_users)\n",
        "    \n",
        "    # Remove rows where both ItemId and ItemName are empty\n",
        "    if not df_item_users.empty:\n",
        "        df_item_users = df_item_users[\n",
        "            (df_item_users['ItemId'] != '') | (df_item_users['ItemName'] != '')\n",
        "        ]\n",
        "    \n",
        "    return df_item_users"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "35225e32-dd41-44f5-b7ad-a422e8ebb6e1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract item access information from all object types\n",
        "try:\n",
        "    print(\"Extracting item user information...\")\n",
        "    df_item_users = extract_item_users(results)\n",
        "    \n",
        "    if df_item_users.empty:\n",
        "        print(\"df_item_users dataframe is empty\")\n",
        "    else:\n",
        "        print(f\"Extracted {len(df_item_users)} item user records\")\n",
        "        \n",
        "        # Add data to write array\n",
        "        write_list.append({\"df\": df_item_users, \"name\": \"item_users\"})\n",
        "        \n",
        "        if display_data:\n",
        "            display(df_item_users)\n",
        "            \n",
        "except Exception as ex:\n",
        "    print(f\"Error extracting item users: {ex}\")\n",
        "    # Create empty dataframe to prevent errors in downstream processing\n",
        "    df_item_users = pd.DataFrame(columns=[\n",
        "        'WorkspaceId', 'ItemId', 'ItemType', 'ItemName', 'AccessRight', \n",
        "        'EmailAddress', 'DisplayName', 'Identifier', 'GraphId', 'PrincipalType', 'UserType'\n",
        "    ])\n",
        "    write_list.append({\"df\": df_item_users, \"name\": \"item_users\"})"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "bf7d7d5e-ffc8-4e75-8760-e4d3f34eb53e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write data to Lakehouse\n",
        "\n",
        "This last sequence writes the extracted tenant meta data to FUAM Lakehouse\n",
        "\n",
        "The **write_list** array will be iterated, which writes the extracted data to tables"
      ],
      "metadata": {},
      "id": "a436990f-a6ff-46cc-b5c3-a0ed978ad119"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to dynamically write different data frames to lakehouse. Depending on the keep_history variable data get added daily or overwritten in the respective delta tables. If the pandas dataframe is empty, no result will be written to Delta tables\n",
        "The following parameters need to be configured:\n",
        "- df: Pandas Dataframe containing the data to be written\n",
        "- table_name: Target table name"
      ],
      "metadata": {},
      "id": "cb14dc20-4070-4a9f-bf33-d3820f448ae1"
    },
    {
      "cell_type": "code",
      "source": [
        "def write_data_to_gold(df, table_name):\n",
        "    if df.empty:\n",
        "        print(\"No data for table \" + table_name + \" existing\")\n",
        "    else:\n",
        "        # Transfer pandas df to spark df\n",
        "        spark_df = spark.createDataFrame(df)\n",
        "        #spark_df.printSchema()\n",
        "        spark_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(table_name)"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "03c8c285-bfe3-4087-b735-6f25bb8f5cb3"
    },
    {
      "cell_type": "code",
      "source": [
        "for it in write_list:\n",
        "    try:\n",
        "        print(\"Artifact:\", it[\"name\"])\n",
        "        print(f\"Loading to\", it[\"name\"], \"table\")\n",
        "        print(\"--------\")\n",
        "        write_data_to_gold(it[\"df\"], it[\"name\"])\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "ab61b705-73ed-4c0c-a642-7225ed7b042c"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_empty_table_if_missing (table_name, default_columns) : \n",
        "    if not spark.catalog.tableExists(table_name):\n",
        "        print(\"Create table \" + table_name)\n",
        "        records = []\n",
        "        dummy_record = {}\n",
        "        for col in default_columns:\n",
        "            dummy_record[col] = \"Dummy\"\n",
        "        records.append(dummy_record)\n",
        "        pdf = pd.DataFrame(   records )\n",
        "        spark_df = spark.createDataFrame( pdf)\n",
        "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(table_name)"
      ],
      "outputs": [],
      "execution_count": 42,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "2f5e4e73-9145-4cff-b678-dc07cc38cdf7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create missing tables\n",
        "Create empty tables in case there has been an error while creating the tables from API.\n",
        "\n",
        "This step is important to prevent breaking the semantic model in a case of error."
      ],
      "metadata": {},
      "id": "841b79e9-4182-4735-823a-3df9dd892cb7"
    },
    {
      "cell_type": "code",
      "source": [
        "create_empty_table_if_missing(\"reports\", [\"WorkspaceId\",\"ReportId\", \"SemanticModelId\", \"OriginalReportObjectId\", \"SemanticModelWorkspaceId\" ])\n",
        "create_empty_table_if_missing(\"semantic_models\", [\"WorkspaceId\",\"SemanticModelId\"])\n",
        "create_empty_table_if_missing(\"dataflows\", [\"WorkspaceId\",\"DataflowId\",\"Name\",\"description\", \"Generation\", \"ModifiedDateTime\"])\n",
        "create_empty_table_if_missing(\"dashboards\", [\"WorkspaceId\",\"DashboardId\"])\n",
        "create_empty_table_if_missing(\"datasource_instances\", [\"DatasourceId\",\"GatewayId\"])\n",
        "create_empty_table_if_missing(\"workspaces_scanned_users\", [\"WorkspaceId\",\"GraphId\", \"Identifier\", \"profile.id\"])\n",
        "create_empty_table_if_missing(\"eventhouses\", [\"WorkspaceId\",\"EventhouseId\", \"Name\"])\n",
        "create_empty_table_if_missing(\"pipelines\", [\"WorkspaceId\",\"EventhouseId\", \"Name\"])\n",
        "create_empty_table_if_missing(\"reflexes\", [\"WorkspaceId\", \"ReflexId\", \"Name\"])\n",
        "create_empty_table_if_missing(\"notebooks\", [\"WorkspaceId\",\"NotebookId\", \"Name\"])\n",
        "create_empty_table_if_missing(\"environments\", [\"WorkspaceId\", \"EnvironmentId\", \"Name\"])"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "7f961aa6-cee5-4d0b-a9e3-a106bec76e8b"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "widgets": {},
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d"
      },
      "environment": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}