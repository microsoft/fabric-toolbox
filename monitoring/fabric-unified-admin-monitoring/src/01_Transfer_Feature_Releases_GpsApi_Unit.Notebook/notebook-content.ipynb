{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785109b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48325bc2",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (can be passed from Pipeline)\n",
    "fabric_gps_api_url = \"https://fabric-gps.com/api/releases\"\n",
    "modified_within_days = 90  # Get features modified in last N days (1-30 per API, but we'll fetch all)\n",
    "page_size = 200  # Max allowed by API\n",
    "include_planned = True  # Include planned features (not yet shipped)\n",
    "include_shipped = True  # Include shipped features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c7883",
   "metadata": {},
   "source": [
    "## Step 1: Fetch Feature Releases from Fabric GPS API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb075233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_fabric_gps_releases(base_url, page_size=200, modified_within_days=None, include_planned=True, include_shipped=True):\n",
    "    \"\"\"\n",
    "    Fetch all releases from Fabric GPS API with pagination\n",
    "    Returns list of release objects\n",
    "    \"\"\"\n",
    "    all_releases = []\n",
    "    page = 1\n",
    "    \n",
    "    print(f\"ðŸ”„ Fetching from Fabric GPS API: {base_url}\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Build query parameters\n",
    "            params = {\n",
    "                \"page\": page,\n",
    "                \"page_size\": page_size\n",
    "            }\n",
    "            \n",
    "            if modified_within_days and modified_within_days <= 30:\n",
    "                params[\"modified_within_days\"] = modified_within_days\n",
    "            \n",
    "            # Make request\n",
    "            response = requests.get(base_url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract releases\n",
    "            releases = data.get(\"data\", [])\n",
    "            \n",
    "            if not releases:\n",
    "                print(f\"  â†’ No more data on page {page}\")\n",
    "                break\n",
    "            \n",
    "            # Filter by release_status if specified\n",
    "            filtered_releases = []\n",
    "            for release in releases:\n",
    "                status = release.get(\"release_status\", \"\")\n",
    "                \n",
    "                # Apply filters\n",
    "                if status == \"Planned\" and not include_planned:\n",
    "                    continue\n",
    "                if status == \"Shipped\" and not include_shipped:\n",
    "                    continue\n",
    "                    \n",
    "                filtered_releases.append(release)\n",
    "            \n",
    "            all_releases.extend(filtered_releases)\n",
    "            \n",
    "            # Check pagination\n",
    "            pagination = data.get(\"pagination\", {})\n",
    "            has_next = pagination.get(\"has_next\", False)\n",
    "            total_items = pagination.get(\"total_items\", 0)\n",
    "            \n",
    "            print(f\"  â†’ Page {page}: Fetched {len(filtered_releases)} releases (Total: {len(all_releases)}/{total_items})\")\n",
    "            \n",
    "            if not has_next:\n",
    "                print(f\"  âœ… Reached end of data\")\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error fetching page {page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nâœ… Total releases fetched: {len(all_releases)}\")\n",
    "    return all_releases\n",
    "\n",
    "# Fetch all releases\n",
    "releases_data = fetch_all_fabric_gps_releases(\n",
    "    fabric_gps_api_url, \n",
    "    page_size=page_size,\n",
    "    modified_within_days=modified_within_days if modified_within_days <= 30 else None,\n",
    "    include_planned=include_planned,\n",
    "    include_shipped=include_shipped\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Sample releases:\")\n",
    "for release in releases_data[:3]:\n",
    "    print(f\"  - {release.get('feature_name')} ({release.get('release_type')}) - {release.get('product_name')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c24c88",
   "metadata": {},
   "source": [
    "## Step 2: Transform to FUAM Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eebb7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fabric_gps_to_fuam(releases):\n",
    "    \"\"\"\n",
    "    Transform Fabric GPS release data to FUAM feature_releases schema\n",
    "    \"\"\"\n",
    "    transformed = []\n",
    "    \n",
    "    for release in releases:\n",
    "        # Parse release date\n",
    "        release_date_str = release.get(\"release_date\")\n",
    "        try:\n",
    "            if release_date_str:\n",
    "                release_date = datetime.strptime(release_date_str, \"%Y-%m-%d\")\n",
    "            else:\n",
    "                release_date = None  # Planned features may not have a date yet\n",
    "        except:\n",
    "            release_date = None\n",
    "        \n",
    "        # Parse last modified\n",
    "        last_modified_str = release.get(\"last_modified\")\n",
    "        try:\n",
    "            last_modified = datetime.strptime(last_modified_str, \"%Y-%m-%d\") if last_modified_str else datetime.now()\n",
    "        except:\n",
    "            last_modified = datetime.now()\n",
    "        \n",
    "        # Determine if preview\n",
    "        release_type = release.get(\"release_type\", \"\")\n",
    "        is_preview = \"preview\" in release_type.lower()\n",
    "        \n",
    "        # Map product_name to workload (align with FUAM naming)\n",
    "        product_name = release.get(\"product_name\", \"Unknown\")\n",
    "        workload_mapping = {\n",
    "            \"Power BI\": \"Power BI\",\n",
    "            \"Data Factory\": \"Data Factory\",\n",
    "            \"Data Engineering\": \"Data Engineering\",\n",
    "            \"Data Science\": \"Data Science\",\n",
    "            \"Data Warehouse\": \"Data Warehouse\",\n",
    "            \"Real-Time Intelligence\": \"Real-Time Intelligence\",\n",
    "            \"OneLake\": \"Data Engineering\",\n",
    "            \"Administration, Governance and Security\": \"Governance\",\n",
    "            \"Cosmos DB (NoSQL)\": \"Cosmos DB\"\n",
    "        }\n",
    "        workload = workload_mapping.get(product_name, product_name)\n",
    "        \n",
    "        # Create feature record\n",
    "        feature = {\n",
    "            \"feature_id\": release.get(\"release_item_id\"),  # Use Fabric GPS UUID\n",
    "            \"feature_name\": release.get(\"feature_name\", \"Unknown\"),\n",
    "            \"feature_description\": release.get(\"feature_description\"),\n",
    "            \"workload\": workload,\n",
    "            \"product_name\": product_name,\n",
    "            \"release_date\": release_date,\n",
    "            \"release_type\": release_type,\n",
    "            \"release_status\": release.get(\"release_status\", \"Unknown\"),\n",
    "            \"is_preview\": is_preview,\n",
    "            \"is_planned\": release.get(\"release_status\") == \"Planned\",\n",
    "            \"is_shipped\": release.get(\"release_status\") == \"Shipped\",\n",
    "            \"last_modified\": last_modified,\n",
    "            \"source_url\": f\"https://fabric-gps.com/api/releases?release_item_id={release.get('release_item_id')}\",\n",
    "            \"source\": \"Fabric GPS API\",\n",
    "            \"extracted_date\": datetime.now()\n",
    "        }\n",
    "        \n",
    "        transformed.append(feature)\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "print(\"ðŸ”„ Transforming releases to FUAM schema...\")\n",
    "features_data = transform_fabric_gps_to_fuam(releases_data)\n",
    "print(f\"âœ… Transformed {len(features_data)} features\")\n",
    "\n",
    "# Statistics\n",
    "preview_count = sum(1 for f in features_data if f[\"is_preview\"])\n",
    "planned_count = sum(1 for f in features_data if f[\"is_planned\"])\n",
    "shipped_count = sum(1 for f in features_data if f[\"is_shipped\"])\n",
    "\n",
    "print(f\"\\nðŸ“Š Feature Breakdown:\")\n",
    "print(f\"  - Preview features: {preview_count}\")\n",
    "print(f\"  - Planned features: {planned_count}\")\n",
    "print(f\"  - Shipped features: {shipped_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd949aa",
   "metadata": {},
   "source": [
    "## Step 3: Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4feed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"feature_description\", StringType(), True),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"release_date\", TimestampType(), True),  # Nullable for planned features\n",
    "    StructField(\"release_type\", StringType(), True),\n",
    "    StructField(\"release_status\", StringType(), True),\n",
    "    StructField(\"is_preview\", BooleanType(), False),\n",
    "    StructField(\"is_planned\", BooleanType(), False),\n",
    "    StructField(\"is_shipped\", BooleanType(), False),\n",
    "    StructField(\"last_modified\", TimestampType(), False),\n",
    "    StructField(\"source_url\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"extracted_date\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "df_features = spark.createDataFrame(features_data, schema=schema)\n",
    "\n",
    "print(f\"âœ… Created DataFrame with {df_features.count()} rows\")\n",
    "df_features.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7f50e",
   "metadata": {},
   "source": [
    "## Step 4: Write to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc62e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table path\n",
    "table_name = \"feature_releases_roadmap\"\n",
    "table_path = f\"Tables/{table_name}\"\n",
    "\n",
    "print(f\"ðŸ”„ Writing to Delta table: {table_name}\")\n",
    "\n",
    "# Write with merge logic (upsert)\n",
    "try:\n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    # Check if table exists\n",
    "    if DeltaTable.isDeltaTable(spark, table_path):\n",
    "        print(\"  â†’ Table exists, performing MERGE (upsert)...\")\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(spark, table_path)\n",
    "        \n",
    "        # Merge: Update existing, insert new\n",
    "        # Use last_modified to track changes\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df_features.alias(\"source\"),\n",
    "            \"target.feature_id = source.feature_id\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"source.last_modified > target.last_modified\",\n",
    "            set={\n",
    "                \"feature_name\": \"source.feature_name\",\n",
    "                \"feature_description\": \"source.feature_description\",\n",
    "                \"workload\": \"source.workload\",\n",
    "                \"product_name\": \"source.product_name\",\n",
    "                \"release_date\": \"source.release_date\",\n",
    "                \"release_type\": \"source.release_type\",\n",
    "                \"release_status\": \"source.release_status\",\n",
    "                \"is_preview\": \"source.is_preview\",\n",
    "                \"is_planned\": \"source.is_planned\",\n",
    "                \"is_shipped\": \"source.is_shipped\",\n",
    "                \"last_modified\": \"source.last_modified\",\n",
    "                \"source_url\": \"source.source_url\",\n",
    "                \"extracted_date\": \"source.extracted_date\"\n",
    "            }\n",
    "        ).whenNotMatchedInsertAll(\n",
    "        ).execute()\n",
    "        \n",
    "        print(\"  âœ… MERGE completed\")\n",
    "    else:\n",
    "        print(\"  â†’ Table doesn't exist, creating new table...\")\n",
    "        df_features.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "        print(\"  âœ… Table created\")\n",
    "    \n",
    "    # Show final count\n",
    "    final_count = spark.read.format(\"delta\").load(table_path).count()\n",
    "    print(f\"\\nâœ… Total records in {table_name}: {final_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error writing to Delta: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce4ca77",
   "metadata": {},
   "source": [
    "## Step 5: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š Feature Release Roadmap Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df_summary = spark.read.format(\"delta\").load(table_path)\n",
    "\n",
    "# By workload/product\n",
    "print(\"\\nðŸ”¸ By Workload:\")\n",
    "df_summary.groupBy(\"workload\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# By release type\n",
    "print(\"\\nðŸ”¸ By Release Type:\")\n",
    "df_summary.groupBy(\"release_type\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# By release status\n",
    "print(\"\\nðŸ”¸ By Release Status:\")\n",
    "df_summary.groupBy(\"release_status\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# Preview vs GA\n",
    "print(\"\\nðŸ”¸ Preview vs GA:\")\n",
    "df_summary.groupBy(\"is_preview\").count().show(truncate=False)\n",
    "\n",
    "# Planned vs Shipped\n",
    "print(\"\\nðŸ”¸ Planned vs Shipped:\")\n",
    "df_summary.select(\"is_planned\", \"is_shipped\").groupBy(\"is_planned\", \"is_shipped\").count().show(truncate=False)\n",
    "\n",
    "# Recent modifications (last 30 days)\n",
    "recent_cutoff = datetime.now() - timedelta(days=30)\n",
    "recent_count = df_summary.filter(F.col(\"last_modified\") >= recent_cutoff).count()\n",
    "print(f\"\\nðŸ”¸ Features modified in last 30 days: {recent_count}\")\n",
    "\n",
    "# Upcoming releases (planned)\n",
    "planned_count = df_summary.filter(F.col(\"is_planned\") == True).count()\n",
    "print(f\"ðŸ”¸ Planned features (roadmap): {planned_count}\")\n",
    "\n",
    "# Show upcoming preview features\n",
    "print(\"\\nðŸ”¸ Upcoming Preview Features (sample):\")\n",
    "df_summary.filter((F.col(\"is_preview\") == True) & (F.col(\"is_planned\") == True)) \\\n",
    "    .select(\"feature_name\", \"product_name\", \"release_date\", \"release_status\") \\\n",
    "    .orderBy(F.col(\"release_date\").asc()) \\\n",
    "    .show(10, truncate=60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… Transfer Feature Releases GpsApi Unit - COMPLETED\")\n",
    "print(f\"ðŸ’¡ Tip: Use this table for roadmap planning and preview feature tracking\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
