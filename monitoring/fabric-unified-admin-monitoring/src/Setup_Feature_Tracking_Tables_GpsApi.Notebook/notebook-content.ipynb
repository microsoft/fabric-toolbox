{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e13348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d714890",
   "metadata": {},
   "source": [
    "## Step 1: Create `feature_releases` Table (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Creating table: feature_releases\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "schema_feature_releases = StructType([\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"release_date\", TimestampType(), False),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"is_preview\", BooleanType(), False),\n",
    "    StructField(\"source_url\", StringType(), True),\n",
    "    StructField(\"extracted_date\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Create empty DataFrame\n",
    "df_feature_releases = spark.createDataFrame([], schema_feature_releases)\n",
    "\n",
    "# Write to Delta\n",
    "table_path = \"Tables/feature_releases\"\n",
    "df_feature_releases.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "\n",
    "print(\"‚úÖ Table created: feature_releases\")\n",
    "print(f\"   Location: {table_path}\")\n",
    "print(\"\\n   Schema:\")\n",
    "for field in schema_feature_releases.fields:\n",
    "    print(f\"     - {field.name}: {field.dataType.simpleString()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282f309",
   "metadata": {},
   "source": [
    "## Step 1b: Create `feature_releases_roadmap` Table (Enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Creating table: feature_releases_roadmap (Fabric GPS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "schema_roadmap = StructType([\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"feature_description\", StringType(), True),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"release_date\", TimestampType(), True),  # Nullable for planned features\n",
    "    StructField(\"release_type\", StringType(), True),\n",
    "    StructField(\"release_status\", StringType(), True),\n",
    "    StructField(\"is_preview\", BooleanType(), False),\n",
    "    StructField(\"is_planned\", BooleanType(), False),\n",
    "    StructField(\"is_shipped\", BooleanType(), False),\n",
    "    StructField(\"last_modified\", TimestampType(), False),\n",
    "    StructField(\"source_url\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"extracted_date\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Create empty DataFrame\n",
    "df_roadmap = spark.createDataFrame([], schema_roadmap)\n",
    "\n",
    "# Write to Delta\n",
    "table_path = \"Tables/feature_releases_roadmap\"\n",
    "df_roadmap.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "\n",
    "print(\"‚úÖ Table created: feature_releases_roadmap\")\n",
    "print(f\"   Location: {table_path}\")\n",
    "print(\"\\n   Schema:\")\n",
    "for field in schema_roadmap.fields:\n",
    "    print(f\"     - {field.name}: {field.dataType.simpleString()}\")\n",
    "print(\"\\n   üí° This table includes:\")\n",
    "print(\"      - Complete feature descriptions\")\n",
    "print(\"      - Planned/future features (roadmap)\")\n",
    "print(\"      - Historical change tracking (last_modified)\")\n",
    "print(\"      - Release status (Planned, In Development, Shipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c72b90",
   "metadata": {},
   "source": [
    "## Step 2: Create `preview_features_active` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Creating table: preview_features_active\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "schema_preview_active = StructType([\n",
    "    StructField(\"setting_name\", StringType(), False),\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"similarity_score\", DoubleType(), False),\n",
    "    StructField(\"is_enabled\", BooleanType(), False),\n",
    "    StructField(\"delegate_to_tenant\", BooleanType(), True),\n",
    "    StructField(\"detected_date\", TimestampType(), False),\n",
    "    StructField(\"release_date\", TimestampType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"source_url\", StringType(), True),\n",
    "    StructField(\"days_since_release\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create empty DataFrame\n",
    "df_preview_active = spark.createDataFrame([], schema_preview_active)\n",
    "\n",
    "# Write to Delta\n",
    "table_path = \"Tables/preview_features_active\"\n",
    "df_preview_active.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "\n",
    "print(\"‚úÖ Table created: preview_features_active\")\n",
    "print(f\"   Location: {table_path}\")\n",
    "print(\"\\n   Schema:\")\n",
    "for field in schema_preview_active.fields:\n",
    "    print(f\"     - {field.name}: {field.dataType.simpleString()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c869f",
   "metadata": {},
   "source": [
    "## Step 3: Create `feature_alerts` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1505fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Creating table: feature_alerts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "schema_alerts = StructType([\n",
    "    StructField(\"alert_id\", StringType(), False),\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"alert_type\", StringType(), False),\n",
    "    StructField(\"severity\", StringType(), False),\n",
    "    StructField(\"message\", StringType(), False),\n",
    "    StructField(\"setting_name\", StringType(), True),\n",
    "    StructField(\"similarity_score\", DoubleType(), True),\n",
    "    StructField(\"days_since_release\", IntegerType(), True),\n",
    "    StructField(\"alert_date\", TimestampType(), False),\n",
    "    StructField(\"acknowledged\", BooleanType(), False),\n",
    "    StructField(\"acknowledged_date\", TimestampType(), True),\n",
    "    StructField(\"acknowledged_by\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create empty DataFrame\n",
    "df_alerts = spark.createDataFrame([], schema_alerts)\n",
    "\n",
    "# Write to Delta\n",
    "table_path = \"Tables/feature_alerts\"\n",
    "df_alerts.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "\n",
    "print(\"‚úÖ Table created: feature_alerts\")\n",
    "print(f\"   Location: {table_path}\")\n",
    "print(\"\\n   Schema:\")\n",
    "for field in schema_alerts.fields:\n",
    "    print(f\"     - {field.name}: {field.dataType.simpleString()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9af32",
   "metadata": {},
   "source": [
    "## Step 4: Create Helper Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Creating helper views for SQL Endpoint...\")\n",
    "\n",
    "# View 1: Active Preview Features (for quick querying)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW vw_active_preview_features AS\n",
    "    SELECT \n",
    "        feature_name,\n",
    "        workload,\n",
    "        setting_name,\n",
    "        days_since_release,\n",
    "        similarity_score,\n",
    "        release_date,\n",
    "        detected_date\n",
    "    FROM preview_features_active\n",
    "    WHERE is_enabled = true\n",
    "    ORDER BY detected_date DESC\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created view: vw_active_preview_features\")\n",
    "\n",
    "# View 2: Unacknowledged Critical Alerts\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW vw_critical_alerts AS\n",
    "    SELECT \n",
    "        alert_id,\n",
    "        feature_name,\n",
    "        workload,\n",
    "        alert_type,\n",
    "        severity,\n",
    "        message,\n",
    "        alert_date\n",
    "    FROM feature_alerts\n",
    "    WHERE acknowledged = false \n",
    "      AND severity IN ('Critical', 'Warning')\n",
    "    ORDER BY alert_date DESC\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created view: vw_critical_alerts\")\n",
    "\n",
    "# View 3: Feature Release Timeline\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW vw_feature_timeline AS\n",
    "    SELECT \n",
    "        feature_name,\n",
    "        workload,\n",
    "        status,\n",
    "        is_preview,\n",
    "        release_date,\n",
    "        DATEDIFF(CURRENT_DATE(), release_date) as days_since_release\n",
    "    FROM feature_releases\n",
    "    ORDER BY release_date DESC\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created view: vw_feature_timeline\")\n",
    "\n",
    "# View 4: Roadmap Upcoming Features (NEW)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW vw_roadmap_upcoming AS\n",
    "    SELECT \n",
    "        feature_name,\n",
    "        feature_description,\n",
    "        product_name,\n",
    "        workload,\n",
    "        release_type,\n",
    "        release_status,\n",
    "        release_date,\n",
    "        is_preview,\n",
    "        is_planned,\n",
    "        last_modified,\n",
    "        CASE \n",
    "            WHEN release_date IS NULL THEN NULL\n",
    "            ELSE DATEDIFF(release_date, CURRENT_DATE())\n",
    "        END as days_until_release\n",
    "    FROM feature_releases_roadmap\n",
    "    WHERE is_planned = true\n",
    "      AND (release_date IS NULL OR release_date >= CURRENT_DATE())\n",
    "    ORDER BY release_date ASC NULLS LAST, last_modified DESC\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created view: vw_roadmap_upcoming\")\n",
    "\n",
    "print(\"\\n‚úÖ All views created successfully\")\n",
    "print(\"   ‚Üí These views are accessible via SQL Endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b37e4e",
   "metadata": {},
   "source": [
    "## ‚úÖ Setup Complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0659717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ FEATURE TRACKING SETUP COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify all tables exist\n",
    "tables = [\n",
    "    \"feature_releases\",\n",
    "    \"feature_releases_roadmap\", \n",
    "    \"preview_features_active\", \n",
    "    \"feature_alerts\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìã Verifying tables...\")\n",
    "for table in tables:\n",
    "    try:\n",
    "        count = spark.read.format(\"delta\").load(f\"Tables/{table}\").count()\n",
    "        print(f\"  ‚úÖ {table}: {count} rows\")\n",
    "    except:\n",
    "        print(f\"  ‚ùå {table}: ERROR\")\n",
    "\n",
    "# Verify views\n",
    "views = [\n",
    "    \"vw_active_preview_features\",\n",
    "    \"vw_critical_alerts\",\n",
    "    \"vw_feature_timeline\",\n",
    "    \"vw_roadmap_upcoming\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìã Verifying views...\")\n",
    "for view in views:\n",
    "    try:\n",
    "        spark.sql(f\"SELECT * FROM {view} LIMIT 1\")\n",
    "        print(f\"  ‚úÖ {view}\")\n",
    "    except:\n",
    "        print(f\"  ‚ùå {view}: ERROR\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìö Next Steps:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Choose your data source:\")\n",
    "print(\"   a) Microsoft Learn (original):\")\n",
    "print(\"      ‚Üí Run '01_Transfer_Feature_Releases_Unit'\")\n",
    "print(\"   b) Fabric GPS API (enhanced with roadmap):\")\n",
    "    \"print(\\\"      ‚Üí Run '01_Transfer_Feature_Releases_GpsApi_Unit'\\\")\\n\",\n",
    "print(\"\\n2. Run '02_Transfer_Preview_Features_Unit' to detect activated previews\")\n",
    "print(\"   Note: This requires 'tenant_settings' table from FUAM core\")\n",
    "print(\"\\n3. Run '03_Transfer_Feature_Alerts_Unit' to generate alerts\")\n",
    "print(\"\\n4. OR run the full pipeline: 'Load_Feature_Tracking_E2E'\")\n",
    "print(\"\\nüí° Recommended:\")\n",
    "print(\"   - Use Enhanced version (Fabric GPS) for complete roadmap visibility\")\n",
    "print(\"   - Run both versions if you want dual data sources\")\n",
    "print(\"   - Schedule pipeline to run daily\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
