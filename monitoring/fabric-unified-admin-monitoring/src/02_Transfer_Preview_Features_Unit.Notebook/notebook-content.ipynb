{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a7f70e9",
   "metadata": {},
   "source": [
    "# 02 - Transfer Preview Features Unit\n",
    "\n",
    "**Purpose**: Detect activated preview features by comparing tenant settings with feature releases\n",
    "\n",
    "**Inputs**:\n",
    "- Delta Table: `tenant_settings` (from FUAM)\n",
    "- Delta Table: `feature_releases` (from 01_Transfer_Feature_Releases_Unit)\n",
    "\n",
    "**Outputs**:\n",
    "- Delta Table: `preview_features_active`\n",
    "\n",
    "**Frequency**: Daily (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5c031",
   "metadata": {},
   "source": [
    "## Step 1: Load Feature Releases (Preview only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Loading preview feature releases...\")\n",
    "\n",
    "df_features = spark.read.format(\"delta\").load(\"Tables/feature_releases\")\n",
    "\n",
    "# Filter to preview features only\n",
    "df_preview_features = df_features.filter(F.col(\"is_preview\") == True)\n",
    "\n",
    "preview_count = df_preview_features.count()\n",
    "print(f\"âœ… Loaded {preview_count} preview features\")\n",
    "\n",
    "# Show sample\n",
    "df_preview_features.select(\"feature_id\", \"feature_name\", \"workload\", \"release_date\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5353e2d",
   "metadata": {},
   "source": [
    "## Step 2: Load Tenant Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Loading tenant settings...\")\n",
    "\n",
    "df_tenant_settings = spark.read.format(\"delta\").load(\"Tables/tenant_settings\")\n",
    "\n",
    "settings_count = df_tenant_settings.count()\n",
    "print(f\"âœ… Loaded {settings_count} tenant settings\")\n",
    "\n",
    "# Show sample\n",
    "df_tenant_settings.select(\"settingName\", \"enabled\", \"delegateToTenant\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7422f6",
   "metadata": {},
   "source": [
    "## Step 3: Map Settings to Features\n",
    "\n",
    "This creates a mapping between tenant setting names and preview feature names using fuzzy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd21a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(a, b):\n",
    "    \"\"\"Calculate similarity between two strings (0-1)\"\"\"\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "def map_settings_to_features(df_settings, df_features):\n",
    "    \"\"\"\n",
    "    Map tenant settings to preview features based on name similarity\n",
    "    Returns DataFrame with matches\n",
    "    \"\"\"\n",
    "    \n",
    "    # Collect feature names for matching\n",
    "    features_list = df_features.select(\"feature_id\", \"feature_name\", \"workload\").collect()\n",
    "    settings_list = df_settings.select(\"settingName\", \"enabled\", \"delegateToTenant\").collect()\n",
    "    \n",
    "    matches = []\n",
    "    \n",
    "    for setting in settings_list:\n",
    "        setting_name = setting[\"settingName\"]\n",
    "        \n",
    "        # Skip if setting is not enabled\n",
    "        if not setting[\"enabled\"]:\n",
    "            continue\n",
    "        \n",
    "        # Find best matching feature\n",
    "        best_match = None\n",
    "        best_score = 0.0\n",
    "        \n",
    "        for feature in features_list:\n",
    "            feature_name = feature[\"feature_name\"]\n",
    "            \n",
    "            # Calculate similarity\n",
    "            score = similarity_score(setting_name, feature_name)\n",
    "            \n",
    "            # Also check for keyword matches\n",
    "            setting_words = set(setting_name.lower().split())\n",
    "            feature_words = set(feature_name.lower().split())\n",
    "            common_words = setting_words & feature_words\n",
    "            \n",
    "            if common_words:\n",
    "                score += len(common_words) * 0.1  # Boost score for common words\n",
    "            \n",
    "            if score > best_score and score > 0.3:  # Threshold 0.3\n",
    "                best_score = score\n",
    "                best_match = feature\n",
    "        \n",
    "        if best_match:\n",
    "            matches.append({\n",
    "                \"setting_name\": setting_name,\n",
    "                \"feature_id\": best_match[\"feature_id\"],\n",
    "                \"feature_name\": best_match[\"feature_name\"],\n",
    "                \"workload\": best_match[\"workload\"],\n",
    "                \"similarity_score\": best_score,\n",
    "                \"is_enabled\": setting[\"enabled\"],\n",
    "                \"delegate_to_tenant\": setting[\"delegateToTenant\"],\n",
    "                \"detected_date\": datetime.now()\n",
    "            })\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Mapping settings to preview features...\")\n",
    "\n",
    "matches = map_settings_to_features(df_tenant_settings, df_preview_features)\n",
    "\n",
    "print(f\"âœ… Found {len(matches)} potential preview features activated\")\n",
    "\n",
    "# Preview matches\n",
    "if matches:\n",
    "    print(\"\\nðŸ“‹ Sample matches:\")\n",
    "    for match in matches[:5]:\n",
    "        print(f\"  - {match['feature_name']}\")\n",
    "        print(f\"    â†’ Setting: {match['setting_name']}\")\n",
    "        print(f\"    â†’ Similarity: {match['similarity_score']:.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1740ab",
   "metadata": {},
   "source": [
    "## Step 4: Create DataFrame with Activated Previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"setting_name\", StringType(), False),\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"similarity_score\", DoubleType(), False),\n",
    "    StructField(\"is_enabled\", BooleanType(), False),\n",
    "    StructField(\"delegate_to_tenant\", BooleanType(), True),\n",
    "    StructField(\"detected_date\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "df_active_previews = spark.createDataFrame(matches, schema=schema)\n",
    "\n",
    "print(f\"âœ… Created DataFrame with {df_active_previews.count()} activated preview features\")\n",
    "df_active_previews.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e776aac",
   "metadata": {},
   "source": [
    "## Step 5: Enrich with Feature Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Enriching with feature release details...\")\n",
    "\n",
    "# Join with feature_releases to get release_date, status, etc.\n",
    "df_enriched = df_active_previews.join(\n",
    "    df_preview_features.select(\n",
    "        \"feature_id\",\n",
    "        \"release_date\",\n",
    "        \"status\",\n",
    "        \"source_url\"\n",
    "    ),\n",
    "    on=\"feature_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Calculate days since release\n",
    "df_enriched = df_enriched.withColumn(\n",
    "    \"days_since_release\",\n",
    "    F.datediff(F.current_date(), F.col(\"release_date\"))\n",
    ")\n",
    "\n",
    "print(\"âœ… Enrichment completed\")\n",
    "df_enriched.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28244a88",
   "metadata": {},
   "source": [
    "## Step 6: Write to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b079a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"preview_features_active\"\n",
    "table_path = f\"Tables/{table_name}\"\n",
    "\n",
    "print(f\"ðŸ”„ Writing to Delta table: {table_name}\")\n",
    "\n",
    "try:\n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    # Check if table exists\n",
    "    if DeltaTable.isDeltaTable(spark, table_path):\n",
    "        print(\"  â†’ Table exists, performing MERGE (upsert)...\")\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(spark, table_path)\n",
    "        \n",
    "        # Merge: Update existing, insert new\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df_enriched.alias(\"source\"),\n",
    "            \"target.feature_id = source.feature_id AND target.setting_name = source.setting_name\"\n",
    "        ).whenMatchedUpdate(\n",
    "            set={\n",
    "                \"is_enabled\": \"source.is_enabled\",\n",
    "                \"detected_date\": \"source.detected_date\",\n",
    "                \"days_since_release\": \"source.days_since_release\"\n",
    "            }\n",
    "        ).whenNotMatchedInsertAll(\n",
    "        ).execute()\n",
    "        \n",
    "        print(\"  âœ… MERGE completed\")\n",
    "    else:\n",
    "        print(\"  â†’ Table doesn't exist, creating new table...\")\n",
    "        df_enriched.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "        print(\"  âœ… Table created\")\n",
    "    \n",
    "    # Show final count\n",
    "    final_count = spark.read.format(\"delta\").load(table_path).count()\n",
    "    print(f\"\\nâœ… Total activated preview features in {table_name}: {final_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error writing to Delta: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e5bdd",
   "metadata": {},
   "source": [
    "## Step 7: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d84f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š Activated Preview Features Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_summary = spark.read.format(\"delta\").load(table_path)\n",
    "\n",
    "# By workload\n",
    "print(\"\\nðŸ”¸ By Workload:\")\n",
    "df_summary.groupBy(\"workload\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# By similarity score range\n",
    "print(\"\\nðŸ”¸ By Confidence Level (Similarity Score):\")\n",
    "df_summary.withColumn(\n",
    "    \"confidence\",\n",
    "    F.when(F.col(\"similarity_score\") >= 0.7, \"High (â‰¥0.7)\")\n",
    "     .when(F.col(\"similarity_score\") >= 0.5, \"Medium (0.5-0.7)\")\n",
    "     .otherwise(\"Low (<0.5)\")\n",
    ").groupBy(\"confidence\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# Recently released (last 30 days) but already activated\n",
    "recent_activated = df_summary.filter(F.col(\"days_since_release\") <= 30).count()\n",
    "print(f\"\\nðŸ”¸ Features activated within 30 days of release: {recent_activated}\")\n",
    "\n",
    "# Features that have been in preview for long time\n",
    "long_preview = df_summary.filter(F.col(\"days_since_release\") > 180).count()\n",
    "print(f\"ðŸ”¸ Features in preview for >180 days: {long_preview}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Transfer Preview Features Unit - COMPLETED\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
