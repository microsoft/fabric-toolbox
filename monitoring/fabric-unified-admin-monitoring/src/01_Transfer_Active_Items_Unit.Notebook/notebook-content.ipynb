{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Active items\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: #D69AFE;\">**MERGE**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_Items_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Files** from FUAM_Lakehouse folder **bronze_file_location** variable\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**1 Delta table** in FUAM_Lakehouse \n",
        "- **gold_table_name** variable value"
      ],
      "metadata": {},
      "id": "cceec85a-5432-4959-a940-51d7b49c0df9"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode, to_date, date_format, lit\n",
        "import pyspark.sql.functions as f\n",
        "from delta.tables import *\n",
        "import datetime\n",
        "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\") # needed for automatic schema evolution in merge "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "8e0dfb76-d449-4c62-b4ee-a55850f6f1b6"
    },
    {
      "cell_type": "code",
      "source": [
        "## Parameters\n",
        "display_data = False"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "b5eb77a7-0eaa-4e0a-81c7-37d488a4b31c"
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables\n",
        "bronze_file_location = f\"Files/raw/active_items/\"\n",
        "silver_table_name = \"FUAM_Staging_Lakehouse.active_items_silver\"\n",
        "gold_table_name = \"active_items\"\n",
        "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "519f57fd-0fdb-4646-8fa9-aac66087fe04"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Silver table, if exists\n",
        "if spark.catalog.tableExists(silver_table_name):\n",
        "    del_query = \"DELETE FROM \" + silver_table_name\n",
        "    spark.sql(del_query)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "d96bad65-f3c4-46fe-a54a-67ac04a67c80"
    },
    {
      "cell_type": "code",
      "source": [
        "# This function converts all complex data types to StringType\n",
        "def convert_columns_to_string(schema, parent = \"\", lvl = 0):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    - schema: Dataframe schema as StructType\n",
        "    \n",
        "    Output: List\n",
        "    Returns a list of columns in the schema casting them to String to use in a selectExpr Spark function.\n",
        "    \"\"\"\n",
        "    \n",
        "    lst=[]\n",
        "    \n",
        "    for x in schema:\n",
        "        # check if complex datatype has to be converted to string\n",
        "        if str(x.dataType) in {\"DateType()\", \"StringType()\", \"BooleanType()\", \"LongType()\", \"IntegerType()\", \"DoubleType()\", \"FloatType()\"}:\n",
        "            # no need to convert\n",
        "            lst.append(\"{col}\".format(col=x.name))\n",
        "        else:\n",
        "            # it has to be converted\n",
        "            # print(str(x.dataType))\n",
        "            lst.append(\"cast({col} as string) as {col}\".format(col=x.name))\n",
        "\n",
        "    return lst"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "ee9ec13d-3175-4e00-a736-d5f889752e92"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Bronze data\n",
        "bronze_df = spark.read.option(\"multiline\", \"true\").json(bronze_file_location)\n",
        "\n",
        "if display_data:\n",
        "    display(bronze_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "14cc398a-d46f-4fa7-88c8-56dc243cf9b0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode json subset structure\n",
        "exploded_df = bronze_df.select(explode(\"itemEntities\").alias(\"d\"))\n",
        "\n",
        "# Select all columns (columns are dynamic)\n",
        "silver_df = exploded_df.select(\n",
        "    col(\"d.*\")\n",
        "    )\n",
        "\n",
        "# Convert key(s) to upper case\n",
        "silver_df = silver_df.withColumn(\"id\", f.upper(f.col(\"id\")))\n",
        "silver_df = silver_df.withColumn(\"capacityId\", f.upper(f.col(\"capacityId\")))\n",
        "silver_df = silver_df.withColumn(\"workspaceId\", f.upper(f.col(\"workspaceId\")))\n",
        "\n",
        "# Added column creatorUserPrincipalName to enable easier identification of creator\n",
        "silver_df = silver_df.withColumn(\"creatorUserPrincipalName\", f.col(\"creatorPrincipal.userDetails.userPrincipalName\"))\n",
        "\n",
        "# Drop duplicates (API returns sometimes duplicated values)\n",
        "silver_df = silver_df.dropDuplicates()\n",
        "\n",
        "if display_data:\n",
        "    display(silver_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "id": "f8d02ab2-19a5-4877-9ffe-454cdc95c802"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out id '00000000-0000-0000-0000-000000000000', which causes duplicates at some customers\n",
        "silver_df = silver_df.filter(silver_df.id != '00000000-0000-0000-0000-000000000000')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "af14b871-df15-4d36-a265-bfb2c855295f"
    },
    {
      "cell_type": "code",
      "source": [
        "# show converted table schema\n",
        "if display_data:\n",
        "    convert_columns_to_string(silver_df.schema)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "1fd36928-cd34-4ff7-a631-9caaaa12f0f1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert silver_df's complex data type columns to StringType columns\n",
        "silver_df_converted = silver_df.selectExpr(convert_columns_to_string(silver_df.schema))         "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "f47e952e-8b76-4e7a-86fb-9edacb0d6fe9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Write prepared bronze_df to silver delta table\n",
        "silver_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(silver_table_name)\n",
        "\n",
        "if display_data:\n",
        "    display(silver_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "id": "080d2187-e9be-4675-931f-04ee2f29bcd5"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This function maps and merges the silver data to gold dynamically\n",
        "def write_silver_to_gold(silver_table_name, gold_table_name, ids):\n",
        "    query = \"SELECT *, current_timestamp() AS fuam_modified_at, False as fuam_deleted  FROM \" + silver_table_name \n",
        "    silver_df = spark.sql(query)\n",
        "    \n",
        "    if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):\n",
        "        # if exists -> MERGE to gold\n",
        "        print(\"Gold table exists and will be merged.\")\n",
        "        gold_df = DeltaTable.forName(spark, gold_table_name)\n",
        "\n",
        "\n",
        "        gold_columns = gold_df.toDF().columns\n",
        "        silver_columns = silver_df.columns\n",
        "        combined_columns = list(set(gold_columns) | set(silver_columns))\n",
        "        id_cols = {}\n",
        "        merge_id_stmt = ''\n",
        "        for col in combined_columns:\n",
        "            if col in ids:\n",
        "                merge_id_stmt =  merge_id_stmt +  \" t.\" + col + \" = s.\" + col + \" and\"\n",
        "                id_cols[col] = \"s.\" + col\n",
        "\n",
        "                \n",
        "        # delete last and in merge id statement\n",
        "        merge_id_stmt = merge_id_stmt[:-4]\n",
        "\n",
        "\n",
        "        # Merge silver (s = source) to gold (t = target)\n",
        "        try:\n",
        "            merge = (gold_df.alias('t') \\\n",
        "            .merge(silver_df.alias('s'), merge_id_stmt )) \\\n",
        "            .whenMatchedUpdateAll() \\\n",
        "            .whenNotMatchedInsertAll() \\\n",
        "            .whenNotMatchedBySourceUpdate( condition = \"t.fuam_deleted == False or t.fuam_deleted IS NULL\", set = {\"fuam_deleted\" : \"True\", \"fuam_modified_at\": \"current_timestamp()\"} )\n",
        "            \n",
        "            merge.execute()\n",
        "        except:\n",
        "        # In case the tables already exist, but the fuam column are not existent because of an old version do merge whenNotMatchedBySourceUpdate\n",
        "            merge = (gold_df.alias('t') \\\n",
        "            .merge(silver_df.alias('s'), merge_id_stmt )) \\\n",
        "            .whenMatchedUpdateAll() \\\n",
        "            .whenNotMatchedInsertAll() \\\n",
        "                        \n",
        "            merge.execute()\n",
        "\n",
        "    else:\n",
        "        # else -> INSERT to gold\n",
        "        print(\"Gold table will be created.\")\n",
        "\n",
        "        silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "9aad5193-a856-49cc-a86f-9a03b71661a6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge data to gold table\n",
        "write_silver_to_gold(silver_table_name, gold_table_name, ['id'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "e3b04945-1480-4a12-be03-29a69237a7a9"
    },
    {
      "cell_type": "code",
      "source": [
        "# write history of bronze files\n",
        "\n",
        "mssparkutils.fs.cp(bronze_file_location, bronze_file_location.replace(\"Files/raw/\", \"Files/history/\") + datetime.datetime.now().strftime('%Y/%m/%d') + \"/\", True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "7edc7384-0392-489b-853a-d1b75a74794d"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "widgets": {},
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d",
        "known_lakehouses": []
      },
      "environment": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}