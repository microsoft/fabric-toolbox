{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark",
      "jupyter_kernel_name": null
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d",
        "known_lakehouses": [
          {
            "id": "6cff634b-88f7-3505-bed2-c03a36776a8b"
          }
        ]
      },
      "environment": {}
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    }
  },
  "cells": [
    {
      "id": "2e5e455b-990e-4d6c-873a-09dd1b5e85e2",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Capacity Metrics (by Kind)\n",
        "by Workspace by Kind by Day\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: #D69AFE;\">**MERGE**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_Capacity_Metrics_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Capacity Metrics** via SemPy DAX execute query function\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**1 Delta table** in FUAM_Lakehouse \n",
        "- **gold_table_name** variable value"
      ]
    },
    {
      "id": "33d4923b-706b-4618-912c-c8e4c982217f",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "import sempy.fabric as fabric\n",
        "from datetime import datetime, timedelta\n",
        "import datetime as dt\n",
        "import pyspark.sql.functions as f\n",
        "from delta.tables import *"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "35635a3a-2fa5-4e51-be87-e11a9444c402",
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "## Parameters\n",
        "# These parameters will be overwritten while executing the notebook \n",
        "# from Load_FUAM_Data_E2E Pipeline\n",
        "metric_days_in_scope = 3\n",
        "metric_workspace = \"d0182223-a6a9-4aa1-bf5c-24d8ee582b9d\"\n",
        "metric_dataset = \"fe58232b-5b8a-4c8a-9c94-83db50937bd7\"\n",
        "display_data = True"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "812eac31-61be-4d22-82dc-3c538baf2279",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "## Variables\n",
        "silver_table_name = \"FUAM_Staging_Lakehouse.capacity_metrics_by_item_kind_by_day_silver\"\n",
        "gold_table_name = \"capacity_metrics_by_item_kind_by_day\"\n",
        "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\"\n",
        "count_of_connectivity_errors = 0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "475ac778-5e14-4300-9db8-8b433d96ffb2",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Check Table Status\n",
        "version = ''\n",
        "\n",
        "try: \n",
        "    check_table_structure_query = \"\"\"DEFINE    MPARAMETER 'DefaultCapacityID' = \"0000000-0000-0000-0000-00000000\"\n",
        "                                    EVALUATE   SUMMARIZECOLUMNS(\"Background billable CU %\", [Background billable CU %]    )\"\"\"\n",
        "    check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)\n",
        "    print(\"INFO: Test for v47 successful\")\n",
        "    version = 'v47'\n",
        "except:\n",
        "    print(\"INFO: Test for v47 failed\")\n",
        "\n",
        "if version == '':\n",
        "    try:\n",
        "        check_table_structure_query = \"\"\"EVALUATE ROW(\"Background billable CU %\", 'All Measures'[Background billable CU %])\"\"\"\n",
        "        check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)\n",
        "        print(\"INFO: Test for v40 successful\")\n",
        "        version = 'v40'\n",
        "    except:\n",
        "        print(\"INFO: Test for v40 failed\")\n",
        "\n",
        "if version == '':\n",
        "    try:\n",
        "        check_table_structure_query_alternative = \"\"\"EVALUATE ROW(\"xBackground__\", 'All Measures'[xBackground %])\"\"\"\n",
        "        check_table_structure_df_alternative = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query_alternative)\n",
        "        version = 'v37'\n",
        "        print(\"INFO: Test for v37 successful\")\n",
        "    except:\n",
        "        print(\"INFO: Test for v37 failed\")\n",
        "\n",
        "\n",
        "# Validate version compatibility\n",
        "if version != '':\n",
        "    print( f'INFO: Version {version} is valid')\n",
        "else: \n",
        "    raise Exception(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9eb856a2-8257-4bfd-98b5-a39d7b92ad82",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Fetch capacities from connected capacity metrics app\n",
        "try:\n",
        "  if version in ['v47', 'v44', 'v40']:\n",
        "    capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities, \"capacity Id\", Capacities[capacity Id] , \"state\" , Capacities[state] )\"\"\"\n",
        "  else:\n",
        "    capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities, \"capacity Id\", Capacities[CapacityId] , \"state\" , Capacities[state] )\"\"\"\n",
        "  capacities = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=capacity_query)\n",
        "  capacities.columns = ['CapacityId', 'State']\n",
        "  capacities = spark.createDataFrame(capacities)\n",
        "except Exception as e:\n",
        "  notebookutils.notebook.exit(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "46c219ef-e6e0-4348-b660-7eebb91c5929",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "source": [
        "if display_data:\n",
        "    display(capacities)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "faef17f9-b6e2-45b7-bd1e-0c9b6008665b",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Iterate days\n",
        "def iterate_dates(start_date, end_date):\n",
        "    # Init array\n",
        "    dates = []\n",
        "    # Convert string inputs to datetime objects\n",
        "    start = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    \n",
        "    # Initialize current date as start date\n",
        "    current_date = start.date()\n",
        "    \n",
        "    while current_date <= end.date():\n",
        "\n",
        "        dates.append(\n",
        "            {\n",
        "                \"date\": current_date,\n",
        "                \"year\": current_date.year,\n",
        "                \"month\": current_date.month,\n",
        "                \"day\": current_date.day\n",
        "            })\n",
        "        # Move to the next day\n",
        "        current_date += dt.timedelta(days=1)\n",
        "\n",
        "    return dates"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d1ecced1-882a-4d74-9697-b17c5735273b",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Clean Silver table\n",
        "try:\n",
        "    query = \"DELETE FROM \" + silver_table_name\n",
        "    spark.sql(query)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(\"Silver table doesn't exist yet.\") "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "90d5f440-4785-4801-931b-6d798b2ec02b",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "source": [
        "# Iterate capacities and days\n",
        "for cap in capacities.collect():\n",
        "    capacity_id = cap[0]\n",
        "    \n",
        "    print(f\"INFO: Scoped CapacityId: {capacity_id}\")\n",
        "\n",
        "    try:\n",
        "        # Get today's date\n",
        "        today = datetime.now()\n",
        "\n",
        "        # Calculate the dates between today and days_in_scope\n",
        "        days_ago = today - timedelta(days=metric_days_in_scope)\n",
        "\n",
        "        # Format dates in 'yyyy-mm-dd'\n",
        "        today_str = today.strftime('%Y-%m-%d')\n",
        "        days_ago_str = days_ago.strftime('%Y-%m-%d')\n",
        "\n",
        "        date_array = iterate_dates(days_ago_str, end_date=today_str)\n",
        "\n",
        "        # Iterate days for current capacity\n",
        "        for date in date_array:\n",
        "\n",
        "            year = date['year']\n",
        "            month = date['month']\n",
        "            day = date['day']\n",
        "            date_label = str(year) + '-' + str(month) + '-' + str(day)\n",
        "            print(f\"INFO: Get data for CapacityId: {capacity_id}\")\n",
        "\n",
        "            dax_query_v47 = f\"\"\" DEFINE \n",
        "                    MPARAMETER 'CapacitiesList' = {{\\\"{capacity_id}\\\"}}\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('Metrics By Item Operation And Day'[Date])),\n",
        "                                            'Metrics By Item Operation And Day'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                    SUMMARIZECOLUMNS(\n",
        "                                            'Metrics By Item Operation And Day'[Capacity Id],\n",
        "                                            'Metrics By Item Operation And Day'[Workspace Id],\n",
        "                                            'Metrics By Item Operation And Day'[Date],\n",
        "                                            'Items'[Item Kind],\n",
        "                                            FILTER(Capacities, Capacities[capacity Id] =  \\\"{capacity_id}\\\" ),\n",
        "                                            __DS0FilterTable,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"S_Dur\", SUM('Metrics By Item Operation And Day'[Duration (s)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"S_CU\", SUM('Metrics By Item Operation And Day'[CU (s)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"TH_M\", SUM('Metrics By Item Operation And Day'[Throttling (min)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_U\", SUM('Metrics By Item Operation And Day'[Users]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_SO\", SUM('Metrics By Item Operation And Day'[Successful operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_RO\", SUM('Metrics By Item Operation And Day'[Rejected operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_O\", SUM('Metrics By Item Operation And Day'[Operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_IO\", SUM('Metrics By Item Operation And Day'[Invalid operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_FO\", SUM('Metrics By Item Operation And Day'[Failed operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_CO\", SUM('Metrics By Item Operation And Day'[Cancelled operations])\n",
        "                                            )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC  \"\"\"\n",
        "\n",
        "            dax_query_v44 = f\"\"\" DEFINE \n",
        "                    MPARAMETER 'CapacitiesList' = {{\\\"{capacity_id}\\\"}}\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('Metrics By Item Operation And Day'[Date])),\n",
        "                                            'Metrics By Item Operation And Day'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                    SUMMARIZECOLUMNS(\n",
        "                                            'Metrics By Item Operation And Day'[Capacity Id],\n",
        "                                            'Metrics By Item Operation And Day'[Workspace Id],\n",
        "                                            'Metrics By Item Operation And Day'[Date],\n",
        "                                            'Items'[Item Kind],\n",
        "                                            FILTER(Capacities, Capacities[capacity Id] =  \\\"{capacity_id}\\\" ),\n",
        "                                            __DS0FilterTable,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"S_Dur\", SUM('Metrics By Item Operation And Day'[Duration (s)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"S_CU\", SUM('Metrics By Item Operation And Day'[CU (s)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"TH_M\", SUM('Metrics By Item Operation And Day'[Throttling (min)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_U\", SUM('Metrics By Item Operation And Day'[Users]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_SO\", SUM('Metrics By Item Operation And Day'[Successful operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_RO\", SUM('Metrics By Item Operation And Day'[Rejected operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_O\", SUM('Metrics By Item Operation And Day'[Operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_IO\", SUM('Metrics By Item Operation And Day'[Invalid operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_FO\", SUM('Metrics By Item Operation And Day'[Failed operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_CO\", SUM('Metrics By Item Operation And Day'[Cancelled operations])\n",
        "                                            )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC  \"\"\"\n",
        "\n",
        "            dax_query_v40 = f\"\"\"\n",
        "                 DEFINE \n",
        "                    MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('Metrics By Item Operation And Day'[Date])),\n",
        "                                            'Metrics By Item Operation And Day'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                    SUMMARIZECOLUMNS(\n",
        "                                            'Metrics By Item Operation And Day'[Capacity Id],\n",
        "                                            'Metrics By Item Operation And Day'[Workspace Id],\n",
        "                                            'Metrics By Item Operation And Day'[Date],\n",
        "                                            'Items'[Item Kind],\n",
        "                                            FILTER(Capacities, Capacities[Capacity Id] = \\\"{capacity_id}\\\"  ),\n",
        "                                            __DS0FilterTable,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"S_Dur\", SUM('Metrics By Item Operation And Day'[Duration (s)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"S_CU\", SUM('Metrics By Item Operation And Day'[CU (s)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"TH_M\", SUM('Metrics By Item Operation And Day'[Throttling (min)]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_U\", SUM('Metrics By Item Operation And Day'[Users]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_SO\", SUM('Metrics By Item Operation And Day'[Successful operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_RO\", SUM('Metrics By Item Operation And Day'[Rejected operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_O\", SUM('Metrics By Item Operation And Day'[Operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_IO\", SUM('Metrics By Item Operation And Day'[Invalid operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_FO\", SUM('Metrics By Item Operation And Day'[Failed operations]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"C_CO\", SUM('Metrics By Item Operation And Day'[Cancelled operations])\n",
        "                                            )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            dax_query_v37 = f\"\"\"\n",
        "                DEFINE \n",
        "                    MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('MetricsByItemandDay'[Date])),\n",
        "                                            'MetricsByItemandDay'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                    SUMMARIZECOLUMNS(\n",
        "                                            Capacities[capacityId],\n",
        "                                            Items[WorkspaceId],\n",
        "                                            'MetricsByItemandDay'[Date],\n",
        "                                            'Items'[ItemKind],\n",
        "                                            FILTER(Capacities, Capacities[capacityId] = \\\"{capacity_id}\\\" ),\n",
        "                                            __DS0FilterTable,\n",
        "                                            \"S_Dur\", SUM('MetricsByItemandDay'[sum_duration]),\n",
        "                                            \"S_CU\", SUM('MetricsByItemandDay'[sum_CU]),\n",
        "                                            \"TH_M\", SUM('MetricsByItemandDay'[Throttling (min)]),\n",
        "                                            \"C_U\", SUM('MetricsByItemandDay'[count_users]),\n",
        "                                            \"C_SO\", SUM('MetricsByItemandDay'[count_successful_operations]),\n",
        "                                            \"C_RO\", SUM('MetricsByItemandDay'[count_rejected_operations]),\n",
        "                                            \"C_O\", SUM('MetricsByItemandDay'[count_operations]),\n",
        "                                            \"C_IO\", SUM('MetricsByItemandDay'[count_Invalid_operations]),\n",
        "                                            \"C_FO\", SUM('MetricsByItemandDay'[count_failure_operations]),\n",
        "                                            \"C_CO\", SUM('MetricsByItemandDay'[count_cancelled_operations])\n",
        "                                            )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC\n",
        "                    \"\"\"\n",
        "\n",
        "            dax_query = \"\"\n",
        "            # Choose query\n",
        "            if version == 'v47':\n",
        "                dax_query = dax_query_v47\n",
        "            elif version == 'v44':\n",
        "                dax_query = dax_query_v44\n",
        "            elif version == 'v40':\n",
        "                dax_query = dax_query_v40\n",
        "            elif version == 'v37':\n",
        "                dax_query = dax_query_v37\n",
        "\n",
        "           \n",
        "            \n",
        "            # Execute DAX query\n",
        "            capacity_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=dax_query)\n",
        "            capacity_df.columns = ['CapacityId', 'WorkspaceId', 'Date',  \n",
        "                                    'ItemKind', 'DurationInSec','TotalCUs', 'ThrottlingInMin', \n",
        "                                    'UserCount','SuccessOperationCount', 'RejectedOperationCount','OperationCount',\n",
        "                                    'InvalidOperationCount','FailureOperationCount','CancelledOperationCount', 'DateKey']\n",
        "            \n",
        "            if not(capacity_df.empty):\n",
        "                # Transfer pandas df to spark df\n",
        "                capacity_df = spark.createDataFrame(capacity_df)\n",
        "                \n",
        "                if display_data:\n",
        "                    display(capacity_df)\n",
        "\n",
        "                # Write prepared bronze_df to silver delta table\n",
        "                print(f\"INFO: Appending data. Capacity: {capacity_id} on Date: {date_label}\")\n",
        "                capacity_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(silver_table_name)\n",
        "            else:\n",
        "                print(f\"INFO: No data for CapacityId: {capacity_id} on Date: {date_label}\")\n",
        "\n",
        "    except Exception as ex:\n",
        "        print('ERROR: Exception for CapacityId: ' + capacity_id + '. ->' + str(ex))\n",
        "        continue"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "32cefa39-d2c5-4135-b033-083b14981b0d",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Handle error if interface is not reachable\n",
        "if count_of_connectivity_errors > 0: \n",
        "    print(\"WARNING: Connection to at least one capacity is not possible. Please review the configurations.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7d1705fe-8449-4b56-b0b9-fce5acf31cb8",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "try:\n",
        "    # Get Silver table data\n",
        "    query = \"SELECT * FROM  \" + silver_table_name\n",
        "    silver_df = spark.sql(query)\n",
        "\n",
        "except Exception as ex:\n",
        "    # Handle error if interface is not reachable\n",
        "    if count_of_connectivity_errors > 0: \n",
        "        notebookutils.notebook.exit(\"ERROR: Connection to capacity metrics is not possible. Please review the configurations.\")\n",
        "    else:\n",
        "        notebookutils.notebook.exit(\"ERROR: Silver table doesn't exist yet. This issue occurs also when the connection to capacity metrics is not configured correctly or endpoint is not reachable.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "aa41bad9-299c-44b9-a0c9-768a7f92a166",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Check if gold table exists\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):\n",
        "    # if exists -> MERGE to gold\n",
        "    print(\"INFO: Gold table exists and will be merged.\")\n",
        "\n",
        "    gold_df = DeltaTable.forPath(spark, gold_table_name_with_prefix)\n",
        "    # Merge silver (s = source) to gold (t = target)\n",
        "    gold_df.alias('t') \\\n",
        "    .merge(\n",
        "        silver_df.alias('s'),\n",
        "        \"s.CapacityId = t.CapacityId AND s.WorkspaceId = t.WorkspaceId AND s.Date = t.Date AND s.ItemKind = t.ItemKind\"\n",
        "    ) \\\n",
        "    .whenMatchedUpdate(set =\n",
        "        {\n",
        "             \"DurationInSec\": \"s.DurationInSec\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
        "            ,\"UserCount\": \"s.UserCount\"\n",
        "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
        "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
        "            ,\"OperationCount\": \"s.OperationCount\"\n",
        "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
        "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
        "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
        "        }\n",
        "    ) \\\n",
        "    .whenNotMatchedInsert(values =\n",
        "        {\n",
        "             \"CapacityId\": \"s.CapacityId\"\n",
        "            ,\"WorkspaceId\": \"s.WorkspaceId\"\n",
        "            ,\"Date\": \"s.Date\"\n",
        "            ,\"ItemKind\": \"s.ItemKind\"\n",
        "            ,\"DurationInSec\": \"s.DurationInSec\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
        "            ,\"UserCount\": \"s.UserCount\"\n",
        "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
        "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
        "            ,\"OperationCount\": \"s.OperationCount\"\n",
        "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
        "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
        "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
        "            ,\"DateKey\": \"s.DateKey\"\n",
        "        }\n",
        "    ) \\\n",
        "    .execute()\n",
        "\n",
        "else:\n",
        "    # else -> INSERT to gold\n",
        "    print(\"INFO: Gold table will be created.\")\n",
        "\n",
        "    silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8de99e60-3b24-41f0-a84e-72b8c0af289a",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Clean Silver table\n",
        "query = \"DELETE FROM \" + silver_table_name\n",
        "spark.sql(query)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}