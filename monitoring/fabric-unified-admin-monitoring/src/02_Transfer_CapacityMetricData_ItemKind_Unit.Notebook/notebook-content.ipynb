{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "d640e2d5-03e4-4147-98c7-d4591d30283b",
            "metadata": {},
            "source": [
                "#### Capacity Metrics (by Kind)\n",
                "by Workspace by Kind by Day\n",
                "\n",
                "##### Data ingestion strategy:\n",
                "<mark style=\"background: #D69AFE;\">**MERGE**</mark>\n",
                "\n",
                "##### Related pipeline:\n",
                "\n",
                "**Load_Capacity_Metrics_E2E**\n",
                "\n",
                "##### Source:\n",
                "\n",
                "**CapacityMetricCloneDQ** via SemPy DAX execute query function\n",
                "\n",
                "##### Target:\n",
                "\n",
                "**1 Delta table** in FUAM_Lakehouse \n",
                "- **gold_table_name** variable value"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ea04a53c-a7b0-4dc1-a67f-626628d77ad9",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:11:21.7627579Z\",\"execution_finish_time\":\"2025-01-27T09:11:31.8226897Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "import sempy.fabric as fabric\n",
                "from datetime import datetime, timedelta\n",
                "import datetime as dt\n",
                "import pyspark.sql.functions as f\n",
                "from delta.tables import *"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dcc2d0df-5297-441c-9df2-84686682f755",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:11:31.9670888Z\",\"execution_finish_time\":\"2025-01-27T09:11:32.3060151Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "## Parameters\n",
                "metric_days_in_scope = 14\n",
                "metric_workspace = \"Microsoft Fabric Capacity Metrics v30\"\n",
                "metric_dataset = \"Fabric Capacity Metrics\"\n",
                "display_data = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5e0bc19e-6685-4eb4-bc97-1a6b83afc5f4",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:11:32.4480088Z\",\"execution_finish_time\":\"2025-01-27T09:11:32.6908368Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "## Variables\n",
                "silver_table_name = \"FUAM_Lakehouse.capacity_metrics_by_item_kind_by_day\"\n",
                "gold_table_name = \"capacity_metrics_by_item_kind_by_day\"\n",
                "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5f24dea6-fa0a-4692-92c0-32de283c47a1",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:11:32.8287704Z\",\"execution_finish_time\":\"2025-01-27T09:11:33.6478525Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Fetch Capacities (without PPT capacities)\n",
                "query = \"\"\"\n",
                "SELECT \n",
                "    CapacityId\n",
                "FROM FUAM_Lakehouse.capacities\n",
                "WHERE SKU != 'PP3'\n",
                "AND state = 'Active'\n",
                "\"\"\"\n",
                "capacities = spark.sql(query)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4e1334ef-5f7c-4444-8651-fef4aa72f0f3",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:11:33.7754892Z\",\"execution_finish_time\":\"2025-01-27T09:11:34.5966267Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "collapsed": false,
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "if display_data:\n",
                "    display(capacities)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "959509d6-4647-427c-90ae-b4edab474e4d",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:11:34.7263466Z\",\"execution_finish_time\":\"2025-01-27T09:11:34.963209Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Iterate days\n",
                "def iterate_dates(start_date, end_date):\n",
                "    # Init array\n",
                "    dates = []\n",
                "    # Convert string inputs to datetime objects\n",
                "    start = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
                "    end = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
                "    \n",
                "    # Initialize current date as start date\n",
                "    current_date = start.date()\n",
                "    \n",
                "    while current_date <= end.date():\n",
                "\n",
                "        dates.append(\n",
                "            {\n",
                "                \"date\": current_date,\n",
                "                \"year\": current_date.year,\n",
                "                \"month\": current_date.month,\n",
                "                \"day\": current_date.day\n",
                "            })\n",
                "        # Move to the next day\n",
                "        current_date += dt.timedelta(days=1)\n",
                "\n",
                "    return dates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aaae3981-7e98-4481-9441-c4a946412409",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:11:35.1042777Z\",\"execution_finish_time\":\"2025-01-27T09:11:40.0238037Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Clean Silver table\n",
                "try:\n",
                "    query = \"DELETE FROM \" + silver_table_name\n",
                "    spark.sql(query)\n",
                "\n",
                "except Exception as ex:\n",
                "    print(\"Silver table doesn't exist yet.\") "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de5386ef-b266-48fb-9369-b153da0d0c04",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:11:40.1694005Z\",\"execution_finish_time\":\"2025-01-27T09:14:53.5860046Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "collapsed": false,
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Iterate capacities and days\n",
                "for cap in capacities.collect():\n",
                "    capacity_id = cap[0]\n",
                "    print(capacity_id)\n",
                "\n",
                "    try:\n",
                "        # Get today's date\n",
                "        today = datetime.now()\n",
                "\n",
                "        # Calculate the dates between today and days_in_scope\n",
                "        days_ago = today - timedelta(days=metric_days_in_scope)\n",
                "\n",
                "        # Format dates in 'yyyy-mm-dd'\n",
                "        today_str = today.strftime('%Y-%m-%d')\n",
                "        days_ago_str = days_ago.strftime('%Y-%m-%d')\n",
                "\n",
                "        date_array = iterate_dates(days_ago_str, end_date=today_str)\n",
                "\n",
                "        # Iterate days for current capacity\n",
                "        for date in date_array:\n",
                "\n",
                "            year = date['year']\n",
                "            month = date['month']\n",
                "            day = date['day']\n",
                "            date_label = str(year) + '-' + str(month) + '-' + str(day)\n",
                "            print(capacity_id)\n",
                "\n",
                "            dax_query = f\"\"\"\n",
                "                    DEFINE \n",
                "                    MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
                "\n",
                "                    VAR __DS0FilterTable = \n",
                "                                        FILTER(\n",
                "                                            KEEPFILTERS(VALUES('MetricsByItemandDay'[Date])),\n",
                "                                            'MetricsByItemandDay'[Date] = DATE({year}, {month}, {day})\n",
                "                                        )\n",
                "\n",
                "                    VAR __DS0Core = \n",
                "                                    SUMMARIZECOLUMNS(\n",
                "                                            Capacities[capacityId],\n",
                "                                            Items[WorkspaceId],\n",
                "                                            'MetricsByItemandDay'[Date],\n",
                "                                            'Items'[ItemKind],\n",
                "                                            FILTER(Capacities, Capacities[capacityId] = \\\"{capacity_id}\\\" ),\n",
                "                                            __DS0FilterTable,\n",
                "                                            \"S_Dur\", SUM('MetricsByItemandDay'[sum_duration]),\n",
                "                                            \"S_CU\", SUM('MetricsByItemandDay'[sum_CU]),\n",
                "                                            \"TH_M\", SUM('MetricsByItemandDay'[Throttling (min)]),\n",
                "                                            \"C_U\", SUM('MetricsByItemandDay'[count_users]),\n",
                "                                            \"C_SO\", SUM('MetricsByItemandDay'[count_successful_operations]),\n",
                "                                            \"C_RO\", SUM('MetricsByItemandDay'[count_rejected_operations]),\n",
                "                                            \"C_O\", SUM('MetricsByItemandDay'[count_operations]),\n",
                "                                            \"C_IO\", SUM('MetricsByItemandDay'[count_Invalid_operations]),\n",
                "                                            \"C_FO\", SUM('MetricsByItemandDay'[count_failure_operations]),\n",
                "                                            \"C_CO\", SUM('MetricsByItemandDay'[count_cancelled_operations])\n",
                "                                            )\n",
                "                    EVALUATE\n",
                "                        ADDCOLUMNS(\n",
                "                            FILTER(__DS0Core, [S_CU] > 0),\n",
                "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
                "                        ) ORDER BY [S_CU] DESC\n",
                "                    \"\"\"\n",
                "\n",
                "            # Execute DAX query\n",
                "            capacity_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=dax_query)\n",
                "            capacity_df.columns = ['CapacityId', 'WorkspaceId', 'Date',  \n",
                "                                    'ItemKind', 'DurationInSec','TotalCUs', 'ThrottlingInMin', \n",
                "                                    'UserCount','SuccessOperationCount', 'RejectedOperationCount','OperationCount',\n",
                "                                    'InvalidOperationCount','FailureOperationCount','CancelledOperationCount', 'DateKey']\n",
                "            \n",
                "            if not(capacity_df.empty):\n",
                "                # Transfer pandas df to spark df\n",
                "                capacity_df = spark.createDataFrame(capacity_df)\n",
                "\n",
                "                if display_data:\n",
                "                    display(capacity_df)\n",
                "\n",
                "                # Write prepared bronze_df to silver delta table\n",
                "                print(f\"Appending data. Capacity: {capacity_id}. Date: {date_label}\")\n",
                "                capacity_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(silver_table_name)\n",
                "            else:\n",
                "                print(f\"No data. Capacity: {capacity_id}. Date: {date_label}\")\n",
                "\n",
                "    except Exception as ex:\n",
                "        print(ex)\n",
                "        continue"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4ac27758-0175-46eb-9b68-d996b908bf76",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:14:53.7370722Z\",\"execution_finish_time\":\"2025-01-27T09:14:54.586259Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Get Silver table data\n",
                "query = \"SELECT * FROM \" + silver_table_name\n",
                "silver_df = spark.sql(query)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7bb0ff85-ab07-4f5e-bf5e-49475e7f2be7",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:14:54.7157017Z\",\"execution_finish_time\":\"2025-01-27T09:15:03.1027032Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Check if gold table exists\n",
                "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):\n",
                "    # if exists -> MERGE to gold\n",
                "    print(\"Gold table exists and will be merged.\")\n",
                "\n",
                "    gold_df = DeltaTable.forPath(spark, gold_table_name_with_prefix)\n",
                "    # Merge silver (s = source) to gold (t = target)\n",
                "    gold_df.alias('t') \\\n",
                "    .merge(\n",
                "        silver_df.alias('s'),\n",
                "        \"s.CapacityId = t.CapacityId AND s.WorkspaceId = t.WorkspaceId AND s.Date = t.Date AND s.ItemKind = t.ItemKind\"\n",
                "    ) \\\n",
                "    .whenMatchedUpdate(set =\n",
                "        {\n",
                "             \"DurationInSec\": \"s.DurationInSec\"\n",
                "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
                "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
                "            ,\"UserCount\": \"s.UserCount\"\n",
                "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
                "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
                "            ,\"OperationCount\": \"s.OperationCount\"\n",
                "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
                "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
                "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
                "        }\n",
                "    ) \\\n",
                "    .whenNotMatchedInsert(values =\n",
                "        {\n",
                "             \"CapacityId\": \"s.CapacityId\"\n",
                "            ,\"WorkspaceId\": \"s.WorkspaceId\"\n",
                "            ,\"Date\": \"s.Date\"\n",
                "            ,\"ItemKind\": \"s.ItemKind\"\n",
                "            ,\"DurationInSec\": \"s.DurationInSec\"\n",
                "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
                "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
                "            ,\"UserCount\": \"s.UserCount\"\n",
                "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
                "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
                "            ,\"OperationCount\": \"s.OperationCount\"\n",
                "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
                "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
                "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
                "            ,\"DateKey\": \"s.DateKey\"\n",
                "        }\n",
                "    ) \\\n",
                "    .execute()\n",
                "\n",
                "else:\n",
                "    # else -> INSERT to gold\n",
                "    print(\"Gold table will be created.\")\n",
                "\n",
                "    silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3971a614-bc18-4c97-9981-3e43b20ab119",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:15:03.2599749Z\",\"execution_finish_time\":\"2025-01-27T09:15:04.891592Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Clean Silver table\n",
                "query = \"DELETE FROM \" + silver_table_name\n",
                "spark.sql(query)"
            ]
        }
    ],
    "metadata": {
        "a365ComputeOptions": null,
        "dependencies": {
            "environment": {},
            "lakehouse": {
                "default_lakehouse": "729eb8a2-8070-5ed8-ad43-dccbc00b32af",
                "default_lakehouse_name": "FUAM_Lakehouse",
                "default_lakehouse_workspace_id": "eb764c8b-cf3b-55be-adf4-348fe9233657",
                "known_lakehouses": []
            }
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Synapse PySpark",
            "language": "Python",
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "sessionKeepAliveTimeout": 0,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "widgets": {}
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
