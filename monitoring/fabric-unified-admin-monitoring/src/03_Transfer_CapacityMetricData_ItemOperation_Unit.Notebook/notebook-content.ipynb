{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark",
      "jupyter_kernel_name": null
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d",
        "known_lakehouses": [
          {
            "id": "6cff634b-88f7-3505-bed2-c03a36776a8b"
          }
        ]
      },
      "environment": {}
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    }
  },
  "cells": [
    {
      "id": "877fe097-6ce2-4f3e-a8c1-77955a1205ac",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Get Capacity Metrics (by Item by Operation)\n",
        "by Workspace by Kind by Item by Operation by Day\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: #D69AFE;\">**MERGE**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_Capacity_Metrics_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Capacity Metrics** via SemPy DAX execute query function\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**1 Delta table** in FUAM_Lakehouse \n",
        "- **gold_table_name** variable value\n"
      ]
    },
    {
      "id": "729970cb-58a0-4e34-90b9-56bda8b89e6e",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "import sempy.fabric as fabric\n",
        "from datetime import datetime, timedelta\n",
        "import datetime as dt\n",
        "import pyspark.sql.functions as f\n",
        "from delta.tables import *"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0f17a208-9e9d-4b6a-a65e-45a35ce69d73",
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "## Parameters\n",
        "# These parameters will be overwritten while executing the notebook \n",
        "# from Load_FUAM_Data_E2E Pipeline\n",
        "metric_days_in_scope = 3\n",
        "metric_workspace = \"d0182223-a6a9-4aa1-bf5c-24d8ee582b9d\"\n",
        "metric_dataset = \"fe58232b-5b8a-4c8a-9c94-83db50937bd7\"\n",
        "display_data = True"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0f076e4f-def1-4542-a2b2-513517a4073f",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "## Variables\n",
        "silver_table_name = \"FUAM_Staging_Lakehouse.capacity_metrics_by_item_by_operation_by_day_silver\"\n",
        "gold_table_name = \"capacity_metrics_by_item_by_operation_by_day\"\n",
        "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\"\n",
        "count_of_connectivity_errors = 0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "69acabad-0434-4f97-afc9-71bede3da1d1",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Check Table Status\n",
        "version = ''\n",
        "\n",
        "try: \n",
        "    check_table_structure_query = \"\"\"DEFINE    MPARAMETER 'DefaultCapacityID' = \"0000000-0000-0000-0000-00000000\"\n",
        "                                    EVALUATE   SUMMARIZECOLUMNS(\"Background billable CU %\", [Background billable CU %]    )\"\"\"\n",
        "    check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)\n",
        "    print(\"INFO: Test for v47 successful\")\n",
        "    version = 'v47'\n",
        "except:\n",
        "    print(\"INFO: Test for v47 failed\")\n",
        "\n",
        "if version == '':\n",
        "    try:\n",
        "        check_table_structure_query = \"\"\"EVALUATE ROW(\"Background billable CU %\", 'All Measures'[Background billable CU %])\"\"\"\n",
        "        check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)\n",
        "        print(\"INFO: Test for v40 successful\")\n",
        "        version = 'v40'\n",
        "    except:\n",
        "        print(\"INFO: Test for v40 failed\")\n",
        "\n",
        "if version == '':\n",
        "    try:\n",
        "        check_table_structure_query_alternative = \"\"\"EVALUATE ROW(\"xBackground__\", 'All Measures'[xBackground %])\"\"\"\n",
        "        check_table_structure_df_alternative = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query_alternative)\n",
        "        version = 'v37'\n",
        "        print(\"INFO: Test for v37 successful\")\n",
        "    except:\n",
        "        print(\"INFO: Test for v37 failed\")\n",
        "\n",
        "\n",
        "# Validate version compatibility\n",
        "if version != '':\n",
        "    print( f'INFO: Version {version} is valid')\n",
        "else: \n",
        "    raise Exception(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fbb0b9d9-a24e-4c08-97cf-3bd9848f4f9f",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Fetch capacities from connected capacity metrics app\n",
        "try:\n",
        "  if version in ['v47', 'v44', 'v40']:\n",
        "    capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities, \"capacity Id\", Capacities[capacity Id] , \"state\" , Capacities[state] )\"\"\"\n",
        "  else:\n",
        "    capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities, \"capacity Id\", Capacities[CapacityId] , \"state\" , Capacities[state] )\"\"\"\n",
        "  capacities = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=capacity_query)\n",
        "  capacities.columns = ['CapacityId', 'State']\n",
        "  capacities = spark.createDataFrame(capacities)\n",
        "except Exception as e:\n",
        "  notebookutils.notebook.exit(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "36bbd404-daa6-48c7-8139-9b55021254c8",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "source": [
        "if display_data:\n",
        "    display(capacities)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1134b4ff-8e41-4346-8f30-aa036526a1f7",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Iterate days\n",
        "def iterate_dates(start_date, end_date):\n",
        "    # Init array\n",
        "    dates = []\n",
        "    # Convert string inputs to datetime objects\n",
        "    start = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    \n",
        "    # Initialize current date as start date\n",
        "    current_date = start.date()\n",
        "    \n",
        "    while current_date <= end.date():\n",
        "\n",
        "        dates.append(\n",
        "            {\n",
        "                \"date\": current_date,\n",
        "                \"year\": current_date.year,\n",
        "                \"month\": current_date.month,\n",
        "                \"day\": current_date.day\n",
        "            })\n",
        "        # Move to the next day\n",
        "        current_date += dt.timedelta(days=1)\n",
        "\n",
        "    return dates"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6de110f2-9985-4d7a-8282-cada338b27d2",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Clean Silver table\n",
        "try:\n",
        "    query = \"DELETE FROM \" + silver_table_name\n",
        "    spark.sql(query)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(\"INFO: Silver table doesn't exist yet.\") "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "20e1c831-69c2-4845-b61d-4b1d2e0f7ab6",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "source": [
        "# Iterate workspaces and days\n",
        "for ca in capacities.collect():\n",
        "    capacity_id = ca[0]\n",
        "\n",
        "    print(f\"INFO: Scoped CapacityId: {capacity_id}\")\n",
        "\n",
        "    try:\n",
        "        # Get today's date\n",
        "        today = datetime.now()\n",
        "\n",
        "        # Calculate the dates between today and days_in_scope\n",
        "        days_ago = today - timedelta(days=metric_days_in_scope)\n",
        "\n",
        "        # Format dates in 'yyyy-mm-dd'\n",
        "        today_str = today.strftime('%Y-%m-%d')\n",
        "        days_ago_str = days_ago.strftime('%Y-%m-%d')\n",
        "\n",
        "        date_array = iterate_dates(days_ago_str, end_date=today_str)\n",
        "\n",
        "        # Iterate days for current capacity\n",
        "        for date in date_array:\n",
        "            year = date['year']\n",
        "            month = date['month']\n",
        "            day = date['day']\n",
        "            date_label = str(year) + '-' + str(month) + '-' + str(day)\n",
        "            print(f\"INFO: Get data for CapacityId: {capacity_id}\")\n",
        "\n",
        "            dax_query_v47 = f\"\"\"                \n",
        "             DEFINE\n",
        "                   MPARAMETER 'CapacitiesList' = {{ \\\"{capacity_id}\\\" }}\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('Metrics By Item Operation And Day'[Date])),\n",
        "                                            'Metrics By Item Operation And Day'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                SUMMARIZECOLUMNS(\n",
        "                                        Capacities[capacity Id],\n",
        "                                        Items[Workspace Id],\n",
        "                                        'Metrics By Item Operation And Day'[Date],\n",
        "                                        'Metrics By Item Operation And Day'[Item Id],\n",
        "                                        'Items'[Item Kind],\n",
        "                                        'Metrics By Item Operation And Day'[Operation name],\n",
        "                                        FILTER(Capacities, Capacities[capacity Id] = \\\"{capacity_id}\\\" ),\n",
        "                                        __DS0FilterTable,\n",
        "                                        \"S_Dur\", SUM('Metrics By Item Operation And Day'[Duration (s)]),\n",
        "                                        \"S_CU\", SUM('Metrics By Item Operation And Day'[CU (s)]),\n",
        "                                        \"TH_M\", SUM('Metrics By Item Operation And Day'[Throttling (min)]),\n",
        "                                        \"C_U\", SUM('Metrics By Item Operation And Day'[Users]),\n",
        "                                        \"C_SO\", SUM('Metrics By Item Operation And Day'[Successful operations]),\n",
        "                                        \"C_RO\", SUM('Metrics By Item Operation And Day'[Rejected operations]),\n",
        "                                        \"C_O\", SUM('Metrics By Item Operation And Day'[Operations]),\n",
        "                                        \"C_IO\", SUM('Metrics By Item Operation And Day'[Invalid operations]),\n",
        "                                        \"C_FO\", SUM('Metrics By Item Operation And Day'[Failed operations]),\n",
        "                                        \"C_CO\", SUM('Metrics By Item Operation And Day'[Cancelled operations])\n",
        "                                        )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC\n",
        "            \"\"\"\n",
        "\n",
        "            dax_query_v44 = f\"\"\"                \n",
        "             DEFINE\n",
        "                   MPARAMETER 'CapacitiesList' = {{ \\\"{capacity_id}\\\" }}\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('Metrics By Item Operation And Day'[Date])),\n",
        "                                            'Metrics By Item Operation And Day'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                SUMMARIZECOLUMNS(\n",
        "                                        Capacities[capacity Id],\n",
        "                                        Items[Workspace Id],\n",
        "                                        'Metrics By Item Operation And Day'[Date],\n",
        "                                        'Metrics By Item Operation And Day'[Item Id],\n",
        "                                        'Items'[Item Kind],\n",
        "                                        'Metrics By Item Operation And Day'[Operation name],\n",
        "                                        FILTER(Capacities, Capacities[capacity Id] = \\\"{capacity_id}\\\" ),\n",
        "                                        __DS0FilterTable,\n",
        "                                        \"S_Dur\", SUM('Metrics By Item Operation And Day'[Duration (s)]),\n",
        "                                        \"S_CU\", SUM('Metrics By Item Operation And Day'[CU (s)]),\n",
        "                                        \"TH_M\", SUM('Metrics By Item Operation And Day'[Throttling (min)]),\n",
        "                                        \"C_U\", SUM('Metrics By Item Operation And Day'[Users]),\n",
        "                                        \"C_SO\", SUM('Metrics By Item Operation And Day'[Successful operations]),\n",
        "                                        \"C_RO\", SUM('Metrics By Item Operation And Day'[Rejected operations]),\n",
        "                                        \"C_O\", SUM('Metrics By Item Operation And Day'[Operations]),\n",
        "                                        \"C_IO\", SUM('Metrics By Item Operation And Day'[Invalid operations]),\n",
        "                                        \"C_FO\", SUM('Metrics By Item Operation And Day'[Failed operations]),\n",
        "                                        \"C_CO\", SUM('Metrics By Item Operation And Day'[Cancelled operations])\n",
        "                                        )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC\n",
        "            \"\"\"\n",
        "\n",
        "            dax_query_v40 = f\"\"\"\n",
        "                DEFINE\n",
        "                    MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('Metrics By Item Operation And Day'[Date])),\n",
        "                                            'Metrics By Item Operation And Day'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                SUMMARIZECOLUMNS(\n",
        "                                        Capacities[capacity Id],\n",
        "                                        Items[Workspace Id],\n",
        "                                        'Metrics By Item Operation And Day'[Date],\n",
        "                                        'Metrics By Item Operation And Day'[Item Id],\n",
        "                                        'Items'[Item Kind],\n",
        "                                        'Metrics By Item Operation And Day'[Operation name],\n",
        "                                        FILTER(Capacities, Capacities[capacity Id] = \\\"{capacity_id}\\\" ),\n",
        "                                        __DS0FilterTable,\n",
        "                                        \"S_Dur\", SUM('Metrics By Item Operation And Day'[Duration (s)]),\n",
        "                                        \"S_CU\", SUM('Metrics By Item Operation And Day'[CU (s)]),\n",
        "                                        \"TH_M\", SUM('Metrics By Item Operation And Day'[Throttling (min)]),\n",
        "                                        \"C_U\", SUM('Metrics By Item Operation And Day'[Users]),\n",
        "                                        \"C_SO\", SUM('Metrics By Item Operation And Day'[Successful operations]),\n",
        "                                        \"C_RO\", SUM('Metrics By Item Operation And Day'[Rejected operations]),\n",
        "                                        \"C_O\", SUM('Metrics By Item Operation And Day'[Operations]),\n",
        "                                        \"C_IO\", SUM('Metrics By Item Operation And Day'[Invalid operations]),\n",
        "                                        \"C_FO\", SUM('Metrics By Item Operation And Day'[Failed operations]),\n",
        "                                        \"C_CO\", SUM('Metrics By Item Operation And Day'[Cancelled operations])\n",
        "                                        )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC\n",
        "     \n",
        "            \"\"\"\n",
        "\n",
        "            dax_query_v37 = f\"\"\"\n",
        "                DEFINE\n",
        "                    MPARAMETER 'CapacityID' =  \\\"{capacity_id}\\\"\n",
        "\n",
        "                    VAR __DS0FilterTable = \n",
        "                                        FILTER(\n",
        "                                            KEEPFILTERS(VALUES('MetricsByItemandOperationandDay'[Date])),\n",
        "                                            'MetricsByItemandOperationandDay'[Date] = DATE({year}, {month}, {day})\n",
        "                                        )\n",
        "\n",
        "                    VAR __DS0Core = \n",
        "                                SUMMARIZECOLUMNS(\n",
        "                                        Capacities[capacityId],\n",
        "                                        Items[WorkspaceId],\n",
        "                                        'MetricsByItemandOperationandDay'[Date],\n",
        "                                        'MetricsByItemandOperationandDay'[ItemId],\n",
        "                                        'Items'[ItemKind],\n",
        "                                        'MetricsByItemandOperationandDay'[OperationName],\n",
        "                                        FILTER(Capacities, Capacities[capacityId] = \\\"{capacity_id}\\\" ),\n",
        "                                        __DS0FilterTable,\n",
        "                                        \"S_Dur\", SUM('MetricsByItemandOperationandDay'[sum_duration]),\n",
        "                                        \"S_CU\", SUM('MetricsByItemandOperationandDay'[sum_CU]),\n",
        "                                        \"TH_M\", SUM('MetricsByItemandOperationandDay'[Throttling (min)]),\n",
        "                                        \"C_U\", SUM('MetricsByItemandOperationandDay'[count_users]),\n",
        "                                        \"C_SO\", SUM('MetricsByItemandOperationandDay'[count_successful_operations]),\n",
        "                                        \"C_RO\", SUM('MetricsByItemandOperationandDay'[count_rejected_operations]),\n",
        "                                        \"C_O\", SUM('MetricsByItemandOperationandDay'[count_operations]),\n",
        "                                        \"C_IO\", SUM('MetricsByItemandOperationandDay'[count_Invalid_operations]),\n",
        "                                        \"C_FO\", SUM('MetricsByItemandOperationandDay'[count_failure_operations]),\n",
        "                                        \"C_CO\", SUM('MetricsByItemandOperationandDay'[count_cancelled_operations])\n",
        "                                        )\n",
        "                    EVALUATE\n",
        "                        ADDCOLUMNS(\n",
        "                            FILTER(__DS0Core, [S_CU] > 0),\n",
        "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
        "                        ) ORDER BY [S_CU] DESC      \n",
        "            \"\"\"\n",
        "            \n",
        "\n",
        "            dax_query = \"\"\n",
        "            # Choose query\n",
        "            if version == 'v47':\n",
        "                dax_query = dax_query_v47\n",
        "            elif version == 'v44':\n",
        "                dax_query = dax_query_v44\n",
        "            elif version == 'v40':\n",
        "                dax_query = dax_query_v40\n",
        "            elif version == 'v37':\n",
        "                dax_query = dax_query_v37\n",
        "\n",
        "            # Execute DAX query\n",
        "            capacity_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=dax_query)\n",
        "            capacity_df.columns = [ 'CapacityId', 'WorkspaceId', 'Date', \n",
        "                                    'ItemId', 'ItemKind', 'OperationName', 'DurationInSec',\n",
        "                                    'TotalCUs', 'ThrottlingInMin', 'UserCount','SuccessOperationCount', \n",
        "                                    'RejectedOperationCount','OperationCount','InvalidOperationCount',\n",
        "                                    'FailureOperationCount','CancelledOperationCount', 'DateKey'\n",
        "                                    ]\n",
        "            if not(capacity_df.empty):\n",
        "                # Transfer pandas df to spark df\n",
        "                capacity_df = spark.createDataFrame(capacity_df)\n",
        "\n",
        "                if display_data:\n",
        "                    display(capacity_df)\n",
        "\n",
        "                # Write data in Lakehouse\n",
        "                print(f\"INFO: Appending data for CapacityId: {capacity_id} on Date: {date_label}\")\n",
        "                capacity_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(silver_table_name) \n",
        "            else:\n",
        "                print(f\"INFO: No data for CapacityId: {capacity_id} on Date: {date_label}\")\n",
        "\n",
        "    except Exception as ex:\n",
        "        count_of_connectivity_errors += 1\n",
        "        print('ERROR: Exception for CapacityId: ' + capacity_id + '. ->' + str(ex))\n",
        "        continue\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a9e35939-9c28-4469-a9f0-33814ac4f3d7",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Handle error if interface is not reachable\n",
        "if count_of_connectivity_errors > 0: \n",
        "    print(\"WARNING: Connection to at least one capacity is not possible. Please review the configurations.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2fa52a18-aaa1-48f1-9bd0-6ec7fae7b215",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "try:\n",
        "    # Get Silver table data\n",
        "    query = \"SELECT * FROM  \" + silver_table_name\n",
        "    silver_df = spark.sql(query)\n",
        "\n",
        "except Exception as ex:\n",
        "    # Handle error if interface is not reachable\n",
        "    if count_of_connectivity_errors > 0: \n",
        "        notebookutils.notebook.exit(\"ERROR: Connection to capacity metrics is not possible. Please review the configurations.\")\n",
        "    else:\n",
        "        notebookutils.notebook.exit(\"ERROR: Silver table doesn't exist yet. This issue occurs also when the connection to capacity metrics is not configured correctly or endpoint is not reachable.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fcb33708-3445-4c18-9a41-731b84515456",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Check if gold table exists\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):\n",
        "    # if exists -> MERGE to gold\n",
        "    print(\"INFO: Gold table exists and will be merged.\")\n",
        "\n",
        "    gold_df = DeltaTable.forPath(spark, gold_table_name_with_prefix)\n",
        "    # Merge silver (s = source) to gold (t = target)\n",
        "    gold_df.alias('t') \\\n",
        "    .merge(\n",
        "        silver_df.alias('s'),\n",
        "        \"s.CapacityId = t.CapacityId AND s.WorkspaceId = t.WorkspaceId AND s.Date = t.Date AND s.ItemId = t.ItemId AND s.ItemKind = t.ItemKind AND s.OperationName = t.OperationName\"\n",
        "    ) \\\n",
        "    .whenMatchedUpdate(set =\n",
        "        {\n",
        "             \"DurationInSec\": \"s.DurationInSec\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
        "            ,\"UserCount\": \"s.UserCount\"\n",
        "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
        "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
        "            ,\"OperationCount\": \"s.OperationCount\"\n",
        "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
        "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
        "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
        "        }\n",
        "    ) \\\n",
        "    .whenNotMatchedInsert(values =\n",
        "        {\n",
        "             \"CapacityId\": \"s.CapacityId\"\n",
        "            ,\"WorkspaceId\": \"s.WorkspaceId\"\n",
        "            ,\"Date\": \"s.Date\"\n",
        "            ,\"ItemId\": \"s.ItemId\"\n",
        "            ,\"ItemKind\": \"s.ItemKind\"\n",
        "            ,\"OperationName\": \"s.OperationName\"\n",
        "            ,\"DurationInSec\": \"s.DurationInSec\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
        "            ,\"UserCount\": \"s.UserCount\"\n",
        "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
        "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
        "            ,\"OperationCount\": \"s.OperationCount\"\n",
        "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
        "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
        "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
        "            ,\"DateKey\": \"s.DateKey\"\n",
        "        }\n",
        "    ) \\\n",
        "    .execute()\n",
        "\n",
        "else:\n",
        "    # else -> INSERT to gold\n",
        "    print(\"INFO: Gold table will be created.\")\n",
        "\n",
        "    silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "85deb365-54fd-4178-8bd9-898d04644cff",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Clean Silver table\n",
        "query = \"DELETE FROM \" + silver_table_name\n",
        "spark.sql(query)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}