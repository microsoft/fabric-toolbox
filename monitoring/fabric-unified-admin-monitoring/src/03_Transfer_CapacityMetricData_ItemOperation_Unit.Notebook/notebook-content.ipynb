{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "#### Get Capacity Metrics (by Item by Operation)\n",
                "by Workspace by Kind by Item by Operation by Day\n",
                "\n",
                "##### Data ingestion strategy:\n",
                "<mark style=\"background: #D69AFE;\">**MERGE**</mark>\n",
                "\n",
                "##### Related pipeline:\n",
                "\n",
                "**Load_Capacity_Metrics_E2E**\n",
                "\n",
                "##### Source:\n",
                "\n",
                "**CapacityMetricCloneDQ** via SemPy DAX execute query function\n",
                "\n",
                "##### Target:\n",
                "\n",
                "**1 Delta table** in FUAM_Lakehouse \n",
                "- **gold_table_name** variable value\n"
            ],
            "metadata": {},
            "id": "eb45896e-9773-4d41-8c08-7b66409f64f3"
        },
        {
            "cell_type": "code",
            "source": [
                "import sempy.fabric as fabric\n",
                "from datetime import datetime, timedelta\n",
                "import datetime as dt\n",
                "import pyspark.sql.functions as f\n",
                "from delta.tables import *"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:17:09.3496019Z\",\"execution_finish_time\":\"2025-01-27T09:17:19.4994196Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "c3aa73d3-30ba-4b2c-8b3e-7902a84e14f4"
        },
        {
            "cell_type": "code",
            "source": [
                "## Parameters\n",
                "metric_days_in_scope = 14\n",
                "metric_workspace = \"Microsoft Fabric Capacity Metrics v30\"\n",
                "metric_dataset = \"Fabric Capacity Metrics\"\n",
                "display_data = True"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ],
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:17:19.6315135Z\",\"execution_finish_time\":\"2025-01-27T09:17:19.8841571Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "32769254-61c2-49c6-9d1a-b9deace1672a"
        },
        {
            "cell_type": "code",
            "source": [
                "## Variables\n",
                "silver_table_name = \"FUAM_Staging_Lakehouse.capacity_metrics_by_item_by_operation_by_day_silver\"\n",
                "gold_table_name = \"capacity_metrics_by_item_by_operation_by_day\"\n",
                "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\""
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:17:20.0398611Z\",\"execution_finish_time\":\"2025-01-27T09:17:20.3168263Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "c4dc4f08-4c44-4bbf-a520-416b2d1677af"
        },
        {
            "cell_type": "code",
            "source": [
                "# Fetch Workspaces on Capacities (without PPT capacities)\n",
                "query = \"\"\"\n",
                "SELECT DISTINCT\n",
                "     ca.CapacityId\n",
                "  \n",
                "FROM FUAM_Lakehouse.capacities ca \n",
                "WHERE ca.SKU != 'PP3'\n",
                "AND ca.state = 'Active'\n",
                "\"\"\"\n",
                "capacities = spark.sql(query)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:17:20.4656738Z\",\"execution_finish_time\":\"2025-01-27T09:17:34.9671979Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "7f45ab4d-77a8-4e3e-b447-60080e961895"
        },
        {
            "cell_type": "code",
            "source": [
                "if display_data:\n",
                "    display(capacities)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "collapsed": false,
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:17:35.1045996Z\",\"execution_finish_time\":\"2025-01-27T09:17:41.4964595Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "da6f2a2d-5b97-458e-998e-74c9a9979c53"
        },
        {
            "cell_type": "code",
            "source": [
                "# Iterate days\n",
                "def iterate_dates(start_date, end_date):\n",
                "    # Init array\n",
                "    dates = []\n",
                "    # Convert string inputs to datetime objects\n",
                "    start = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
                "    end = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
                "    \n",
                "    # Initialize current date as start date\n",
                "    current_date = start.date()\n",
                "    \n",
                "    while current_date <= end.date():\n",
                "\n",
                "        dates.append(\n",
                "            {\n",
                "                \"date\": current_date,\n",
                "                \"year\": current_date.year,\n",
                "                \"month\": current_date.month,\n",
                "                \"day\": current_date.day\n",
                "            })\n",
                "        # Move to the next day\n",
                "        current_date += dt.timedelta(days=1)\n",
                "\n",
                "    return dates"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:17:42.1797494Z\",\"execution_finish_time\":\"2025-01-27T09:17:42.5414961Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "ff88aaa6-f73b-4899-8f06-bdf505744ffa"
        },
        {
            "cell_type": "code",
            "source": [
                "# Clean Silver table\n",
                "try:\n",
                "    query = \"DELETE FROM \" + silver_table_name\n",
                "    spark.sql(query)\n",
                "\n",
                "except Exception as ex:\n",
                "    print(\"Silver table doesn't exist yet.\") "
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:17:42.7004863Z\",\"execution_finish_time\":\"2025-01-27T09:17:51.0013938Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "76ade6a9-b811-492a-816e-1d6f74ab426c"
        },
        {
            "cell_type": "code",
            "source": [
                "# Iterate workspaces and days\n",
                "for ca in capacities.collect():\n",
                "    capacity_id = ca[0]\n",
                "    print(capacity_id)\n",
                "\n",
                "    try:\n",
                "        # Get today's date\n",
                "        today = datetime.now()\n",
                "\n",
                "        # Calculate the dates between today and days_in_scope\n",
                "        days_ago = today - timedelta(days=metric_days_in_scope)\n",
                "\n",
                "        # Format dates in 'yyyy-mm-dd'\n",
                "        today_str = today.strftime('%Y-%m-%d')\n",
                "        days_ago_str = days_ago.strftime('%Y-%m-%d')\n",
                "\n",
                "        date_array = iterate_dates(days_ago_str, end_date=today_str)\n",
                "\n",
                "        # Iterate days for current capacity\n",
                "        for date in date_array:\n",
                "            year = date['year']\n",
                "            month = date['month']\n",
                "            day = date['day']\n",
                "            date_label = str(year) + '-' + str(month) + '-' + str(day)\n",
                "            print(capacity_id)\n",
                "\n",
                "            dax_query = f\"\"\"\n",
                "                DEFINE\n",
                "                    MPARAMETER 'CapacityID' =  \\\"{capacity_id}\\\"\n",
                "\n",
                "                    VAR __DS0FilterTable = \n",
                "                                        FILTER(\n",
                "                                            KEEPFILTERS(VALUES('MetricsByItemandOperationandDay'[Date])),\n",
                "                                            'MetricsByItemandOperationandDay'[Date] = DATE({year}, {month}, {day})\n",
                "                                        )\n",
                "\n",
                "                    VAR __DS0Core = \n",
                "                                SUMMARIZECOLUMNS(\n",
                "                                        Capacities[capacityId],\n",
                "                                        Items[WorkspaceId],\n",
                "                                        'MetricsByItemandOperationandDay'[Date],\n",
                "                                        'MetricsByItemandOperationandDay'[ItemId],\n",
                "                                        'Items'[ItemKind],\n",
                "                                        'MetricsByItemandOperationandDay'[OperationName],\n",
                "                                        FILTER(Capacities, Capacities[capacityId] = \\\"{capacity_id}\\\" ),\n",
                "                                        __DS0FilterTable,\n",
                "                                        \"S_Dur\", SUM('MetricsByItemandOperationandDay'[sum_duration]),\n",
                "                                        \"S_CU\", SUM('MetricsByItemandOperationandDay'[sum_CU]),\n",
                "                                        \"TH_M\", SUM('MetricsByItemandOperationandDay'[Throttling (min)]),\n",
                "                                        \"C_U\", SUM('MetricsByItemandOperationandDay'[count_users]),\n",
                "                                        \"C_SO\", SUM('MetricsByItemandOperationandDay'[count_successful_operations]),\n",
                "                                        \"C_RO\", SUM('MetricsByItemandOperationandDay'[count_rejected_operations]),\n",
                "                                        \"C_O\", SUM('MetricsByItemandOperationandDay'[count_operations]),\n",
                "                                        \"C_IO\", SUM('MetricsByItemandOperationandDay'[count_Invalid_operations]),\n",
                "                                        \"C_FO\", SUM('MetricsByItemandOperationandDay'[count_failure_operations]),\n",
                "                                        \"C_CO\", SUM('MetricsByItemandOperationandDay'[count_cancelled_operations])\n",
                "                                        )\n",
                "                    EVALUATE\n",
                "                        ADDCOLUMNS(\n",
                "                            FILTER(__DS0Core, [S_CU] > 0),\n",
                "                            \"DateKey\", FORMAT([Date], \"yyyymmdd\")\n",
                "                        ) ORDER BY [S_CU] DESC      \n",
                "            \"\"\"\n",
                "\n",
                "            # Execute DAX query\n",
                "            capacity_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=dax_query)\n",
                "            capacity_df.columns = [ 'CapacityId', 'WorkspaceId', 'Date', \n",
                "                                    'ItemId', 'ItemKind', 'OperationName', 'DurationInSec',\n",
                "                                    'TotalCUs', 'ThrottlingInMin', 'UserCount','SuccessOperationCount', \n",
                "                                    'RejectedOperationCount','OperationCount','InvalidOperationCount',\n",
                "                                    'FailureOperationCount','CancelledOperationCount', 'DateKey'\n",
                "                                    ]\n",
                "            if not(capacity_df.empty):\n",
                "                # Transfer pandas df to spark df\n",
                "                capacity_df = spark.createDataFrame(capacity_df)\n",
                "\n",
                "                # Write data in Lakehouse\n",
                "                print(f\"Appending data. Capacity: {capacity_id}. Date: {date_label}\")\n",
                "                capacity_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(silver_table_name) \n",
                "            else:\n",
                "                print(f\"No data. Capacity: {capacity_id}. Date: {date_label}. \")\n",
                "\n",
                "    except Exception as ex:\n",
                "        print('Expection for ca: ' + capacity_id + '. ->' + str(ex))\n",
                "        continue\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:17:51.1372474Z\",\"execution_finish_time\":\"2025-01-27T09:20:59.405519Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "c7296361-8c10-4d26-a1ed-f99a47d60b44"
        },
        {
            "cell_type": "code",
            "source": [
                "# Get Silver table data\n",
                "query = \"SELECT * FROM \" + silver_table_name\n",
                "silver_df = spark.sql(query)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:20:59.5752515Z\",\"execution_finish_time\":\"2025-01-27T09:21:00.4848088Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "7a5006ec-2a2f-4c7a-8563-e1badea4d3a0"
        },
        {
            "cell_type": "code",
            "source": [
                "# Check if gold table exists\n",
                "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):\n",
                "    # if exists -> MERGE to gold\n",
                "    print(\"Gold table exists and will be merged.\")\n",
                "\n",
                "    gold_df = DeltaTable.forPath(spark, gold_table_name_with_prefix)\n",
                "    # Merge silver (s = source) to gold (t = target)\n",
                "    gold_df.alias('t') \\\n",
                "    .merge(\n",
                "        silver_df.alias('s'),\n",
                "        \"s.CapacityId = t.CapacityId AND s.WorkspaceId = t.WorkspaceId AND s.Date = t.Date AND s.ItemId = t.ItemId AND s.ItemKind = t.ItemKind AND s.OperationName = t.OperationName\"\n",
                "    ) \\\n",
                "    .whenMatchedUpdate(set =\n",
                "        {\n",
                "             \"DurationInSec\": \"s.DurationInSec\"\n",
                "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
                "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
                "            ,\"UserCount\": \"s.UserCount\"\n",
                "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
                "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
                "            ,\"OperationCount\": \"s.OperationCount\"\n",
                "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
                "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
                "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
                "        }\n",
                "    ) \\\n",
                "    .whenNotMatchedInsert(values =\n",
                "        {\n",
                "             \"CapacityId\": \"s.CapacityId\"\n",
                "            ,\"WorkspaceId\": \"s.WorkspaceId\"\n",
                "            ,\"Date\": \"s.Date\"\n",
                "            ,\"ItemId\": \"s.ItemId\"\n",
                "            ,\"ItemKind\": \"s.ItemKind\"\n",
                "            ,\"OperationName\": \"s.OperationName\"\n",
                "            ,\"DurationInSec\": \"s.DurationInSec\"\n",
                "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
                "            ,\"ThrottlingInMin\": \"s.ThrottlingInMin\"\n",
                "            ,\"UserCount\": \"s.UserCount\"\n",
                "            ,\"SuccessOperationCount\": \"s.SuccessOperationCount\"\n",
                "            ,\"RejectedOperationCount\": \"s.RejectedOperationCount\"\n",
                "            ,\"OperationCount\": \"s.OperationCount\"\n",
                "            ,\"InvalidOperationCount\": \"s.InvalidOperationCount\"\n",
                "            ,\"FailureOperationCount\": \"s.FailureOperationCount\"\n",
                "            ,\"CancelledOperationCount\": \"s.CancelledOperationCount\"\n",
                "            ,\"DateKey\": \"s.DateKey\"\n",
                "        }\n",
                "    ) \\\n",
                "    .execute()\n",
                "\n",
                "else:\n",
                "    # else -> INSERT to gold\n",
                "    print(\"Gold table will be created.\")\n",
                "\n",
                "    silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:21:00.620876Z\",\"execution_finish_time\":\"2025-01-27T09:21:10.9922858Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "2cb48c88-62e9-4cb0-b8e9-932900433c2d"
        },
        {
            "cell_type": "code",
            "source": [
                "# Clean Silver table\n",
                "query = \"DELETE FROM \" + silver_table_name\n",
                "spark.sql(query)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:21:11.1180224Z\",\"execution_finish_time\":\"2025-01-27T09:21:12.6630115Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "37a20095-46ec-4a54-8615-74ae1e2edf67"
        }
    ],
    "metadata": {
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "widgets": {},
        "sessionKeepAliveTimeout": 0,
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "kernelspec": {
            "name": "synapse_pyspark",
            "language": "Python",
            "display_name": "Synapse PySpark"
        },
        "a365ComputeOptions": null,
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "synapse_widget": {
            "version": "0.1",
            "state": {}
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "dependencies": {
            "lakehouse": {
                "default_lakehouse": "729eb8a2-8070-5ed8-ad43-dccbc00b32af",
                "default_lakehouse_name": "FUAM_Lakehouse",
                "default_lakehouse_workspace_id": "eb764c8b-cf3b-55be-adf4-348fe9233657"
            },
            "environment": {}
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}