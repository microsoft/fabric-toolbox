{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Capacity Refreshables\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: #D69AFE;\">**MERGE**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_Capacity_Refreshables_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Files** from FUAM_Lakehouse folder **bronze_file_location** variable\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**3 Delta tables** in FUAM_Lakehouse \n",
        "- **gold_days_table_name** variable value\n",
        "- **gold_times_table_name** variable value\n",
        "- **gold_details_table_name** variable value\n"
      ],
      "metadata": {},
      "id": "d711f824-a9b4-430c-9153-c3538176a8c4"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode, to_date, date_format, lit, to_timestamp, unix_timestamp\n",
        "from delta.tables import *\n",
        "import pyspark.sql.functions as f\n",
        "import datetime"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "a9ca0cd9-3c8c-4146-826b-70d8889fe0d5"
    },
    {
      "cell_type": "code",
      "source": [
        "## Parameters\n",
        "display_data = False"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "9f123418-71c2-4738-b46e-fc9935704ea2"
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables\n",
        "bronze_file_location = f\"Files/raw/refreshables/\"\n",
        "\n",
        "gold_main_table_name = \"capacity_refreshables\"\n",
        "gold_main_table_name_with_prefix = f\"Tables/{gold_main_table_name}\"\n",
        "\n",
        "gold_days_table_name = \"capacity_refreshable_days\"\n",
        "gold_days_table_name_with_prefix = f\"Tables/{gold_days_table_name}\"\n",
        "\n",
        "gold_times_table_name = \"capacity_refreshable_times\"\n",
        "gold_times_table_name_with_prefix = f\"Tables/{gold_times_table_name}\"\n",
        "\n",
        "gold_summary_table_name = \"capacity_refreshable_summaries\"\n",
        "gold_summary_table_name_with_prefix = f\"Tables/{gold_summary_table_name}\"\n",
        "\n",
        "gold_details_table_name = \"capacity_refreshable_details\"\n",
        "gold_details_table_name_with_prefix = f\"Tables/{gold_details_table_name}\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "e47d94a1-928d-4805-b71e-e31815baf5c5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get bronze data"
      ],
      "metadata": {},
      "id": "289d6a00-255f-445e-a061-018bdd080b8d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Bronze data\n",
        "bronze_df = spark.read.option(\"multiline\", \"true\").json(bronze_file_location)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "7597a8c5-1b84-4439-bb1c-4bb60ec537cb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode json subset structure\n",
        "exploded_df = bronze_df.select(explode(\"value\").alias(\"d\"))\n",
        "\n",
        "# This prevents the notebook running into an error when no capacity refreshables are existant in the tenant\n",
        "if exploded_df.count() == 0 :\n",
        "    notebookutils.notebook.exit(\"No Capacity Refreshables available\")\n",
        " \n",
        "\n",
        "# Extract json objects to tabular form\n",
        "extracted_df = exploded_df.select(col(\"d.*\")).alias(\"d\")\n",
        "\n",
        "#display(extracted_df)\n",
        "\n",
        "# Convert key(s) to upper case\n",
        "extracted_df = extracted_df.withColumn(\"CapacityId\", f.upper(f.col(\"d.capacity.id\")))\n",
        "extracted_df = extracted_df.withColumn(\"WorkspaceId\", f.upper(f.col(\"d.group.id\")))\n",
        "extracted_df = extracted_df.withColumn(\"ItemId\", f.upper(f.col(\"id\")))\n",
        "\n",
        "# Drop unneccessary columns\n",
        "extracted_df = extracted_df.drop(\"capacity\")\n",
        "extracted_df = extracted_df.drop(\"group\")\n",
        "\n",
        "# Drop duplicates\n",
        "extracted_df = extracted_df.dropDuplicates()\n",
        "\n",
        "if display_data:\n",
        "    display(extracted_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "id": "dbb9a280-4b28-4670-93d4-a25db74f1d9e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract main attributes"
      ],
      "metadata": {},
      "id": "50d378fd-5c72-418a-b059-2bc49a20ecc3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select main columns\n",
        "silver_main_df = extracted_df.select(\n",
        "     col(\"CapacityId\")    \n",
        "    ,col(\"WorkspaceId\")\n",
        "    ,col(\"ItemId\")\n",
        "    ,col(\"d.name\").alias(\"ItemName\")\n",
        "    ,col(\"d.refreshSchedule.enabled\").alias(\"IsRefreshEnabled\")\n",
        "    ,col(\"d.refreshSchedule.localTimeZoneId\").alias(\"LocalTimeZoneId\")\n",
        "    ,col(\"kind\").alias(\"Kind\")\n",
        "    ).dropDuplicates()\n",
        "\n",
        "if display_data:\n",
        "    display(silver_main_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "id": "57df6fa9-1f59-4c2c-94a1-161e44782e75"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge main\n",
        "# Check if gold table exists\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_main_table_name):\n",
        "    # if exists -> MERGE to gold\n",
        "    print(\"Gold table exists and will be merged.\")\n",
        "\n",
        "    gold_main_df = DeltaTable.forPath(spark, gold_main_table_name_with_prefix)\n",
        "    # Merge silver (s = source) to gold (t = target)\n",
        "    gold_main_df.alias('t') \\\n",
        "    .merge(\n",
        "        silver_main_df.alias('s'),\n",
        "        \"s.WorkspaceId = t.WorkspaceId AND s.ItemId = t.ItemId\"\n",
        "    ) \\\n",
        "    .whenMatchedUpdate(set=\n",
        "        {\n",
        "            \"CapacityId\": \"s.CapacityId\",\n",
        "            \"ItemName\": \"s.ItemName\",\n",
        "            \"IsRefreshEnabled\": \"s.IsRefreshEnabled\",\n",
        "            \"LocalTimeZoneId\": \"s.LocalTimeZoneId\",\n",
        "            \"Kind\": \"s.Kind\"\n",
        "        }\n",
        "    ) \\\n",
        "    .whenNotMatchedInsertAll() \\\n",
        "    .execute()\n",
        "    #.whenNotMatchedBySourceDelete() \\\n",
        "    \n",
        "\n",
        "else:\n",
        "    # else -> INSERT to gold\n",
        "    print(\"Gold table will be created.\")\n",
        "\n",
        "    silver_main_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_main_table_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "cc5da048-cbc2-4e99-aefd-2f41dcc243a8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract Days"
      ],
      "metadata": {},
      "id": "a4d5071c-1196-470f-aa98-b63562c88e2a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract days\n",
        "silver_days_df = extracted_df.select(\n",
        "     col(\"CapacityId\")    \n",
        "    ,col(\"WorkspaceId\")\n",
        "    ,col(\"ItemId\")\n",
        "    ,col(\"d.refreshSchedule.days\").alias(\"Days\")\n",
        "    )\n",
        "\n",
        "silver_days_df = silver_days_df.select(\n",
        "      col(\"CapacityId\")\n",
        "     ,col(\"WorkspaceId\")\n",
        "     ,col(\"ItemId\")\n",
        "     ,explode('Days').alias('Day')\n",
        ") \\\n",
        ".dropDuplicates()\n",
        "\n",
        "if display_data:\n",
        "    display(silver_days_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "id": "2db85859-bcf7-4196-8939-71b09a928ef4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge days\n",
        "# Check if gold table exists\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_days_table_name):\n",
        "    # if exists -> MERGE to gold\n",
        "    print(\"Gold table exists and will be merged.\")\n",
        "\n",
        "    gold_days_df = DeltaTable.forPath(spark, gold_days_table_name_with_prefix)\n",
        "    # Merge silver (s = source) to gold (t = target)\n",
        "    gold_days_df.alias('t') \\\n",
        "    .merge(\n",
        "        silver_days_df.alias('s'),\n",
        "        \"s.WorkspaceId = t.WorkspaceId AND s.ItemId = t.ItemId AND s.Day = t.Day\"\n",
        "    ) \\\n",
        "    .whenNotMatchedInsertAll() \\\n",
        "    .execute()\n",
        "    # .whenNotMatchedBySourceDelete() \\\n",
        "\n",
        "else:\n",
        "    # else -> INSERT to gold\n",
        "    print(\"Gold table will be created.\")\n",
        "\n",
        "    silver_days_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_days_table_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "7b3c27e7-b959-4661-9e4c-7227d32976eb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract Times"
      ],
      "metadata": {},
      "id": "122f5763-a50c-4b41-b73d-f32dfeccc661"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract times\n",
        "silver_times_df = extracted_df \\\n",
        ".select(\n",
        "     col(\"CapacityId\")    \n",
        "    ,col(\"WorkspaceId\")\n",
        "    ,col(\"ItemId\")\n",
        "    ,col(\"d.refreshSchedule.times\").alias(\"Times\")\n",
        "    )\n",
        "\n",
        "silver_times_df = silver_times_df.select(\n",
        "      col(\"CapacityId\")\n",
        "     ,col(\"WorkspaceId\")\n",
        "     ,col(\"ItemId\")\n",
        "     ,explode('Times').alias('Time')\n",
        ") \\\n",
        ".dropDuplicates()\n",
        "\n",
        "if display_data:\n",
        "    display(silver_times_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "id": "4aa7a092-bfd3-4664-a4dc-9500c5aa44ff"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge Times\n",
        "# Check if gold table exists\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_times_table_name):\n",
        "    # if exists -> MERGE to gold\n",
        "    print(\"Gold table exists and will be merged.\")\n",
        "\n",
        "    gold_times_df = DeltaTable.forPath(spark, gold_times_table_name_with_prefix)\n",
        "    # Merge silver (s = source) to gold (t = target)\n",
        "    gold_times_df.alias('t') \\\n",
        "    .merge(\n",
        "        silver_times_df.alias('s'),\n",
        "        \"s.WorkspaceId = t.WorkspaceId AND s.ItemId = t.ItemId AND s.Time = t.Time\"\n",
        "    ) \\\n",
        "    .whenNotMatchedInsertAll() \\\n",
        "    .execute()\n",
        "\n",
        "else:\n",
        "    # else -> INSERT to gold\n",
        "    print(\"Gold table will be created.\")\n",
        "\n",
        "    silver_times_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_times_table_name)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "5c39c3be-1e7c-4fdb-ad24-a29e68842361"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract refresh summary & details"
      ],
      "metadata": {},
      "id": "2b1f56c5-f59e-4852-9cab-2aa690240829"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select refresh summary\n",
        "silver_summary_df = None\n",
        "\n",
        "# Try selecting with additional columns\n",
        "try:\n",
        "    silver_summary_df = extracted_df.select(\n",
        "         col(\"CapacityId\")    \n",
        "        ,col(\"WorkspaceId\")\n",
        "        ,col(\"ItemId\")\n",
        "        ,col(\"d.name\").alias(\"ItemName\")\n",
        "\n",
        "        ,date_format(\"startTime\",\"yyyyMMdd\").alias(\"ConsideredStartDateKey\")\n",
        "\n",
        "        ,to_date(col(\"startTime\").substr(1,10), \"yyyy-MM-dd\").alias(\"ConsideredStartDate\")\n",
        "        ,to_date(col(\"endTime\").substr(1,10), \"yyyy-MM-dd\").alias(\"ConsideredEndDate\")\n",
        "\n",
        "        ,to_timestamp(col(\"startTime\")).alias(\"ConsideredStartTime\")\n",
        "        ,to_timestamp(col(\"endTime\")).alias(\"ConsideredEndTime\")\n",
        "\n",
        "\n",
        "        ,col(\"averageDuration\").alias(\"RefreshAverageDuration\")\n",
        "        ,col(\"medianDuration\").alias(\"RefreshMedianDuration\")\n",
        "        ,col(\"refreshCount\").alias(\"RefreshCount\")\n",
        "        )\n",
        "\n",
        "# Try selecting without additional columns\n",
        "except Exception as ex:\n",
        "    print(\"WARNING: Additional columns are not available. The notebook tries to select data without additional columns.\")\n",
        "\n",
        "    silver_summary_df = extracted_df.select(\n",
        "        col(\"CapacityId\")    \n",
        "        ,col(\"WorkspaceId\")\n",
        "        ,col(\"ItemId\")\n",
        "        ,col(\"d.name\").alias(\"ItemName\")\n",
        "\n",
        "        ,date_format(\"startTime\",\"yyyyMMdd\").alias(\"ConsideredStartDateKey\")\n",
        "\n",
        "        ,to_date(col(\"startTime\").substr(1,10), \"yyyy-MM-dd\").alias(\"ConsideredStartDate\")\n",
        "        ,to_date(col(\"endTime\").substr(1,10), \"yyyy-MM-dd\").alias(\"ConsideredEndDate\")\n",
        "\n",
        "        ,to_timestamp(col(\"startTime\")).alias(\"ConsideredStartTime\")\n",
        "        ,to_timestamp(col(\"endTime\")).alias(\"ConsideredEndTime\")\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "id": "f8e3eb1c-bf90-44ac-a96a-1c1025d656cc"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filter data\n",
        "silver_summary_df = silver_summary_df.where(col(\"StartTime\").isNotNull())\n",
        "\n",
        "# Filter data with wrong end date (This can happen in case of new schedules and will be automatically fixed with future run)\n",
        "silver_summary_df = silver_summary_df.where(col(\"ConsideredEndDate\") != \"0001-01-01\")\n",
        "\n",
        "\n",
        "# Calculate duration\n",
        "silver_summary_df = silver_summary_df.withColumn('ConsiderationDurationSeconds', (unix_timestamp(col(\"ConsideredEndTime\")) - unix_timestamp(col(\"ConsideredStartTime\"))))\n",
        "\n",
        "# Drop duplicates\n",
        "silver_summary_df = silver_summary_df.dropDuplicates()\n",
        "\n",
        "if display_data:\n",
        "    display(silver_summary_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "id": "ccac9602-e63b-4ebd-81f9-bb34ac3ac146"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge summary\n",
        "# Check if gold table exists\n",
        "\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_summary_table_name):\n",
        "    # if exists -> MERGE to gold\n",
        "    print(\"Gold table exists and will be merged.\")\n",
        "\n",
        "    gold_summary_df = DeltaTable.forPath(spark, gold_summary_table_name_with_prefix)\n",
        "\n",
        "    try:\n",
        "        # Merge silver (s = source) to gold (t = target)\n",
        "        gold_summary_df.alias('t') \\\n",
        "        .merge(\n",
        "            silver_summary_df.alias('s'),\n",
        "            \"s.WorkspaceId = t.WorkspaceId AND s.ItemId = t.ItemId AND s.ConsideredStartTime = t.ConsideredStartTime AND s.ConsideredEndTime = t.ConsideredEndTime\"\n",
        "        ) \\\n",
        "        .whenMatchedUpdate(set=\n",
        "            {\n",
        "                \"CapacityId\": \"s.CapacityId\",\n",
        "                \"RefreshAverageDuration\": \"s.RefreshAverageDuration\",\n",
        "                \"RefreshMedianDuration\": \"s.RefreshMedianDuration\",\n",
        "                \"RefreshCount\": \"s.RefreshCount\",\n",
        "                \"ConsiderationDurationSeconds\": \"s.ConsiderationDurationSeconds\"\n",
        "            }\n",
        "        ) \\\n",
        "        .whenNotMatchedInsertAll() \\\n",
        "        .execute()\n",
        "\n",
        "    except Exception as ex:\n",
        "        # Alternatively: Merge silver (s = source) to gold (t = target) without additional columns\n",
        "        print(\"WARNING: Additional columns are not available. The notebook tries to merge data without additional columns.\")\n",
        "\n",
        "        gold_summary_df.alias('t') \\\n",
        "        .merge(\n",
        "            silver_summary_df.alias('s'),\n",
        "            \"s.WorkspaceId = t.WorkspaceId AND s.ItemId = t.ItemId AND s.ConsideredStartTime = t.ConsideredStartTime AND s.ConsideredEndTime = t.ConsideredEndTime\"\n",
        "        ) \\\n",
        "        .whenMatchedUpdate(set=\n",
        "            {\n",
        "                \"CapacityId\": \"s.CapacityId\"\n",
        "            }\n",
        "        ) \\\n",
        "        .whenNotMatchedInsert(\n",
        "                values={\n",
        "                    \"CapacityId\": \"s.CapacityId\",\n",
        "                    \"WorkspaceId\": \"s.WorkspaceId\",\n",
        "                    \"ItemId\": \"s.ItemId\",\n",
        "                    \"ConsideredStartDateKey\": \"s.ConsideredStartDateKey\",\n",
        "                    \"ConsideredStartDate\": \"s.ConsideredStartDate\",\n",
        "                    \"ConsideredStartTime\": \"s.ConsideredStartTime\",\n",
        "                    \"ConsideredEndDate\": \"s.ConsideredEndDate\",\n",
        "                    \"ConsideredEndDate\": \"s.ConsideredEndDate\"\n",
        "                    }\n",
        "        ) \\\n",
        "        .execute()\n",
        "\n",
        "else:\n",
        "    # else -> INSERT to gold\n",
        "    print(\"Gold table will be created.\")\n",
        "\n",
        "    silver_summary_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_summary_table_name)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "cd70f2ae-c8a2-41c3-ab89-eba8a4a7be01"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select last refresh details\n",
        "silver_details_df = extracted_df.select(\n",
        "     col(\"CapacityId\")    \n",
        "    ,col(\"WorkspaceId\")\n",
        "    ,col(\"ItemId\")\n",
        "    ,col(\"d.name\").alias(\"ItemName\")\n",
        "\n",
        "    ,col(\"d.lastRefresh.requestId\").alias(\"RequestId\")\n",
        "    ,date_format(\"d.lastRefresh.startTime\",\"yyyyMMdd\").alias(\"LastRefreshStartDateKey\")\n",
        "\n",
        "    ,col(\"d.lastRefresh.status\").alias(\"Status\")\n",
        "    ,col(\"d.lastRefresh.refreshType\").alias(\"RefreshType\")\n",
        "\n",
        "    ,to_date(col(\"d.lastRefresh.startTime\").substr(1,10), \"yyyy-MM-dd\").alias(\"LastRefreshStartDate\")\n",
        "    ,to_date(col(\"d.lastRefresh.endTime\").substr(1,10), \"yyyy-MM-dd\").alias(\"LastRefreshEndDate\")\n",
        "\n",
        "    ,to_timestamp(col(\"d.lastRefresh.startTime\")).alias(\"LastRefreshStartTime\")\n",
        "    ,to_timestamp(col(\"d.lastRefresh.endTime\")).alias(\"LastRefreshEndTime\")\n",
        "\n",
        "    ,date_format(\"d.lastRefresh.startTime\",\"H\").alias(\"LastRefreshStartHour\")\n",
        "    ,date_format(\"d.lastRefresh.endTime\",\"H\").alias(\"LastRefreshEndHour\")\n",
        "    )\n",
        "\n",
        "# Filter data\n",
        "silver_details_df = silver_details_df.where(col(\"RequestId\").isNotNull())\n",
        "\n",
        "# Filter duplicates\n",
        "silver_details_df = silver_details_df.dropDuplicates()\n",
        "\n",
        "# Calculate duration\n",
        "silver_details_df = silver_details_df.withColumn('DurationInSeconds', ( unix_timestamp(col(\"LastRefreshEndTime\")) - unix_timestamp(col(\"LastRefreshStartTime\")) )   )\n",
        "\n",
        "\n",
        "if display_data:\n",
        "    display(silver_details_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "id": "b945ce36-4abd-4405-9764-fb558d2b5a5c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge details\n",
        "# Check if gold table exists\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_details_table_name):\n",
        "    # if exists -> MERGE to gold\n",
        "    print(\"Gold table exists and will be merged.\")\n",
        "\n",
        "    gold_details_df = DeltaTable.forPath(spark, gold_details_table_name_with_prefix)\n",
        "    # Merge silver (s = source) to gold (t = target)\n",
        "    gold_details_df.alias('t') \\\n",
        "    .merge(\n",
        "        silver_details_df.alias('s'),\n",
        "        \"s.WorkspaceId = t.WorkspaceId AND s.ItemId = t.ItemId AND s.RequestId = t.RequestId\"\n",
        "    ) \\\n",
        "    .whenMatchedUpdate(set=\n",
        "        {\n",
        "            \"CapacityId\": \"s.CapacityId\",\n",
        "            \"DurationInSeconds\": \"s.DurationInSeconds\"\n",
        "        }\n",
        "    ) \\\n",
        "    .whenNotMatchedInsertAll() \\\n",
        "    .execute()\n",
        "\n",
        "else:\n",
        "    # else -> INSERT to gold\n",
        "    print(\"Gold table will be created.\")\n",
        "\n",
        "    silver_details_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_details_table_name)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "6de84211-3c1a-4c54-8fcf-0b0902537bb2"
    },
    {
      "cell_type": "code",
      "source": [
        "# write history of bronze files\n",
        "mssparkutils.fs.cp(bronze_file_location, bronze_file_location.replace(\"Files/raw/\", \"Files/history/\") + datetime.datetime.now().strftime('%Y/%m/%d') + \"/\", True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "c5910822-f54b-48af-86ab-d3fd667b5c7a"
    }
  ],
  "metadata": {
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "widgets": {},
    "sessionKeepAliveTimeout": 0,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "a365ComputeOptions": null,
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d"
      },
      "environment": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}