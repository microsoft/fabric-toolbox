{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark",
      "jupyter_kernel_name": null
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d",
        "known_lakehouses": [
          {
            "id": "6cff634b-88f7-3505-bed2-c03a36776a8b"
          }
        ]
      },
      "environment": {}
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    }
  },
  "cells": [
    {
      "id": "ebd15efe-33fd-4c1b-992b-0e388a1d06a9",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Get Capacity Metrics (Timepoints)\n",
        "by Capacity by Day\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: #D69AFE;\">**MERGE**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_Capacity_Metrics_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Capacity Metrics** via SemPy DAX execute query function\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**1 Delta table** in FUAM_Lakehouse \n",
        "- **gold_table_name** variable value"
      ]
    },
    {
      "id": "d48117a6-b9da-4a3b-ad8c-bc33551dffc7",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "import sempy.fabric as fabric\n",
        "from datetime import datetime, timedelta\n",
        "import datetime as dt\n",
        "import pyspark.sql.functions as f\n",
        "from delta.tables import *"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "20c91122-24bc-4535-bbbc-da64ccecb2fb",
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "## Parameters\n",
        "# These parameters will be overwritten while executing the notebook \n",
        "# from Load_FUAM_Data_E2E Pipeline\n",
        "metric_days_in_scope = 3\n",
        "metric_workspace = \"71ff9c82-46d8-42e7-b602-7a08227faad3\"#\"d0182223-a6a9-4aa1-bf5c-24d8ee582b9d\"\n",
        "metric_dataset = \"87b11cea-fa44-4d0b-a9f8-a34dec691739\" #\"fe58232b-5b8a-4c8a-9c94-83db50937bd7\"\n",
        "display_data = True"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "38365016-9702-4964-9990-c03a7041997d",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "## Variables\n",
        "silver_table_name = \"FUAM_Staging_Lakehouse.capacity_metrics_by_timepoint_silver\"\n",
        "gold_table_name = \"capacity_metrics_by_timepoint\"\n",
        "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\"\n",
        "count_of_connectivity_errors = 0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "13cfcef0-a2bb-4183-b992-bba8d574b1e6",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "token = notebookutils.credentials.getToken(\"pbi\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bb983853-538c-45ef-88f6-318c7cb79b55",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Check Table Status\n",
        "version = ''\n",
        "\n",
        "try: \n",
        "    check_table_structure_query = \"\"\"DEFINE    MPARAMETER 'DefaultCapacityID' = \"0000000-0000-0000-0000-00000000\"\n",
        "                                    EVALUATE   SUMMARIZECOLUMNS(\"Background billable CU %\", [Background billable CU %]    )\"\"\"\n",
        "    check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)\n",
        "    print(\"INFO: Test for v47 successful\")\n",
        "    version = 'v47'\n",
        "except:\n",
        "    print(\"INFO: Test for v47 failed\")\n",
        "\n",
        "if version == '':\n",
        "    try:\n",
        "        check_table_structure_query = \"\"\"EVALUATE ROW(\"Background billable CU %\", 'All Measures'[Background billable CU %])\"\"\"\n",
        "        check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)\n",
        "        print(\"INFO: Test for v40 successful\")\n",
        "        version = 'v40'\n",
        "    except:\n",
        "        print(\"INFO: Test for v40 failed\")\n",
        "\n",
        "if version == '':\n",
        "    try:\n",
        "        check_table_structure_query_alternative = \"\"\"EVALUATE ROW(\"xBackground__\", 'All Measures'[xBackground %])\"\"\"\n",
        "        check_table_structure_df_alternative = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query_alternative)\n",
        "        version = 'v37'\n",
        "        print(\"INFO: Test for v37 successful\")\n",
        "    except:\n",
        "        print(\"INFO: Test for v37 failed\")\n",
        "\n",
        "\n",
        "# Validate version compatibility\n",
        "if version != '':\n",
        "    print( f'INFO: Version {version} is valid')\n",
        "else: \n",
        "    raise Exception(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "81b3fe74-3427-4f0a-92f5-a86d6da8e401",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Fetch capacities from connected capacity metrics app\n",
        "try:\n",
        "  if version in ['v47', 'v44', 'v40']:\n",
        "    capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities, \"capacity Id\", Capacities[capacity Id] , \"state\" , Capacities[state] )\"\"\"\n",
        "  else:\n",
        "    capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities, \"capacity Id\", Capacities[CapacityId] , \"state\" , Capacities[state] )\"\"\"\n",
        "  capacities = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=capacity_query)\n",
        "  capacities.columns = ['CapacityId', 'State']\n",
        "  capacities = spark.createDataFrame(capacities)\n",
        "except Exception as e:\n",
        "  notebookutils.notebook.exit(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b5f9126e-0a6f-40fa-9b03-25b960c9f30a",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "source": [
        "if display_data:\n",
        "    display(capacities)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e6bd8b63-1e2c-4987-a086-fdd2c584bfa3",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Iterate days\n",
        "def iterate_dates(start_date, end_date):\n",
        "    # Init array\n",
        "    dates = []\n",
        "    # Convert string inputs to datetime objects\n",
        "    start = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    \n",
        "    # Initialize current date as start date\n",
        "    current_date = start.date()\n",
        "    \n",
        "    while current_date <= end.date():\n",
        "\n",
        "        dates.append(\n",
        "            {\n",
        "                \"date\": current_date,\n",
        "                \"year\": current_date.year,\n",
        "                \"month\": current_date.month,\n",
        "                \"day\": current_date.day\n",
        "            })\n",
        "        # Move to the next day\n",
        "        current_date += dt.timedelta(days=1)\n",
        "\n",
        "    return dates\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0e8f6f88-b2c3-489d-a3f7-caa71afc0cfb",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Clean Silver table\n",
        "try:\n",
        "    query = \"DELETE FROM \" + silver_table_name\n",
        "    spark.sql(query)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(\"INFO: Silver table doesn't exist yet.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d82bf057-b326-4895-a28a-f2e84ffe472c",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false,
        "cellStatus": ""
      },
      "source": [
        "# Iterate capacities and days\n",
        "for cap in capacities.collect():\n",
        "    capacity_id = cap[0]\n",
        "    \n",
        "    print(f\"INFO: Scoped CapacityId: {capacity_id}\")\n",
        "\n",
        "    try:\n",
        "        # Get today's date\n",
        "        today = datetime.now()\n",
        "\n",
        "        # Calculate the dates between today and days_in_scope\n",
        "        days_ago = today - timedelta(days=metric_days_in_scope)\n",
        "\n",
        "        # Format dates in 'yyyy-mm-dd'\n",
        "        today_str = today.strftime('%Y-%m-%d')\n",
        "        days_ago_str = days_ago.strftime('%Y-%m-%d')\n",
        "\n",
        "        date_array = iterate_dates(days_ago_str, end_date=today_str)\n",
        "        print(f\"INFO: Get data for CapacityId: {capacity_id}\")\n",
        "\n",
        "        # Iterate days for current capacity\n",
        "        for date in date_array:\n",
        "            year = date['year']\n",
        "            month = date['month']\n",
        "            day = date['day']\n",
        "            date_label = str(year) + '-' + str(month) + '-' + str(day)\n",
        "\n",
        "            dax_query_v47 = f\"\"\"\n",
        "            DEFINE\n",
        "            \n",
        "             MPARAMETER 'CapacitiesList' = {{ \\\"{capacity_id}\\\" }}\n",
        "\n",
        "            VAR __DS0Core = \n",
        "                    SUMMARIZECOLUMNS(\n",
        "                        Capacities[capacity Id],\n",
        "                        'TimePoints'[TimePoint],\n",
        "                        FILTER(Capacities, Capacities[capacity Id] = \\\"{capacity_id}\\\" ),\n",
        "                        FILTER(TimePoints,  'TimePoints'[Date] = DATE({year}, {month}, {day})),\n",
        "                        \"B_P\", 'All Measures'[Background billable CU %],\n",
        "                        \"I_P\", 'All Measures'[Interactive billable CU %],\n",
        "                        \"B_NB_P\", 'All Measures'[Background non billable CU %],\n",
        "                        \"I_NB_P\", 'All Measures'[Interactive non billable CU %],\n",
        "                        \"AS_P\", 'All Measures'[SKU CU by timepoint %],\n",
        "                        \"CU_L\", 'All Measures'[CU limit],\n",
        "                        \"T_CU_U_P\", 'All Measures'[Cumulative CU usage % preview],\n",
        "                        \"C_CU_U_S\", 'All Measures'[Cumulative CU usage (s)],\n",
        "                        \"SKU_CU_TP\", 'All Measures'[SKU CU by timepoint],\n",
        "                        \"I_Del_P\", 'All Measures'[Dynamic interactive delay %],\n",
        "                        \"I_Rej_P\", 'All Measures'[Dynamic interactive rejection %],\n",
        "                        \"I_Rej_TH\", 'All Measures'[Interactive rejection threshold],\n",
        "                        \"B_Rej_P\", 'All Measures'[Dynamic background rejection %],\n",
        "                        \"B_Rej_TH\", 'All Measures'[Background rejection threshold],\n",
        "                        \"CO_A_P\", 'All Measures'[Carry over add %],\n",
        "                        \"CO_BD_P\", 'All Measures'[Carry over burndown %],\n",
        "                        \"CO_C_P\", 'All Measures'[Cumulative carry over %],\n",
        "                        \"OV_RL\", 'All Measures'[Overage reference line],\n",
        "                        \"Exp_BD_M\", 'All Measures'[Expected burndown in minutes]\n",
        "                    )\n",
        "\n",
        "            EVALUATE\n",
        "                __DS0Core\n",
        "            \"\"\"\n",
        "            \n",
        "            dax_query_v44 = f\"\"\"\n",
        "            DEFINE\n",
        "            \n",
        "             MPARAMETER 'CapacitiesList' = {{ \\\"{capacity_id}\\\" }}\n",
        "\n",
        "            VAR __DS0Core = \n",
        "                    SUMMARIZECOLUMNS(\n",
        "                        Capacities[capacity Id],\n",
        "                        'TimePoints'[TimePoint],\n",
        "                        FILTER(Capacities, Capacities[capacity Id] = \\\"{capacity_id}\\\" ),\n",
        "                        FILTER(TimePoints,  'TimePoints'[Date] = DATE({year}, {month}, {day})),\n",
        "                        \"B_P\", 'All Measures'[Background billable CU %],\n",
        "                        \"I_P\", 'All Measures'[Interactive billable CU %],\n",
        "                        \"B_NB_P\", 'All Measures'[Background non billable CU %],\n",
        "                        \"I_NB_P\", 'All Measures'[Interactive non billable CU %],\n",
        "                        \"AS_P\", 'All Measures'[SKU CU by timepoint %],\n",
        "                        \"CU_L\", 'All Measures'[CU limit],\n",
        "                        \"T_CU_U_P\", 'All Measures'[Cumulative CU usage % preview],\n",
        "                        \"C_CU_U_S\", 'All Measures'[Cumulative CU usage (s)],\n",
        "                        \"SKU_CU_TP\", 'All Measures'[SKU CU by timepoint],\n",
        "                        \"I_Del_P\", 'All Measures'[Dynamic interactive delay %],\n",
        "                        \"I_Rej_P\", 'All Measures'[Dynamic interactive rejection %],\n",
        "                        \"I_Rej_TH\", 'All Measures'[Interactive rejection threshold],\n",
        "                        \"B_Rej_P\", 'All Measures'[Dynamic background rejection %],\n",
        "                        \"B_Rej_TH\", 'All Measures'[Background rejection threshold],\n",
        "                        \"CO_A_P\", 'All Measures'[Carry over add %],\n",
        "                        \"CO_BD_P\", 'All Measures'[Carry over burndown %],\n",
        "                        \"CO_C_P\", 'All Measures'[Cumulative carry over %],\n",
        "                        \"OV_RL\", 'All Measures'[Overage reference line],\n",
        "                        \"Exp_BD_M\", 'All Measures'[Expected burndown in minutes]\n",
        "                    )\n",
        "\n",
        "            EVALUATE\n",
        "                __DS0Core\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "            dax_query_v40 = f\"\"\"\n",
        "            DEFINE\n",
        "            \n",
        "            MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
        "\n",
        "            VAR __DS0Core = \n",
        "                    SUMMARIZECOLUMNS(\n",
        "                        Capacities[capacity Id],\n",
        "                        'TimePoints'[TimePoint],\n",
        "                        FILTER(Capacities, Capacities[capacity Id] = \\\"{capacity_id}\\\" ),\n",
        "                        FILTER(TimePoints,  'TimePoints'[Date] = DATE({year}, {month}, {day})),\n",
        "                        \"B_P\", 'All Measures'[Background billable CU %],\n",
        "                        \"I_P\", 'All Measures'[Interactive billable CU %],\n",
        "                        \"B_NB_P\", 'All Measures'[Background non billable CU %],\n",
        "                        \"I_NB_P\", 'All Measures'[Interactive non billable CU %],\n",
        "                        \"AS_P\", 'All Measures'[SKU CU by timepoint %],\n",
        "                        \"CU_L\", 'All Measures'[CU limit],\n",
        "                        \"T_CU_U_P\", 'All Measures'[Cumulative CU usage % preview],\n",
        "                        \"C_CU_U_S\", 'All Measures'[Cumulative CU usage (s)],\n",
        "                        \"SKU_CU_TP\", 'All Measures'[SKU CU by timepoint],\n",
        "                        \"I_Del_P\", 'All Measures'[Dynamic interactive delay %],\n",
        "                        \"I_Rej_P\", 'All Measures'[Dynamic interactive rejection %],\n",
        "                        \"I_Rej_TH\", 'All Measures'[Interactive rejection threshold],\n",
        "                        \"B_Rej_P\", 'All Measures'[Dynamic background rejection %],\n",
        "                        \"B_Rej_TH\", 'All Measures'[Background rejection threshold],\n",
        "                        \"CO_A_P\", 'All Measures'[Carry over add %],\n",
        "                        \"CO_BD_P\", 'All Measures'[Carry over burndown %],\n",
        "                        \"CO_C_P\", 'All Measures'[Cumulative carry over %],\n",
        "                        \"OV_RL\", 'All Measures'[Overage reference line],\n",
        "                        \"Exp_BD_M\", 'All Measures'[Expected burndown in minutes]\n",
        "                    )\n",
        "\n",
        "            EVALUATE\n",
        "                __DS0Core\n",
        "            \"\"\"\n",
        "\n",
        "            dax_query_v37 = f\"\"\"\n",
        "            DEFINE\n",
        "\n",
        "            MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
        "\n",
        "            VAR __DS0Core = \n",
        "                    SUMMARIZECOLUMNS(\n",
        "                        Capacities[capacityId],\n",
        "                        'TimePoints'[TimePoint],\n",
        "                        FILTER(Capacities, Capacities[capacityId] = \\\"{capacity_id}\\\" ),\n",
        "                        FILTER(TimePoints,  'TimePoints'[Date] = DATE({year}, {month}, {day})),\n",
        "                        \"B_P\", 'All Measures'[xBackground %],\n",
        "                        \"I_P\", 'All Measures'[xInteractive %],\n",
        "                        \"B_NB_P\", 'All Measures'[xBackground % Preview],\n",
        "                        \"I_NB_P\", 'All Measures'[xInteractive % Preview],\n",
        "                        \"AS_P\", 'All Measures'[SKU CU by TimePoint %],\n",
        "                        \"CU_L\", 'All Measures'[CU Limit],\n",
        "                        \"T_CU_U_P\", 'All Measures'[Cumulative CU Usage % Preview],\n",
        "                        \"C_CU_U_S\", 'All Measures'[Cumulative CU Usage (s)],\n",
        "                        \"SKU_CU_TP\", 'All Measures'[SKU CU by TimePoint],\n",
        "                        \"I_Del_P\", 'All Measures'[Dynamic InteractiveDelay %],\n",
        "                        \"I_Rej_P\", 'All Measures'[Dynamic InteractiveRejection %],\n",
        "                        \"I_Rej_TH\", 'All Measures'[Interactive rejection threshold],\n",
        "                        \"B_Rej_P\", 'All Measures'[Dynamic BackgroundRejection %],\n",
        "                        \"B_Rej_TH\", 'All Measures'[Background rejection threshold],\n",
        "                        \"CO_A_P\", 'All Measures'[xCarryOver_added %],\n",
        "                        \"CO_BD_P\", 'All Measures'[xCarryOver_burndown %],\n",
        "                        \"CO_C_P\", 'All Measures'[xCarryOver_Cumulative %],\n",
        "                        \"OV_RL\", 'All Measures'[Overage reference line],\n",
        "                        \"Exp_BD_M\", 'All Measures'[Expected burndown in minutes]\n",
        "                    )\n",
        "            EVALUATE\n",
        "                __DS0Core\n",
        "            \"\"\"\n",
        "\n",
        "            dax_query = \"\"\n",
        "            # Choose query\n",
        "            if version == 'v47':\n",
        "                dax_query = dax_query_v47\n",
        "            elif version == 'v44':\n",
        "                dax_query = dax_query_v44\n",
        "            elif version == 'v40':\n",
        "                dax_query = dax_query_v40\n",
        "            elif version == 'v37':\n",
        "                dax_query = dax_query_v37\n",
        "                \n",
        "            # Execute DAX query\n",
        "            capacity_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=dax_query)\n",
        "            capacity_df.columns = ['CapacityId', 'TimePoint', 'BackgroundPercentage', 'InteractivePercentage', \n",
        "                                    'BackgroundNonBillablePercentage', 'InteractiveNonBillablePercentage', 'AutoscalePercentage', \n",
        "                                    'CULimitPercentage', 'TotalCUUsagePercentage', 'TotalCUs', 'SKUCUByTimePoint', \n",
        "                                    'InteractiveDelayPercentage', 'InteractiveRejectionPercentage', 'InteractiveRejectionThreshold', \n",
        "                                    'BackgroundRejectionPercentage', 'BackgroundRejectionThreshold', 'CarryOverAddedPercentage', \n",
        "                                    'CarryOverBurndownPercentage', 'CarryOverCumulativePercentage', 'OverageReferenceLine', \n",
        "                                    'ExpectedBurndownInMin']\n",
        "            \n",
        "            if not(capacity_df.empty):\n",
        "                # Transfer pandas df to spark df\n",
        "                capacity_df = spark.createDataFrame(capacity_df)\n",
        "\n",
        "                if display_data:\n",
        "                    display(capacity_df)\n",
        "\n",
        "                # Write prepared bronze_df to silver delta table\n",
        "                print(f\"INFO: Appending data for CapacityId: {capacity_id} on Date: {date_label}\")\n",
        "                capacity_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(silver_table_name)  \n",
        "            else:\n",
        "                print(f\"INFO: No data for CapacityId: {capacity_id} on Date: {date_label}\")\n",
        "\n",
        "    except Exception as ex:\n",
        "        count_of_connectivity_errors += 1\n",
        "        print(ex)\n",
        "        continue"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "813695ae-de38-443f-9e12-af278044357d",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Handle error if interface is not reachable\n",
        "if count_of_connectivity_errors > 0: \n",
        "    print(\"WARNING: Connection to at least one of the capacities is not possible. Please review the configurations.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9a5fe3c2-1fb5-44f0-9e11-bd5c60ed449c",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "try:\n",
        "    # Get Silver table data\n",
        "    query = \"SELECT * FROM  \" + silver_table_name\n",
        "    silver_df = spark.sql(query)\n",
        "\n",
        "except Exception as ex:\n",
        "    # Handle error if interface is not reachable\n",
        "    if count_of_connectivity_errors > 0: \n",
        "        notebookutils.notebook.exit(\"ERROR: Connection to capacity metrics is not possible. Please review the configurations.\")\n",
        "    else:\n",
        "        notebookutils.notebook.exit(\"ERROR: Silver table doesn't exist yet. This issue occurs also when the connection to capacity metrics is not configured correctly or endpoint is not reachable.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6dadaca7-5421-4f08-b81a-2961f669ef1b",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Check if gold table exists\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):\n",
        "    # if exists -> MERGE to gold\n",
        "    print(\"INFO: Gold table exists and will be merged.\")\n",
        "\n",
        "    gold_df = DeltaTable.forPath(spark, gold_table_name_with_prefix)\n",
        "    # Merge silver (s = source) to gold (t = target)\n",
        "    gold_df.alias('t') \\\n",
        "    .merge(\n",
        "        silver_df.alias('s'),\n",
        "        \"s.CapacityId = t.CapacityId AND s.TimePoint = t.TimePoint\"\n",
        "    ) \\\n",
        "    .whenMatchedUpdate(set =\n",
        "        {\n",
        "             \"BackgroundPercentage\": \"s.BackgroundPercentage\"\n",
        "            ,\"InteractivePercentage\": \"s.InteractivePercentage\"\n",
        "            ,\"BackgroundNonBillablePercentage\": \"s.BackgroundNonBillablePercentage\"\n",
        "            ,\"InteractiveNonBillablePercentage\": \"s.InteractiveNonBillablePercentage\"\n",
        "            ,\"AutoscalePercentage\": \"s.AutoscalePercentage\"\n",
        "            ,\"CULimitPercentage\": \"s.CULimitPercentage\"\n",
        "            ,\"TotalCUUsagePercentage\": \"s.TotalCUUsagePercentage\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"SKUCUByTimePoint\": \"s.SKUCUByTimePoint\"\n",
        "            ,\"InteractiveDelayPercentage\": \"s.InteractiveDelayPercentage\"\n",
        "            ,\"InteractiveRejectionPercentage\": \"s.InteractiveRejectionPercentage\"\n",
        "            ,\"InteractiveRejectionThreshold\": \"s.InteractiveRejectionThreshold\"\n",
        "            ,\"BackgroundRejectionPercentage\": \"s.BackgroundRejectionPercentage\"\n",
        "            ,\"BackgroundRejectionThreshold\": \"s.BackgroundRejectionThreshold\"\n",
        "            ,\"CarryOverAddedPercentage\": \"s.CarryOverAddedPercentage\"\n",
        "            ,\"CarryOverBurndownPercentage\": \"s.CarryOverBurndownPercentage\"\n",
        "            ,\"CarryOverCumulativePercentage\": \"s.CarryOverCumulativePercentage\"\n",
        "            ,\"OverageReferenceLine\": \"s.OverageReferenceLine\"\n",
        "            ,\"ExpectedBurndownInMin\": \"s.ExpectedBurndownInMin\"\n",
        "        }\n",
        "    ) \\\n",
        "    .whenNotMatchedInsert(values =\n",
        "        {\n",
        "             \"CapacityId\": \"s.CapacityId\"\n",
        "            ,\"TimePoint\": \"s.TimePoint\"\n",
        "            ,\"BackgroundPercentage\": \"s.BackgroundPercentage\"\n",
        "            ,\"InteractivePercentage\": \"s.InteractivePercentage\"\n",
        "            ,\"BackgroundNonBillablePercentage\": \"s.BackgroundNonBillablePercentage\"\n",
        "            ,\"InteractiveNonBillablePercentage\": \"s.InteractiveNonBillablePercentage\"\n",
        "            ,\"AutoscalePercentage\": \"s.AutoscalePercentage\"\n",
        "            ,\"CULimitPercentage\": \"s.CULimitPercentage\"\n",
        "            ,\"TotalCUUsagePercentage\": \"s.TotalCUUsagePercentage\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"SKUCUByTimePoint\": \"s.SKUCUByTimePoint\"\n",
        "            ,\"InteractiveDelayPercentage\": \"s.InteractiveDelayPercentage\"\n",
        "            ,\"InteractiveRejectionPercentage\": \"s.InteractiveRejectionPercentage\"\n",
        "            ,\"InteractiveRejectionThreshold\": \"s.InteractiveRejectionThreshold\"\n",
        "            ,\"BackgroundRejectionPercentage\": \"s.BackgroundRejectionPercentage\"\n",
        "            ,\"BackgroundRejectionThreshold\": \"s.BackgroundRejectionThreshold\"\n",
        "            ,\"CarryOverAddedPercentage\": \"s.CarryOverAddedPercentage\"\n",
        "            ,\"CarryOverBurndownPercentage\": \"s.CarryOverBurndownPercentage\"\n",
        "            ,\"CarryOverCumulativePercentage\": \"s.CarryOverCumulativePercentage\"\n",
        "            ,\"OverageReferenceLine\": \"s.OverageReferenceLine\"\n",
        "            ,\"ExpectedBurndownInMin\": \"s.ExpectedBurndownInMin\"\n",
        "        }\n",
        "    ) \\\n",
        "    .execute()\n",
        "\n",
        "else:\n",
        "    # else -> INSERT to gold\n",
        "    print(\"INFO: Gold table will be created.\")\n",
        "\n",
        "    silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d9516c7b-3d6d-4d11-9083-280400874f6a",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Clean Silver table\n",
        "query = \"DELETE FROM \" + silver_table_name\n",
        "spark.sql(query)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f1f504af-d926-4492-a294-1c81a8f81a2a",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Aggregate timepoints\n",
        "df = spark.sql(\"SELECT DISTINCT TimePoint, to_date(TimePoint) Date, to_timestamp(date_format(TimePoint, 'yyyy-mm-dd HH:00:00')) Hour  FROM FUAM_Lakehouse.capacity_metrics_by_timepoint\")\n",
        "\n",
        "# Write prepared bronze_df to silver delta table\n",
        "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"calendar_timepoints\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}