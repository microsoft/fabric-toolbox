{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "c345d33c-a9ab-4d3b-9314-5aa3769d4887",
            "metadata": {},
            "source": [
                "#### Get Capacity Metrics (Timepoints)\n",
                "by Capacity by Day\n",
                "\n",
                "##### Data ingestion strategy:\n",
                "<mark style=\"background: #D69AFE;\">**MERGE**</mark>\n",
                "\n",
                "##### Related pipeline:\n",
                "\n",
                "**Load_Capacity_Metrics_E2E**\n",
                "\n",
                "##### Source:\n",
                "\n",
                "**CapacityMetricCloneDQ** via SemPy DAX execute query function\n",
                "\n",
                "##### Target:\n",
                "\n",
                "**1 Delta table** in FUAM_Lakehouse \n",
                "- **gold_table_name** variable value"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8b874014-32b1-4964-a9d5-4d0f35188dce",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:23:52.2195334Z\",\"execution_finish_time\":\"2025-01-27T09:24:02.665541Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "import sempy.fabric as fabric\n",
                "from datetime import datetime, timedelta\n",
                "import datetime as dt\n",
                "import pyspark.sql.functions as f\n",
                "from delta.tables import *"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7c5eb517-8c4c-4e27-87bf-6e45e372107a",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:24:02.7959396Z\",\"execution_finish_time\":\"2025-01-27T09:24:03.105718Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "## Parameters\n",
                "metric_days_in_scope = 14\n",
                "metric_workspace = \"Microsoft Fabric Capacity Metrics v30\"\n",
                "metric_dataset = \"Fabric Capacity Metrics\"\n",
                "display_data = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7c2a3c7f-eeb7-4357-a26e-30a9cb2820cd",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:24:03.2416966Z\",\"execution_finish_time\":\"2025-01-27T09:24:03.4850333Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "## Variables\n",
                "silver_table_name = \"FUAM_Lakehouse.capacity_metrics_by_timepoint\"\n",
                "gold_table_name = \"capacity_metrics_by_timepoint\"\n",
                "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a10f4a01-8007-454b-b7af-4ed6d5eb6cd0",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:24:03.6228145Z\",\"execution_finish_time\":\"2025-01-27T09:24:34.2483374Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Fetch Capacities (without PPT capacities)\n",
                "query = \"\"\"\n",
                "SELECT \n",
                "    CapacityId\n",
                "FROM FUAM_Lakehouse.capacities\n",
                "WHERE SKU != 'PP3'\n",
                "AND state = 'Active'\n",
                "\"\"\"\n",
                "capacities = spark.sql(query)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "935b4179-16dd-4015-b799-f3fa497305b9",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:24:34.37647Z\",\"execution_finish_time\":\"2025-01-27T09:24:34.6484664Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "collapsed": false,
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "if display_data:\n",
                "    display(capacities)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "94515047-42cc-4ec0-80a4-8c1ae8ba570e",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:24:34.7734932Z\",\"execution_finish_time\":\"2025-01-27T09:24:35.0146025Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Iterate days\n",
                "def iterate_dates(start_date, end_date):\n",
                "    # Init array\n",
                "    dates = []\n",
                "    # Convert string inputs to datetime objects\n",
                "    start = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
                "    end = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
                "    \n",
                "    # Initialize current date as start date\n",
                "    current_date = start.date()\n",
                "    \n",
                "    while current_date <= end.date():\n",
                "\n",
                "        dates.append(\n",
                "            {\n",
                "                \"date\": current_date,\n",
                "                \"year\": current_date.year,\n",
                "                \"month\": current_date.month,\n",
                "                \"day\": current_date.day\n",
                "            })\n",
                "        # Move to the next day\n",
                "        current_date += dt.timedelta(days=1)\n",
                "\n",
                "    return dates\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ea7eb33e-d844-4320-a860-0914b192b60c",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:24:35.1536555Z\",\"execution_finish_time\":\"2025-01-27T09:24:48.3217504Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Clean Silver table\n",
                "try:\n",
                "    query = \"DELETE FROM \" + silver_table_name\n",
                "    spark.sql(query)\n",
                "\n",
                "except Exception as ex:\n",
                "    print(\"Silver table doesn't exist yet.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cfd9d44f-c8ec-4858-a6f2-d2bff9929d17",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:24:48.4605515Z\",\"execution_finish_time\":\"2025-01-27T09:32:59.7014751Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "collapsed": false,
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Iterate capacities and days\n",
                "for cap in capacities.collect():\n",
                "    capacity_id = cap[0]\n",
                "    print(capacity_id)\n",
                "\n",
                "    try:\n",
                "        # Get today's date\n",
                "        today = datetime.now()\n",
                "\n",
                "        # Calculate the dates between today and days_in_scope\n",
                "        days_ago = today - timedelta(days=metric_days_in_scope)\n",
                "\n",
                "        # Format dates in 'yyyy-mm-dd'\n",
                "        today_str = today.strftime('%Y-%m-%d')\n",
                "        days_ago_str = days_ago.strftime('%Y-%m-%d')\n",
                "\n",
                "        date_array = iterate_dates(days_ago_str, end_date=today_str)\n",
                "\n",
                "        # Iterate days for current capacity\n",
                "        for date in date_array:\n",
                "            year = date['year']\n",
                "            month = date['month']\n",
                "            day = date['day']\n",
                "            date_label = str(year) + '-' + str(month) + '-' + str(day)\n",
                "            print(capacity_id)\n",
                "\n",
                "            dax_query = f\"\"\"\n",
                "            DEFINE\n",
                "                MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
                "\n",
                "                VAR __DS0FilterTable = \n",
                "                    FILTER(\n",
                "                        KEEPFILTERS(VALUES('TimePoints'[Date])),\n",
                "                        'TimePoints'[Date] = DATE({year}, {month}, {day})\n",
                "                    )\n",
                "\n",
                "                VAR __DS0Core = \n",
                "                    SUMMARIZECOLUMNS(\n",
                "                        Capacities[capacityId],\n",
                "                        'TimePoints'[TimePoint],\n",
                "                        FILTER(Capacities, Capacities[capacityId] = \\\"{capacity_id}\\\" ),\n",
                "                        __DS0FilterTable,\n",
                "                        \"B_P\", 'All Measures'[xBackground %],\n",
                "                        \"I_P\", 'All Measures'[xInteractive %],\n",
                "                        \"B_NB_P\", 'All Measures'[xBackground % Preview],\n",
                "                        \"I_NB_P\", 'All Measures'[xInteractive % Preview],\n",
                "                        \"AS_P\", 'All Measures'[SKU CU by TimePoint %],\n",
                "                        \"CU_L\", 'All Measures'[CU Limit],\n",
                "                        \"T_CU_U_P\", 'All Measures'[Cumulative CU Usage % Preview],\n",
                "                        \"C_CU_U_S\", 'All Measures'[Cumulative CU Usage (s)],\n",
                "                        \"SKU_CU_TP\", 'All Measures'[SKU CU by TimePoint],\n",
                "                        \"I_Del_P\", 'All Measures'[Dynamic InteractiveDelay %],\n",
                "                        \"I_Rej_P\", 'All Measures'[Dynamic InteractiveRejection %],\n",
                "                        \"I_Rej_TH\", 'All Measures'[Interactive rejection threshold],\n",
                "                        \"B_Rej_P\", 'All Measures'[Dynamic BackgroundRejection %],\n",
                "                        \"B_Rej_TH\", 'All Measures'[Background rejection threshold],\n",
                "                        \"CO_A_P\", 'All Measures'[xCarryOver_added %],\n",
                "                        \"CO_BD_P\", 'All Measures'[xCarryOver_burndown %],\n",
                "                        \"CO_C_P\", 'All Measures'[xCarryOver_Cumulative %],\n",
                "                        \"OV_RL\", 'All Measures'[Overage reference line],\n",
                "                        \"Exp_BD_M\", 'All Measures'[Expected burndown in minutes]\n",
                "                    )\n",
                "\n",
                "            EVALUATE\n",
                "                __DS0Core\n",
                "            \"\"\"\n",
                "\n",
                "            \n",
                "\n",
                "\n",
                "            # Execute DAX query\n",
                "            capacity_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=dax_query)\n",
                "            capacity_df.columns = ['CapacityId', 'TimePoint', 'BackgroundPercentage', 'InteractivePercentage', \n",
                "                                    'BackgroundNonBillablePercentage', 'InteractiveNonBillablePercentage', 'AutoscalePercentage', \n",
                "                                    'CULimitPercentage', 'TotalCUUsagePercentage', 'TotalCUs', 'SKUCUByTimePoint', \n",
                "                                    'InteractiveDelayPercentage', 'InteractiveRejectionPercentage', 'InteractiveRejectionThreshold', \n",
                "                                    'BackgroundRejectionPercentage', 'BackgroundRejectionThreshold', 'CarryOverAddedPercentage', \n",
                "                                    'CarryOverBurndownPercentage', 'CarryOverCumulativePercentage', 'OverageReferenceLine', \n",
                "                                    'ExpectedBurndownInMin']\n",
                "            \n",
                "            if not(capacity_df.empty):\n",
                "                # Transfer pandas df to spark df\n",
                "                capacity_df = spark.createDataFrame(capacity_df)\n",
                "\n",
                "                if display_data:\n",
                "                    display(capacity_df)\n",
                "\n",
                "                # Write prepared bronze_df to silver delta table\n",
                "                print(f\"Appending data. Capacity: {capacity_id}. Date: {date_label}\")\n",
                "                capacity_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(silver_table_name)  \n",
                "            else:\n",
                "                print(f\"No data. Capacity: {capacity_id}. Date: {date_label}\")\n",
                "\n",
                "    except Exception as ex:\n",
                "        print(ex)\n",
                "        continue"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aded7b7b-c46d-4f09-ac9a-5a33c73bdf25",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:32:59.8463139Z\",\"execution_finish_time\":\"2025-01-27T09:33:03.3750475Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Get Silver table data\n",
                "query = \"SELECT * FROM  \" + silver_table_name\n",
                "silver_df = spark.sql(query)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "435bc68f-09ba-4972-89b0-6a57f3fae9a6",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:33:03.5019174Z\",\"execution_finish_time\":\"2025-01-27T09:33:18.3519391Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Check if gold table exists\n",
                "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):\n",
                "    # if exists -> MERGE to gold\n",
                "    print(\"Gold table exists and will be merged.\")\n",
                "\n",
                "    gold_df = DeltaTable.forPath(spark, gold_table_name_with_prefix)\n",
                "    # Merge silver (s = source) to gold (t = target)\n",
                "    gold_df.alias('t') \\\n",
                "    .merge(\n",
                "        silver_df.alias('s'),\n",
                "        \"s.CapacityId = t.CapacityId AND s.TimePoint = t.TimePoint\"\n",
                "    ) \\\n",
                "    .whenMatchedUpdate(set =\n",
                "        {\n",
                "             \"BackgroundPercentage\": \"s.BackgroundPercentage\"\n",
                "            ,\"InteractivePercentage\": \"s.InteractivePercentage\"\n",
                "            ,\"BackgroundNonBillablePercentage\": \"s.BackgroundNonBillablePercentage\"\n",
                "            ,\"InteractiveNonBillablePercentage\": \"s.InteractiveNonBillablePercentage\"\n",
                "            ,\"AutoscalePercentage\": \"s.AutoscalePercentage\"\n",
                "            ,\"CULimitPercentage\": \"s.CULimitPercentage\"\n",
                "            ,\"TotalCUUsagePercentage\": \"s.TotalCUUsagePercentage\"\n",
                "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
                "            ,\"SKUCUByTimePoint\": \"s.SKUCUByTimePoint\"\n",
                "            ,\"InteractiveDelayPercentage\": \"s.InteractiveDelayPercentage\"\n",
                "            ,\"InteractiveRejectionPercentage\": \"s.InteractiveRejectionPercentage\"\n",
                "            ,\"InteractiveRejectionThreshold\": \"s.InteractiveRejectionThreshold\"\n",
                "            ,\"BackgroundRejectionPercentage\": \"s.BackgroundRejectionPercentage\"\n",
                "            ,\"BackgroundRejectionThreshold\": \"s.BackgroundRejectionThreshold\"\n",
                "            ,\"CarryOverAddedPercentage\": \"s.CarryOverAddedPercentage\"\n",
                "            ,\"CarryOverBurndownPercentage\": \"s.CarryOverBurndownPercentage\"\n",
                "            ,\"CarryOverCumulativePercentage\": \"s.CarryOverCumulativePercentage\"\n",
                "            ,\"OverageReferenceLine\": \"s.OverageReferenceLine\"\n",
                "            ,\"ExpectedBurndownInMin\": \"s.ExpectedBurndownInMin\"\n",
                "        }\n",
                "    ) \\\n",
                "    .whenNotMatchedInsert(values =\n",
                "        {\n",
                "             \"CapacityId\": \"s.CapacityId\"\n",
                "            ,\"TimePoint\": \"s.TimePoint\"\n",
                "            ,\"BackgroundPercentage\": \"s.BackgroundPercentage\"\n",
                "            ,\"InteractivePercentage\": \"s.InteractivePercentage\"\n",
                "            ,\"BackgroundNonBillablePercentage\": \"s.BackgroundNonBillablePercentage\"\n",
                "            ,\"InteractiveNonBillablePercentage\": \"s.InteractiveNonBillablePercentage\"\n",
                "            ,\"AutoscalePercentage\": \"s.AutoscalePercentage\"\n",
                "            ,\"CULimitPercentage\": \"s.CULimitPercentage\"\n",
                "            ,\"TotalCUUsagePercentage\": \"s.TotalCUUsagePercentage\"\n",
                "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
                "            ,\"SKUCUByTimePoint\": \"s.SKUCUByTimePoint\"\n",
                "            ,\"InteractiveDelayPercentage\": \"s.InteractiveDelayPercentage\"\n",
                "            ,\"InteractiveRejectionPercentage\": \"s.InteractiveRejectionPercentage\"\n",
                "            ,\"InteractiveRejectionThreshold\": \"s.InteractiveRejectionThreshold\"\n",
                "            ,\"BackgroundRejectionPercentage\": \"s.BackgroundRejectionPercentage\"\n",
                "            ,\"BackgroundRejectionThreshold\": \"s.BackgroundRejectionThreshold\"\n",
                "            ,\"CarryOverAddedPercentage\": \"s.CarryOverAddedPercentage\"\n",
                "            ,\"CarryOverBurndownPercentage\": \"s.CarryOverBurndownPercentage\"\n",
                "            ,\"CarryOverCumulativePercentage\": \"s.CarryOverCumulativePercentage\"\n",
                "            ,\"OverageReferenceLine\": \"s.OverageReferenceLine\"\n",
                "            ,\"ExpectedBurndownInMin\": \"s.ExpectedBurndownInMin\"\n",
                "        }\n",
                "    ) \\\n",
                "    .execute()\n",
                "\n",
                "else:\n",
                "    # else -> INSERT to gold\n",
                "    print(\"Gold table will be created.\")\n",
                "\n",
                "    silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "83464f97-b99a-49e3-985d-3ef28016030c",
            "metadata": {
                "cellStatus": "{\"System Administrator\":{\"session_start_time\":null,\"execution_start_time\":\"2025-01-27T09:33:18.5548046Z\",\"execution_finish_time\":\"2025-01-27T09:33:20.1308579Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Clean Silver table\n",
                "query = \"DELETE FROM \" + silver_table_name\n",
                "spark.sql(query)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2da7008d-976e-4186-b855-b59c2fc63a55",
            "metadata": {
                "cellStatus": "{}",
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Aggregate timepoints\n",
                "df = spark.sql(\"SELECT DISTINCT TimePoint, to_date(TimePoint) Date, to_timestamp(date_format(TimePoint, 'yyyy-mm-dd HH:00:00')) Hour  FROM FUAM_Lakehouse.capacity_metrics_by_timepoint\")\n",
                "\n",
                "# Write prepared bronze_df to silver delta table\n",
                "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"calendar_timepoints\")"
            ]
        }
    ],
    "metadata": {
        "a365ComputeOptions": null,
        "dependencies": {
            "environment": {},
            "lakehouse": {
                "default_lakehouse": "729eb8a2-8070-5ed8-ad43-dccbc00b32af",
                "default_lakehouse_name": "FUAM_Lakehouse",
                "default_lakehouse_workspace_id": "eb764c8b-cf3b-55be-adf4-348fe9233657",
                "known_lakehouses": []
            }
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Synapse PySpark",
            "language": "Python",
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "sessionKeepAliveTimeout": 0,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "widgets": {}
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
