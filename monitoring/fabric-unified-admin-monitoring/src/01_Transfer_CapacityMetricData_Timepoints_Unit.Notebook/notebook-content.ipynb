{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Capacity Metrics (Timepoints)\n",
        "by Capacity by Day\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: #D69AFE;\">**MERGE**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_Capacity_Metrics_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Capacity Metrics** via SemPy DAX execute query function\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**1 Delta table** in FUAM_Lakehouse \n",
        "- **gold_table_name** variable value"
      ],
      "metadata": {},
      "id": "c345d33c-a9ab-4d3b-9314-5aa3769d4887"
    },
    {
      "cell_type": "code",
      "source": [
        "import sempy.fabric as fabric\n",
        "from datetime import datetime, timedelta\n",
        "import datetime as dt\n",
        "import pyspark.sql.functions as f\n",
        "from delta.tables import *"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "8b874014-32b1-4964-a9d5-4d0f35188dce"
    },
    {
      "cell_type": "code",
      "source": [
        "## Parameters\n",
        "# These parameters will be overwritten while executing the notebook \n",
        "# from Load_FUAM_Data_E2E Pipeline\n",
        "metric_days_in_scope = 2\n",
        "metric_workspace = \"0865c010-c5ba-4279-b2fc-51d85a369983\"\n",
        "metric_dataset = \"Fabric Capacity Metrics\"\n",
        "display_data = False"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "tags": [
          "parameters"
        ]
      },
      "id": "7c5eb517-8c4c-4e27-87bf-6e45e372107a"
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables\n",
        "silver_table_name = \"FUAM_Staging_Lakehouse.capacity_metrics_by_timepoint_silver\"\n",
        "gold_table_name = \"capacity_metrics_by_timepoint\"\n",
        "gold_table_name_with_prefix = f\"Tables/{gold_table_name}\""
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "7c2a3c7f-eeb7-4357-a26e-30a9cb2820cd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Table Status\n",
        "\n",
        "# Go with primary\n",
        "primary_version_active = True\n",
        "try:\n",
        "    check_table_structure_query = \"\"\"EVALUATE ROW(\"Background billable CU %\", 'All Measures'[Background billable CU %])\"\"\"\n",
        "    check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)\n",
        "except:\n",
        "    primary_version_active = False\n",
        "\n",
        "# Try secondary\n",
        "if primary_version_active == False:\n",
        "    try:\n",
        "        check_table_structure_query_alternative = \"\"\"EVALUATE ROW(\"xBackground__\", 'All Measures'[xBackground %])\"\"\"\n",
        "        check_table_structure_df_alternative = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query_alternative)\n",
        "    except:\n",
        "        primary_version_active = None\n",
        "        print(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")\n",
        "        exit"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "85afb22f-2cd7-4601-8b53-f3820304bf34"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    if primary_version_active == True:\n",
        "        print(\"INFO: Primary version is compatible.\")\n",
        "    elif primary_version_active == False:\n",
        "        print(\"INFO: Secondary version is compatible.\")\n",
        "    else:\n",
        "        print(\"ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.\")"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "706842f2-f526-404a-8af8-2143d0a0f8a2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch capacities from connected capacity metrics app\n",
        "if primary_version_active == True:\n",
        "  capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities, \"capacity Id\", Capacities[capacity Id] , \"state\" , Capacities[state] )\"\"\"\n",
        "else:\n",
        "  capacity_query = \"\"\"EVALUATE SELECTCOLUMNS (    Capacities, \"capacity Id\", Capacities[CapacityId] , \"state\" , Capacities[state] )\"\"\"\n",
        "capacities = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=capacity_query)\n",
        "capacities.columns = ['CapacityId', 'State']\n",
        "capacities = spark.createDataFrame(capacities)"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "id": "c2692fbb-3cf8-4134-8434-d8847040e163"
    },
    {
      "cell_type": "code",
      "source": [
        "if display_data:\n",
        "    display(capacities)"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "cellStatus": "",
        "collapsed": false,
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "935b4179-16dd-4015-b799-f3fa497305b9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate days\n",
        "def iterate_dates(start_date, end_date):\n",
        "    # Init array\n",
        "    dates = []\n",
        "    # Convert string inputs to datetime objects\n",
        "    start = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    \n",
        "    # Initialize current date as start date\n",
        "    current_date = start.date()\n",
        "    \n",
        "    while current_date <= end.date():\n",
        "\n",
        "        dates.append(\n",
        "            {\n",
        "                \"date\": current_date,\n",
        "                \"year\": current_date.year,\n",
        "                \"month\": current_date.month,\n",
        "                \"day\": current_date.day\n",
        "            })\n",
        "        # Move to the next day\n",
        "        current_date += dt.timedelta(days=1)\n",
        "\n",
        "    return dates\n"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "94515047-42cc-4ec0-80a4-8c1ae8ba570e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Silver table\n",
        "try:\n",
        "    query = \"DELETE FROM \" + silver_table_name\n",
        "    spark.sql(query)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(\"INFO: Silver table doesn't exist yet.\")"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "ea7eb33e-d844-4320-a860-0914b192b60c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate capacities and days\n",
        "for cap in capacities.collect():\n",
        "    capacity_id = cap[0]\n",
        "    \n",
        "    print(f\"INFO: Scoped CapacityId: {capacity_id}\")\n",
        "\n",
        "    try:\n",
        "        # Get today's date\n",
        "        today = datetime.now()\n",
        "\n",
        "        # Calculate the dates between today and days_in_scope\n",
        "        days_ago = today - timedelta(days=metric_days_in_scope)\n",
        "\n",
        "        # Format dates in 'yyyy-mm-dd'\n",
        "        today_str = today.strftime('%Y-%m-%d')\n",
        "        days_ago_str = days_ago.strftime('%Y-%m-%d')\n",
        "\n",
        "        date_array = iterate_dates(days_ago_str, end_date=today_str)\n",
        "        print(f\"INFO: Get data for CapacityId: {capacity_id}\")\n",
        "\n",
        "        # Iterate days for current capacity\n",
        "        for date in date_array:\n",
        "            year = date['year']\n",
        "            month = date['month']\n",
        "            day = date['day']\n",
        "            date_label = str(year) + '-' + str(month) + '-' + str(day)\n",
        "            \n",
        "\n",
        "            dax_query_primary = f\"\"\"\n",
        "            DEFINE\n",
        "            \n",
        "            MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
        "\n",
        "            VAR __DS0Core = \n",
        "                    SUMMARIZECOLUMNS(\n",
        "                        Capacities[capacity Id],\n",
        "                        'TimePoints'[TimePoint],\n",
        "                        FILTER(Capacities, Capacities[capacity Id] = \\\"{capacity_id}\\\" ),\n",
        "                        FILTER(TimePoints,  'TimePoints'[Date] = DATE({year}, {month}, {day})),\n",
        "                        \"B_P\", 'All Measures'[Background billable CU %],\n",
        "                        \"I_P\", 'All Measures'[Interactive billable CU %],\n",
        "                        \"B_NB_P\", 'All Measures'[Background non billable CU %],\n",
        "                        \"I_NB_P\", 'All Measures'[Interactive non billable CU %],\n",
        "                        \"AS_P\", 'All Measures'[SKU CU by timepoint %],\n",
        "                        \"CU_L\", 'All Measures'[CU limit],\n",
        "                        \"T_CU_U_P\", 'All Measures'[Cumulative CU usage % preview],\n",
        "                        \"C_CU_U_S\", 'All Measures'[Cumulative CU usage (s)],\n",
        "                        \"SKU_CU_TP\", 'All Measures'[SKU CU by timepoint],\n",
        "                        \"I_Del_P\", 'All Measures'[Dynamic interactive delay %],\n",
        "                        \"I_Rej_P\", 'All Measures'[Dynamic interactive rejection %],\n",
        "                        \"I_Rej_TH\", 'All Measures'[Interactive rejection threshold],\n",
        "                        \"B_Rej_P\", 'All Measures'[Dynamic background rejection %],\n",
        "                        \"B_Rej_TH\", 'All Measures'[Background rejection threshold],\n",
        "                        \"CO_A_P\", 'All Measures'[Carry over add %],\n",
        "                        \"CO_BD_P\", 'All Measures'[Carry over burndown %],\n",
        "                        \"CO_C_P\", 'All Measures'[Cumulative carry over %],\n",
        "                        \"OV_RL\", 'All Measures'[Overage reference line],\n",
        "                        \"Exp_BD_M\", 'All Measures'[Expected burndown in minutes]\n",
        "                    )\n",
        "\n",
        "            EVALUATE\n",
        "                __DS0Core\n",
        "            \"\"\"\n",
        "\n",
        "            dax_query_alternative = f\"\"\"\n",
        "            DEFINE\n",
        "\n",
        "            MPARAMETER 'CapacityID' = \\\"{capacity_id}\\\"\n",
        "\n",
        "            VAR __DS0Core = \n",
        "                    SUMMARIZECOLUMNS(\n",
        "                        Capacities[capacityId],\n",
        "                        'TimePoints'[TimePoint],\n",
        "                        FILTER(Capacities, Capacities[capacityId] = \\\"{capacity_id}\\\" ),\n",
        "                        FILTER(TimePoints,  'TimePoints'[Date] = DATE({year}, {month}, {day})),\n",
        "                        \"B_P\", 'All Measures'[xBackground %],\n",
        "                        \"I_P\", 'All Measures'[xInteractive %],\n",
        "                        \"B_NB_P\", 'All Measures'[xBackground % Preview],\n",
        "                        \"I_NB_P\", 'All Measures'[xInteractive % Preview],\n",
        "                        \"AS_P\", 'All Measures'[SKU CU by TimePoint %],\n",
        "                        \"CU_L\", 'All Measures'[CU Limit],\n",
        "                        \"T_CU_U_P\", 'All Measures'[Cumulative CU Usage % Preview],\n",
        "                        \"C_CU_U_S\", 'All Measures'[Cumulative CU Usage (s)],\n",
        "                        \"SKU_CU_TP\", 'All Measures'[SKU CU by TimePoint],\n",
        "                        \"I_Del_P\", 'All Measures'[Dynamic InteractiveDelay %],\n",
        "                        \"I_Rej_P\", 'All Measures'[Dynamic InteractiveRejection %],\n",
        "                        \"I_Rej_TH\", 'All Measures'[Interactive rejection threshold],\n",
        "                        \"B_Rej_P\", 'All Measures'[Dynamic BackgroundRejection %],\n",
        "                        \"B_Rej_TH\", 'All Measures'[Background rejection threshold],\n",
        "                        \"CO_A_P\", 'All Measures'[xCarryOver_added %],\n",
        "                        \"CO_BD_P\", 'All Measures'[xCarryOver_burndown %],\n",
        "                        \"CO_C_P\", 'All Measures'[xCarryOver_Cumulative %],\n",
        "                        \"OV_RL\", 'All Measures'[Overage reference line],\n",
        "                        \"Exp_BD_M\", 'All Measures'[Expected burndown in minutes]\n",
        "                    )\n",
        "            EVALUATE\n",
        "                __DS0Core\n",
        "            \"\"\"\n",
        "\n",
        "            dax_query = \"\"\n",
        "            # Choose query\n",
        "            if primary_version_active == True:\n",
        "                dax_query = dax_query_primary\n",
        "            else:\n",
        "                dax_query = dax_query_alternative\n",
        "                \n",
        "            # Execute DAX query\n",
        "            capacity_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=dax_query)\n",
        "            capacity_df.columns = ['CapacityId', 'TimePoint', 'BackgroundPercentage', 'InteractivePercentage', \n",
        "                                    'BackgroundNonBillablePercentage', 'InteractiveNonBillablePercentage', 'AutoscalePercentage', \n",
        "                                    'CULimitPercentage', 'TotalCUUsagePercentage', 'TotalCUs', 'SKUCUByTimePoint', \n",
        "                                    'InteractiveDelayPercentage', 'InteractiveRejectionPercentage', 'InteractiveRejectionThreshold', \n",
        "                                    'BackgroundRejectionPercentage', 'BackgroundRejectionThreshold', 'CarryOverAddedPercentage', \n",
        "                                    'CarryOverBurndownPercentage', 'CarryOverCumulativePercentage', 'OverageReferenceLine', \n",
        "                                    'ExpectedBurndownInMin']\n",
        "            \n",
        "            if not(capacity_df.empty):\n",
        "                # Transfer pandas df to spark df\n",
        "                capacity_df = spark.createDataFrame(capacity_df)\n",
        "\n",
        "                if display_data:\n",
        "                    display(capacity_df)\n",
        "\n",
        "                # Write prepared bronze_df to silver delta table\n",
        "                print(f\"INFO: Appending data for CapacityId: {capacity_id} on Date: {date_label}\")\n",
        "                capacity_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(silver_table_name)  \n",
        "            else:\n",
        "                print(f\"INFO: No data for CapacityId: {capacity_id} on Date: {date_label}\")\n",
        "\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "        continue"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "cellStatus": "",
        "collapsed": false,
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "cfd9d44f-c8ec-4858-a6f2-d2bff9929d17"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Silver table data\n",
        "query = \"SELECT * FROM  \" + silver_table_name\n",
        "silver_df = spark.sql(query)"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "aded7b7b-c46d-4f09-ac9a-5a33c73bdf25"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if gold table exists\n",
        "if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):\n",
        "    # if exists -> MERGE to gold\n",
        "    print(\"INFO: Gold table exists and will be merged.\")\n",
        "\n",
        "    gold_df = DeltaTable.forPath(spark, gold_table_name_with_prefix)\n",
        "    # Merge silver (s = source) to gold (t = target)\n",
        "    gold_df.alias('t') \\\n",
        "    .merge(\n",
        "        silver_df.alias('s'),\n",
        "        \"s.CapacityId = t.CapacityId AND s.TimePoint = t.TimePoint\"\n",
        "    ) \\\n",
        "    .whenMatchedUpdate(set =\n",
        "        {\n",
        "             \"BackgroundPercentage\": \"s.BackgroundPercentage\"\n",
        "            ,\"InteractivePercentage\": \"s.InteractivePercentage\"\n",
        "            ,\"BackgroundNonBillablePercentage\": \"s.BackgroundNonBillablePercentage\"\n",
        "            ,\"InteractiveNonBillablePercentage\": \"s.InteractiveNonBillablePercentage\"\n",
        "            ,\"AutoscalePercentage\": \"s.AutoscalePercentage\"\n",
        "            ,\"CULimitPercentage\": \"s.CULimitPercentage\"\n",
        "            ,\"TotalCUUsagePercentage\": \"s.TotalCUUsagePercentage\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"SKUCUByTimePoint\": \"s.SKUCUByTimePoint\"\n",
        "            ,\"InteractiveDelayPercentage\": \"s.InteractiveDelayPercentage\"\n",
        "            ,\"InteractiveRejectionPercentage\": \"s.InteractiveRejectionPercentage\"\n",
        "            ,\"InteractiveRejectionThreshold\": \"s.InteractiveRejectionThreshold\"\n",
        "            ,\"BackgroundRejectionPercentage\": \"s.BackgroundRejectionPercentage\"\n",
        "            ,\"BackgroundRejectionThreshold\": \"s.BackgroundRejectionThreshold\"\n",
        "            ,\"CarryOverAddedPercentage\": \"s.CarryOverAddedPercentage\"\n",
        "            ,\"CarryOverBurndownPercentage\": \"s.CarryOverBurndownPercentage\"\n",
        "            ,\"CarryOverCumulativePercentage\": \"s.CarryOverCumulativePercentage\"\n",
        "            ,\"OverageReferenceLine\": \"s.OverageReferenceLine\"\n",
        "            ,\"ExpectedBurndownInMin\": \"s.ExpectedBurndownInMin\"\n",
        "        }\n",
        "    ) \\\n",
        "    .whenNotMatchedInsert(values =\n",
        "        {\n",
        "             \"CapacityId\": \"s.CapacityId\"\n",
        "            ,\"TimePoint\": \"s.TimePoint\"\n",
        "            ,\"BackgroundPercentage\": \"s.BackgroundPercentage\"\n",
        "            ,\"InteractivePercentage\": \"s.InteractivePercentage\"\n",
        "            ,\"BackgroundNonBillablePercentage\": \"s.BackgroundNonBillablePercentage\"\n",
        "            ,\"InteractiveNonBillablePercentage\": \"s.InteractiveNonBillablePercentage\"\n",
        "            ,\"AutoscalePercentage\": \"s.AutoscalePercentage\"\n",
        "            ,\"CULimitPercentage\": \"s.CULimitPercentage\"\n",
        "            ,\"TotalCUUsagePercentage\": \"s.TotalCUUsagePercentage\"\n",
        "            ,\"TotalCUs\": \"s.TotalCUs\"\n",
        "            ,\"SKUCUByTimePoint\": \"s.SKUCUByTimePoint\"\n",
        "            ,\"InteractiveDelayPercentage\": \"s.InteractiveDelayPercentage\"\n",
        "            ,\"InteractiveRejectionPercentage\": \"s.InteractiveRejectionPercentage\"\n",
        "            ,\"InteractiveRejectionThreshold\": \"s.InteractiveRejectionThreshold\"\n",
        "            ,\"BackgroundRejectionPercentage\": \"s.BackgroundRejectionPercentage\"\n",
        "            ,\"BackgroundRejectionThreshold\": \"s.BackgroundRejectionThreshold\"\n",
        "            ,\"CarryOverAddedPercentage\": \"s.CarryOverAddedPercentage\"\n",
        "            ,\"CarryOverBurndownPercentage\": \"s.CarryOverBurndownPercentage\"\n",
        "            ,\"CarryOverCumulativePercentage\": \"s.CarryOverCumulativePercentage\"\n",
        "            ,\"OverageReferenceLine\": \"s.OverageReferenceLine\"\n",
        "            ,\"ExpectedBurndownInMin\": \"s.ExpectedBurndownInMin\"\n",
        "        }\n",
        "    ) \\\n",
        "    .execute()\n",
        "\n",
        "else:\n",
        "    # else -> INSERT to gold\n",
        "    print(\"INFO: Gold table will be created.\")\n",
        "\n",
        "    silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(gold_table_name)"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "435bc68f-09ba-4972-89b0-6a57f3fae9a6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Silver table\n",
        "query = \"DELETE FROM \" + silver_table_name\n",
        "spark.sql(query)"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "83464f97-b99a-49e3-985d-3ef28016030c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate timepoints\n",
        "df = spark.sql(\"SELECT DISTINCT TimePoint, to_date(TimePoint) Date, to_timestamp(date_format(TimePoint, 'yyyy-mm-dd HH:00:00')) Hour  FROM FUAM_Lakehouse.capacity_metrics_by_timepoint\")\n",
        "\n",
        "# Write prepared bronze_df to silver delta table\n",
        "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"calendar_timepoints\")"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "cellStatus": "",
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "2da7008d-976e-4186-b855-b59c2fc63a55"
    }
  ],
  "metadata": {
    "a365ComputeOptions": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "sessionKeepAliveTimeout": 0,
    "widgets": {},
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "dependencies": {
      "environment": {},
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d",
        "known_lakehouses": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}