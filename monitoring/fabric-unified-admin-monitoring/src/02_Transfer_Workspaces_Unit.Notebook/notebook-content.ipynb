{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark",
      "jupyter_kernel_name": null
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "6cff634b-88f7-3505-bed2-c03a36776a8b",
        "default_lakehouse_name": "FUAM_Lakehouse",
        "default_lakehouse_workspace_id": "88c8d9fa-2c24-3fad-8f46-b36431c7ba1d"
      },
      "environment": {}
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "widgets": {},
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    }
  },
  "cells": [
    {
      "id": "e6f3fbdd-e0f2-48ce-82f6-a393d512149e",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Workspaces\n",
        "\n",
        "##### Data ingestion strategy:\n",
        "<mark style=\"background: #88D5FF;\">**REPLACE**</mark>\n",
        "\n",
        "##### Related pipeline:\n",
        "\n",
        "**Load_PBI_Workspaces_E2E**\n",
        "\n",
        "##### Source:\n",
        "\n",
        "**Files** from FUAM_Lakehouse folder **bronze_file_location** variable\n",
        "\n",
        "##### Target:\n",
        "\n",
        "**1 Delta table** in FUAM_Lakehouse \n",
        "- **gold_table_name** variable value\n"
      ]
    },
    {
      "id": "f03f0365-c0b9-433b-a0d1-17bd3c7966bf",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "import requests\n",
        "from pyspark.sql.functions import col, lit, udf, explode, to_date, json_tuple, from_json, schema_of_json, get_json_object, upper\n",
        "from pyspark.sql.types import StringType, json\n",
        "from pyspark.sql import SparkSession\n",
        "import json\n",
        "from delta.tables import *\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import *\n",
        "import datetime\n",
        "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\") # needed for automatic schema evolution in merge "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b5e7d90a-f8d8-4391-8ad3-09b543c19024",
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "## Parameters\n",
        "display_data = True"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d4154521-76fd-4dcb-bed4-5d5a46ec02de",
      "cell_type": "markdown",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Workspaces via PBI API"
      ]
    },
    {
      "id": "c69faf2b-0bf5-4b43-8fc9-bb44d210f886",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "## Variables\n",
        "pbi_bronze_file_location = f\"Files/raw/workspaces/\"\n",
        "pbi_silver_table_name = \"FUAM_Staging_Lakehouse.workspaces_silver\"\n",
        "pbi_gold_table_name = \"workspaces\"\n",
        "pbi_gold_table_name_with_prefix = f\"Tables/{pbi_gold_table_name}\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "61739131-ac46-46e9-bb6b-7d88a66a7382",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Clean Silver table, if exists\n",
        "if spark.catalog.tableExists(pbi_silver_table_name):\n",
        "    del_query = \"DELETE FROM \" + pbi_silver_table_name\n",
        "    spark.sql(del_query)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "59be80f0-a309-4e83-9fc4-a7666be86bcb",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Get Bronze data\n",
        "bronze_df = spark.read.option(\"multiline\", \"true\").json(pbi_bronze_file_location)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1af6c25a-8b27-4066-b29f-0eb1967ba5a7",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Explode json subset structure\n",
        "exploded_df = bronze_df.select(explode(\"value\").alias(\"d\"))\n",
        "\n",
        "# Extract json objects to tabular form\n",
        "extracted_df = exploded_df.select(col(\"d.*\"))\n",
        "\n",
        "# Convert key(s) to upper case\n",
        "extracted_df = extracted_df.withColumn(\"id\", f.upper(f.col(\"id\")))\n",
        "extracted_df = extracted_df.withColumn(\"capacityId\", f.upper(f.col(\"capacityId\")))\n",
        "\n",
        "# Generate empty description column in case it is not available\n",
        "if  not (\"description\" in extracted_df.columns):\n",
        "    print(\"Create empty description column\")\n",
        "    extracted_df = extracted_df.withColumn(\"description\", lit(\"\"))\n",
        "\n",
        "# Select all columns\n",
        "silver_df = extracted_df.select(\n",
        "    col(\"capacityId\").alias(\"CapacityId\"),\n",
        "    col(\"id\").alias(\"WorkspaceId\"),\n",
        "    col(\"capacityMigrationStatus\").alias(\"CapacityMigrationStatus\"),\n",
        "    col(\"defaultDatasetStorageFormat\").alias(\"DefaultDatasetStorageFormat\"),\n",
        "    col(\"description\").alias(\"Description\"),\n",
        "    col(\"hasWorkspaceLevelSettings \").alias(\"HasWorkspaceLevelSettings\"),\n",
        "    col(\"isOnDedicatedCapacity\").alias(\"IsOnDedicatedCapacity\"),\n",
        "    col(\"isReadOnly\").alias(\"IsReadOnly\"),\n",
        "    col(\"name\").alias(\"WorkspaceName\"),\n",
        "    col(\"state\").alias(\"State\"),\n",
        "    col(\"type\").alias(\"Type\")\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d04bb645-fa43-482a-b96f-292422d5ff63",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "source": [
        "if display_data:\n",
        "    display(silver_df)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "74d9a7a3-cd0b-417d-987a-9ba70c3bb108",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "source": [
        "# Write prepared bronze_df to silver delta table\n",
        "silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(pbi_silver_table_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "581c54c3-0afd-493d-b6d0-14bd415b4158",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "cellStatus": ""
      },
      "source": [
        "\n",
        "# This function maps and merges the silver data to gold dynamically\n",
        "def write_silver_to_gold(pbi_silver_table_name, gold_table_name, ids):\n",
        "    # TODO\n",
        "    query = \"SELECT *, current_timestamp() AS fuam_modified_at, False as fuam_deleted  FROM \" + pbi_silver_table_name \n",
        "    silver_df = spark.sql(query)\n",
        "    \n",
        "    if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', pbi_gold_table_name):\n",
        "        # if exists -> MERGE to gold\n",
        "        print(\"Gold table exists and will be merged.\")\n",
        "        gold_df = DeltaTable.forName(spark, pbi_gold_table_name)\n",
        "\n",
        "\n",
        "        gold_columns = gold_df.toDF().columns\n",
        "        silver_columns = silver_df.columns\n",
        "        combined_columns = list(set(gold_columns) | set(silver_columns))\n",
        "        id_cols = {}\n",
        "        merge_id_stmt = ''\n",
        "        for col in combined_columns:\n",
        "            if col in ids:\n",
        "                merge_id_stmt =  merge_id_stmt +  \" t.\" + col + \" = s.\" + col + \" and\"\n",
        "                id_cols[col] = \"s.\" + col\n",
        "\n",
        "                \n",
        "        # delete last and in merge id statement\n",
        "        merge_id_stmt = merge_id_stmt[:-4]\n",
        "\n",
        "\n",
        "        # Merge silver (s = source) to gold (t = target)\n",
        "        try:\n",
        "            merge = (gold_df.alias('t') \\\n",
        "            .merge(silver_df.alias('s'), merge_id_stmt )) \\\n",
        "            .whenMatchedUpdateAll() \\\n",
        "            .whenNotMatchedInsertAll() \\\n",
        "            .whenNotMatchedBySourceUpdate( condition = \"t.fuam_deleted == False or t.fuam_deleted IS NULL\", set = {\"fuam_deleted\" : \"True\", \"fuam_modified_at\": \"current_timestamp()\"} )\n",
        "            \n",
        "            merge.execute()\n",
        "        except:\n",
        "        # In case the tables already exist, but the fuam column are not existent because of an old version do merge whenNotMatchedBySourceUpdate\n",
        "            merge = (gold_df.alias('t') \\\n",
        "            .merge(silver_df.alias('s'), merge_id_stmt )) \\\n",
        "            .whenMatchedUpdateAll() \\\n",
        "            .whenNotMatchedInsertAll() \\\n",
        "                        \n",
        "            merge.execute()\n",
        "\n",
        "    else:\n",
        "        # else -> INSERT to gold\n",
        "        print(\"Gold table will be created.\")\n",
        "\n",
        "        silver_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(pbi_gold_table_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2cf057a6-822c-44c9-9ceb-e66f2b8859c3",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "cellStatus": ""
      },
      "source": [
        "# Merge semantic model refreshes to gold table\n",
        "write_silver_to_gold(pbi_silver_table_name, pbi_gold_table_name, ['WorkspaceId'])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "375e3168-aa62-4abc-b2ee-09f41a2b8f3f",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# write history of bronze files\n",
        "mssparkutils.fs.cp(pbi_bronze_file_location, pbi_bronze_file_location.replace(\"Files/raw/\", \"Files/history/\") + datetime.datetime.now().strftime('%Y/%m/%d') + \"/\", True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b47463f1-42ee-4a8d-acf5-92abf52e0be2",
      "cell_type": "markdown",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Workspaces via Fabric API"
      ]
    },
    {
      "id": "0e3ffa36-077f-4cda-86d6-de28d15f276c",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "## Variables\n",
        "fb_bronze_file_location = f\"Files/raw/fabric_workspaces/\"\n",
        "fb_silver_table_name = \"FUAM_Staging_Lakehouse.fabric_workspaces_silver\"\n",
        "fb_gold_table_name = \"fabric_workspaces\"\n",
        "fb_gold_table_name_with_prefix = f\"Tables/{fb_gold_table_name}\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3de17ff0-01bd-4eaa-ae97-36b2abb578e6",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Clean Silver table, if exists\n",
        "if spark.catalog.tableExists(fb_silver_table_name):\n",
        "    del_query = \"DELETE FROM \" + fb_silver_table_name\n",
        "    spark.sql(del_query)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "35ccdcdd-a071-4547-86a1-28724e3e5253",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Get Bronze data\n",
        "fb_bronze_df = spark.read.option(\"multiline\", \"true\").json(fb_bronze_file_location)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2509cb86-31d6-4658-a400-a171cc0d3e32",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "source": [
        "if display_data:\n",
        "    display(fb_bronze_df)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ea9c0932-1bff-44fe-b91f-ae7e90979205",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "source": [
        "# Explode json subset structure\n",
        "fb_exploded_df = fb_bronze_df.select(explode(\"workspaces\").alias(\"d\"))\n",
        "# Extract json objects to tabular form\n",
        "fb_extracted_df = fb_exploded_df.select(col(\"d.*\"))\n",
        "\n",
        "\n",
        "if \"domainId\" not in fb_extracted_df.columns:\n",
        "    fb_extracted_df = fb_extracted_df.withColumn(\"domainId\",lit(None) )\n",
        "\n",
        "fb_extracted_df = fb_extracted_df.withColumnRenamed(\"capacityId\", \"CapacityId\")\\\n",
        ".withColumnRenamed(\"id\", \"WorkspaceId\")\\\n",
        ".withColumnRenamed(\"domainId\", \"DomainId\")\n",
        "\n",
        "fb_extracted_df = fb_extracted_df.withColumn(\"DomainId\", upper(\"DomainId\"))\\\n",
        ".withColumn(\"CapacityId\", upper(\"CapacityId\"))\\\n",
        ".withColumn(\"WorkspaceId\", upper(\"WorkspaceId\"))\\\n",
        "\n",
        "\n",
        "if display_data:\n",
        "    display(fb_extracted_df)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "26815eea-5532-4dcd-913b-7c42ea3be2e6",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "source": [
        "# Rename columns if needed\n",
        "fb_silver_df = fb_extracted_df\n",
        "\n",
        "if display_data:\n",
        "    display(fb_silver_df)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "edbb0e9f-ff10-4386-8f48-d7f2890b5c9f",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Write prepared bronze_df to silver delta table\n",
        "fb_silver_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(fb_silver_table_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "68bcb6e6-679b-41b4-805f-7f7c060a6013",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "source": [
        "# Get Silver table data\n",
        "query = \"\"\"\n",
        "SELECT *\n",
        "FROM \"\"\" + fb_silver_table_name\n",
        "\n",
        "\n",
        "fb_silver_df = spark.sql(query)\n",
        "\n",
        "if display_data:\n",
        "     display(fb_silver_df)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e4753342-8e8b-46ca-98eb-60151f0a8112",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "#fb_silver_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(fb_gold_table_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c988687b-fb54-4bc4-8aca-fafa866345f7",
      "cell_type": "markdown",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Merge Workspace Tables"
      ]
    },
    {
      "id": "fb0daa8a-eede-4bf4-a51b-34521dcda657",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "source": [
        "df_workspaces = spark.sql(\"SELECT * FROM FUAM_Lakehouse.workspaces\")\n",
        "display(df_workspaces)\n",
        "\n",
        "if \"DomainId\" not in df_workspaces.columns:\n",
        "    spark.sql(\"ALTER TABLE workspaces ADD COLUMNS (DomainId STRING) \")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cc0c8881-a090-452a-933b-8bd59a488ba0",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "sparksql",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": "",
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "MERGE INTO workspaces trg\n",
        "USING FUAM_Staging_Lakehouse.fabric_workspaces_silver src \n",
        "ON trg.WorkspaceId = src.WorkspaceId\n",
        "WHEN MATCHED THEN\n",
        "UPDATE SET DomainId = src.DomainId"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "280ddbcd-eefb-4818-a406-3d31326c934f",
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "cellStatus": ""
      },
      "source": [
        "# Write history of bronze files\n",
        "from datetime import datetime\n",
        "mssparkutils.fs.cp(fb_bronze_file_location, fb_bronze_file_location.replace(\"Files/raw/\", \"Files/history/\") + datetime.now().strftime('%Y/%m/%d') + \"/\", True)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}