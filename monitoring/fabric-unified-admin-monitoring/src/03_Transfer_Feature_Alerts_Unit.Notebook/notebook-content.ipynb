{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8230571e",
   "metadata": {},
   "source": [
    "# 03 - Transfer Feature Alerts Unit\n",
    "\n",
    "**Purpose**: Generate alerts for newly activated or high-risk preview features\n",
    "\n",
    "**Inputs**:\n",
    "- Delta Table: `preview_features_active`\n",
    "- Delta Table: `feature_alerts` (for historical tracking)\n",
    "\n",
    "**Outputs**:\n",
    "- Delta Table: `feature_alerts`\n",
    "\n",
    "**Frequency**: Daily (recommended)\n",
    "\n",
    "**Alert Triggers**:\n",
    "- New preview feature detected\n",
    "- Preview feature active >90 days (approaching GA)\n",
    "- Preview feature with low confidence match (similarity <0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031c9ec",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88adb1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alert thresholds\n",
    "ALERT_DAYS_THRESHOLD = 90  # Alert if preview active >90 days\n",
    "LOW_CONFIDENCE_THRESHOLD = 0.5  # Alert if similarity score <0.5\n",
    "\n",
    "# Alert severity levels\n",
    "SEVERITY_INFO = \"Info\"\n",
    "SEVERITY_WARNING = \"Warning\"\n",
    "SEVERITY_CRITICAL = \"Critical\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff6936",
   "metadata": {},
   "source": [
    "## Step 1: Load Current Activated Preview Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b23d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Loading activated preview features...\")\n",
    "\n",
    "df_active = spark.read.format(\"delta\").load(\"Tables/preview_features_active\")\n",
    "\n",
    "active_count = df_active.count()\n",
    "print(f\"âœ… Loaded {active_count} activated preview features\")\n",
    "\n",
    "df_active.select(\"feature_id\", \"feature_name\", \"workload\", \"days_since_release\", \"similarity_score\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991be8e",
   "metadata": {},
   "source": [
    "## Step 2: Load Historical Alerts (to avoid duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe05695",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"feature_alerts\"\n",
    "table_path = f\"Tables/{table_name}\"\n",
    "\n",
    "try:\n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    if DeltaTable.isDeltaTable(spark, table_path):\n",
    "        print(\"ðŸ”„ Loading historical alerts...\")\n",
    "        df_historical = spark.read.format(\"delta\").load(table_path)\n",
    "        historical_count = df_historical.count()\n",
    "        print(f\"âœ… Loaded {historical_count} historical alerts\")\n",
    "        \n",
    "        # Get already alerted feature_ids\n",
    "        alerted_features = set([row[\"feature_id\"] for row in df_historical.select(\"feature_id\").distinct().collect()])\n",
    "        print(f\"  â†’ {len(alerted_features)} unique features already alerted\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ No historical alerts found (first run)\")\n",
    "        df_historical = None\n",
    "        alerted_features = set()\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸ No historical alerts (first run): {e}\")\n",
    "    df_historical = None\n",
    "    alerted_features = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440eeb33",
   "metadata": {},
   "source": [
    "## Step 3: Generate Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a948004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alerts(df_active, alerted_features):\n",
    "    \"\"\"\n",
    "    Generate alerts based on business rules\n",
    "    Returns list of alert dicts\n",
    "    \"\"\"\n",
    "    alerts = []\n",
    "    \n",
    "    for row in df_active.collect():\n",
    "        feature_id = row[\"feature_id\"]\n",
    "        feature_name = row[\"feature_name\"]\n",
    "        workload = row[\"workload\"]\n",
    "        days_since_release = row[\"days_since_release\"]\n",
    "        similarity_score = row[\"similarity_score\"]\n",
    "        setting_name = row[\"setting_name\"]\n",
    "        \n",
    "        # Rule 1: NEW PREVIEW FEATURE ACTIVATED\n",
    "        if feature_id not in alerted_features:\n",
    "            alerts.append({\n",
    "                \"alert_id\": f\"NEW_{feature_id}_{datetime.now().strftime('%Y%m%d')}\",\n",
    "                \"feature_id\": feature_id,\n",
    "                \"feature_name\": feature_name,\n",
    "                \"workload\": workload,\n",
    "                \"alert_type\": \"New Preview Activated\",\n",
    "                \"severity\": SEVERITY_INFO,\n",
    "                \"message\": f\"New preview feature '{feature_name}' has been activated (Setting: {setting_name})\",\n",
    "                \"days_since_release\": days_since_release,\n",
    "                \"similarity_score\": similarity_score,\n",
    "                \"alert_date\": datetime.now(),\n",
    "                \"acknowledged\": False\n",
    "            })\n",
    "        \n",
    "        # Rule 2: LONG-RUNNING PREVIEW (>90 days)\n",
    "        if days_since_release and days_since_release > ALERT_DAYS_THRESHOLD:\n",
    "            alert_id = f\"LONGRUN_{feature_id}_{datetime.now().strftime('%Y%m%d')}\"\n",
    "            \n",
    "            alerts.append({\n",
    "                \"alert_id\": alert_id,\n",
    "                \"feature_id\": feature_id,\n",
    "                \"feature_name\": feature_name,\n",
    "                \"workload\": workload,\n",
    "                \"alert_type\": \"Long-Running Preview\",\n",
    "                \"severity\": SEVERITY_WARNING,\n",
    "                \"message\": f\"Preview feature '{feature_name}' has been active for {days_since_release} days. Consider reviewing for GA transition.\",\n",
    "                \"days_since_release\": days_since_release,\n",
    "                \"similarity_score\": similarity_score,\n",
    "                \"alert_date\": datetime.now(),\n",
    "                \"acknowledged\": False\n",
    "            })\n",
    "        \n",
    "        # Rule 3: LOW CONFIDENCE MATCH\n",
    "        if similarity_score < LOW_CONFIDENCE_THRESHOLD:\n",
    "            alerts.append({\n",
    "                \"alert_id\": f\"LOWCONF_{feature_id}_{datetime.now().strftime('%Y%m%d')}\",\n",
    "                \"feature_id\": feature_id,\n",
    "                \"feature_name\": feature_name,\n",
    "                \"workload\": workload,\n",
    "                \"alert_type\": \"Low Confidence Match\",\n",
    "                \"severity\": SEVERITY_CRITICAL,\n",
    "                \"message\": f\"Low confidence match ({similarity_score:.2f}) between setting '{setting_name}' and feature '{feature_name}'. Manual review recommended.\",\n",
    "                \"days_since_release\": days_since_release,\n",
    "                \"similarity_score\": similarity_score,\n",
    "                \"alert_date\": datetime.now(),\n",
    "                \"acknowledged\": False\n",
    "            })\n",
    "    \n",
    "    return alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Generating alerts based on business rules...\")\n",
    "\n",
    "alerts_data = generate_alerts(df_active, alerted_features)\n",
    "\n",
    "print(f\"âœ… Generated {len(alerts_data)} new alerts\")\n",
    "\n",
    "# Preview alerts\n",
    "if alerts_data:\n",
    "    print(\"\\nðŸ“‹ Sample alerts:\")\n",
    "    for alert in alerts_data[:5]:\n",
    "        print(f\"  [{alert['severity']}] {alert['alert_type']}\")\n",
    "        print(f\"    â†’ {alert['message']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a7afc0",
   "metadata": {},
   "source": [
    "## Step 4: Create Alerts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if alerts_data:\n",
    "    schema = StructType([\n",
    "        StructField(\"alert_id\", StringType(), False),\n",
    "        StructField(\"feature_id\", StringType(), False),\n",
    "        StructField(\"feature_name\", StringType(), False),\n",
    "        StructField(\"workload\", StringType(), True),\n",
    "        StructField(\"alert_type\", StringType(), False),\n",
    "        StructField(\"severity\", StringType(), False),\n",
    "        StructField(\"message\", StringType(), False),\n",
    "        StructField(\"days_since_release\", IntegerType(), True),\n",
    "        StructField(\"similarity_score\", DoubleType(), True),\n",
    "        StructField(\"alert_date\", TimestampType(), False),\n",
    "        StructField(\"acknowledged\", BooleanType(), False)\n",
    "    ])\n",
    "    \n",
    "    df_alerts = spark.createDataFrame(alerts_data, schema=schema)\n",
    "    \n",
    "    print(f\"âœ… Created alerts DataFrame with {df_alerts.count()} rows\")\n",
    "    df_alerts.show(5, truncate=False)\n",
    "else:\n",
    "    print(\"â„¹ï¸ No new alerts to generate\")\n",
    "    df_alerts = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae1e2fc",
   "metadata": {},
   "source": [
    "## Step 5: Write Alerts to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49940421",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_alerts and df_alerts.count() > 0:\n",
    "    print(f\"ðŸ”„ Writing alerts to Delta table: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        from delta.tables import DeltaTable\n",
    "        \n",
    "        # Check if table exists\n",
    "        if DeltaTable.isDeltaTable(spark, table_path):\n",
    "            print(\"  â†’ Table exists, appending new alerts...\")\n",
    "            \n",
    "            # Append mode - keep all alerts for historical tracking\n",
    "            df_alerts.write.format(\"delta\").mode(\"append\").save(table_path)\n",
    "            \n",
    "            print(\"  âœ… Alerts appended\")\n",
    "        else:\n",
    "            print(\"  â†’ Table doesn't exist, creating new table...\")\n",
    "            df_alerts.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "            print(\"  âœ… Table created\")\n",
    "        \n",
    "        # Show final count\n",
    "        final_count = spark.read.format(\"delta\").load(table_path).count()\n",
    "        print(f\"\\nâœ… Total alerts in {table_name}: {final_count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error writing alerts: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"â„¹ï¸ No alerts to write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709754b",
   "metadata": {},
   "source": [
    "## Step 6: Alert Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9518879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, table_path):\n",
    "    print(\"\\nðŸ“Š Alert Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_all_alerts = spark.read.format(\"delta\").load(table_path)\n",
    "    \n",
    "    # By severity\n",
    "    print(\"\\nðŸ”¸ By Severity:\")\n",
    "    df_all_alerts.groupBy(\"severity\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "    \n",
    "    # By alert type\n",
    "    print(\"\\nðŸ”¸ By Alert Type:\")\n",
    "    df_all_alerts.groupBy(\"alert_type\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "    \n",
    "    # By workload\n",
    "    print(\"\\nðŸ”¸ By Workload:\")\n",
    "    df_all_alerts.groupBy(\"workload\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "    \n",
    "    # Unacknowledged alerts\n",
    "    unack_count = df_all_alerts.filter(F.col(\"acknowledged\") == False).count()\n",
    "    print(f\"\\nðŸ”¸ Unacknowledged alerts: {unack_count}\")\n",
    "    \n",
    "    # Recent alerts (last 7 days)\n",
    "    recent_cutoff = datetime.now() - timedelta(days=7)\n",
    "    recent_count = df_all_alerts.filter(F.col(\"alert_date\") >= recent_cutoff).count()\n",
    "    print(f\"ðŸ”¸ Alerts in last 7 days: {recent_count}\")\n",
    "    \n",
    "    # Critical alerts\n",
    "    critical_count = df_all_alerts.filter(F.col(\"severity\") == SEVERITY_CRITICAL).count()\n",
    "    print(f\"ðŸ”¸ Critical alerts: {critical_count}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"âœ… Transfer Feature Alerts Unit - COMPLETED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0650ea7",
   "metadata": {},
   "source": [
    "## Step 7: Export Alerts for Data Activator (Optional)\n",
    "\n",
    "This creates a view that Data Activator can monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67823959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, table_path):\n",
    "    print(\"\\nðŸ”„ Creating view for Data Activator integration...\")\n",
    "    \n",
    "    df_activator = spark.read.format(\"delta\").load(table_path)\n",
    "    \n",
    "    # Filter to unacknowledged critical/warning alerts\n",
    "    df_activator_filtered = df_activator.filter(\n",
    "        (F.col(\"acknowledged\") == False) & \n",
    "        (F.col(\"severity\").isin([SEVERITY_CRITICAL, SEVERITY_WARNING]))\n",
    "    )\n",
    "    \n",
    "    # Create or replace temp view\n",
    "    df_activator_filtered.createOrReplaceTempView(\"vw_feature_alerts_active\")\n",
    "    \n",
    "    alert_count = df_activator_filtered.count()\n",
    "    print(f\"âœ… Created view 'vw_feature_alerts_active' with {alert_count} active alerts\")\n",
    "    print(\"   â†’ This view can be monitored by Data Activator for real-time notifications\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
