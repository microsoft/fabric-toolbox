{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be68f0c",
   "metadata": {},
   "source": [
    "# 01 - Transfer Feature Releases Unit\n",
    "\n",
    "**Purpose**: Extract Microsoft Fabric feature releases from Microsoft Learn what's new documentation\n",
    "\n",
    "**Inputs**:\n",
    "- Microsoft Learn URL: https://learn.microsoft.com/en-us/fabric/get-started/whats-new\n",
    "\n",
    "**Outputs**:\n",
    "- Delta Table: `feature_releases`\n",
    "\n",
    "**Frequency**: Daily (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00196e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc0cf4",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7540d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (can be passed from Pipeline)\n",
    "whats_new_url = \"https://learn.microsoft.com/en-us/fabric/get-started/whats-new\"\n",
    "lookback_days = 90  # Extract features from last 90 days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba066b",
   "metadata": {},
   "source": [
    "## Step 1: Fetch What's New Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ”„ Fetching Fabric What's New from: {whats_new_url}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(whats_new_url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    content = response.text\n",
    "    print(f\"âœ… Successfully fetched {len(content)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error fetching content: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8fb67",
   "metadata": {},
   "source": [
    "## Step 2: Parse Feature Releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eced003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_feature_releases(html_content):\n",
    "    \"\"\"\n",
    "    Parse feature releases from Microsoft Learn What's New page\n",
    "    Returns list of dicts with feature information\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Pattern to match feature sections\n",
    "    # Example: ## September 2024\n",
    "    month_pattern = r'##\\s+([A-Za-z]+\\s+\\d{4})'\n",
    "    \n",
    "    # Pattern to match feature entries\n",
    "    # Example: ### Data Warehouse - New feature (Preview)\n",
    "    feature_pattern = r'###\\s+(.+?)\\s*(?:\\(([^)]+)\\))?'\n",
    "    \n",
    "    # Split content by months\n",
    "    months = re.split(month_pattern, html_content)\n",
    "    \n",
    "    for i in range(1, len(months), 2):\n",
    "        if i+1 >= len(months):\n",
    "            break\n",
    "            \n",
    "        month_str = months[i].strip()\n",
    "        month_content = months[i+1]\n",
    "        \n",
    "        try:\n",
    "            release_date = datetime.strptime(month_str, \"%B %Y\")\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Find all features in this month\n",
    "        feature_matches = re.finditer(feature_pattern, month_content)\n",
    "        \n",
    "        for match in feature_matches:\n",
    "            feature_name = match.group(1).strip()\n",
    "            status = match.group(2).strip() if match.group(2) else \"GA\"\n",
    "            \n",
    "            # Determine workload from feature name\n",
    "            workload = \"Unknown\"\n",
    "            if any(kw in feature_name.lower() for kw in [\"data warehouse\", \"warehouse\", \"sql\"]):\n",
    "                workload = \"Data Warehouse\"\n",
    "            elif any(kw in feature_name.lower() for kw in [\"data factory\", \"pipeline\", \"dataflow\"]):\n",
    "                workload = \"Data Factory\"\n",
    "            elif any(kw in feature_name.lower() for kw in [\"power bi\", \"semantic model\", \"report\"]):\n",
    "                workload = \"Power BI\"\n",
    "            elif any(kw in feature_name.lower() for kw in [\"real-time\", \"kql\", \"eventhouse\"]):\n",
    "                workload = \"Real-Time Intelligence\"\n",
    "            elif any(kw in feature_name.lower() for kw in [\"lakehouse\", \"onelake\"]):\n",
    "                workload = \"Data Engineering\"\n",
    "            elif any(kw in feature_name.lower() for kw in [\"data science\", \"ml\", \"notebook\"]):\n",
    "                workload = \"Data Science\"\n",
    "            \n",
    "            # Determine if it's a preview feature\n",
    "            is_preview = \"preview\" in status.lower()\n",
    "            \n",
    "            features.append({\n",
    "                \"feature_id\": f\"{release_date.strftime('%Y%m')}_{hash(feature_name) % 100000}\",\n",
    "                \"feature_name\": feature_name,\n",
    "                \"workload\": workload,\n",
    "                \"release_date\": release_date,\n",
    "                \"status\": status,\n",
    "                \"is_preview\": is_preview,\n",
    "                \"source_url\": whats_new_url,\n",
    "                \"extracted_date\": datetime.now()\n",
    "            })\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Parsing feature releases...\")\n",
    "features_data = parse_feature_releases(content)\n",
    "\n",
    "# Filter to last N days\n",
    "cutoff_date = datetime.now() - timedelta(days=lookback_days)\n",
    "features_data = [f for f in features_data if f[\"release_date\"] >= cutoff_date]\n",
    "\n",
    "print(f\"âœ… Parsed {len(features_data)} feature releases from last {lookback_days} days\")\n",
    "\n",
    "# Preview\n",
    "if features_data:\n",
    "    print(\"\\nðŸ“‹ Sample features:\")\n",
    "    for feat in features_data[:3]:\n",
    "        print(f\"  - {feat['feature_name']} ({feat['status']}) - {feat['workload']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa947a",
   "metadata": {},
   "source": [
    "## Step 3: Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df118f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"release_date\", TimestampType(), False),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"is_preview\", BooleanType(), False),\n",
    "    StructField(\"source_url\", StringType(), True),\n",
    "    StructField(\"extracted_date\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "df_features = spark.createDataFrame(features_data, schema=schema)\n",
    "\n",
    "print(f\"âœ… Created DataFrame with {df_features.count()} rows\")\n",
    "df_features.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eeb551",
   "metadata": {},
   "source": [
    "## Step 4: Write to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table path\n",
    "table_name = \"feature_releases\"\n",
    "table_path = f\"Tables/{table_name}\"\n",
    "\n",
    "print(f\"ðŸ”„ Writing to Delta table: {table_name}\")\n",
    "\n",
    "# Write with merge logic (upsert)\n",
    "try:\n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    # Check if table exists\n",
    "    if DeltaTable.isDeltaTable(spark, table_path):\n",
    "        print(\"  â†’ Table exists, performing MERGE (upsert)...\")\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(spark, table_path)\n",
    "        \n",
    "        # Merge: Update existing, insert new\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df_features.alias(\"source\"),\n",
    "            \"target.feature_id = source.feature_id\"\n",
    "        ).whenMatchedUpdateAll(\n",
    "        ).whenNotMatchedInsertAll(\n",
    "        ).execute()\n",
    "        \n",
    "        print(\"  âœ… MERGE completed\")\n",
    "    else:\n",
    "        print(\"  â†’ Table doesn't exist, creating new table...\")\n",
    "        df_features.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "        print(\"  âœ… Table created\")\n",
    "    \n",
    "    # Show final count\n",
    "    final_count = spark.read.format(\"delta\").load(table_path).count()\n",
    "    print(f\"\\nâœ… Total records in {table_name}: {final_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error writing to Delta: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f45b2",
   "metadata": {},
   "source": [
    "## Step 5: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625df20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š Feature Release Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_summary = spark.read.format(\"delta\").load(table_path)\n",
    "\n",
    "# By workload\n",
    "print(\"\\nðŸ”¸ By Workload:\")\n",
    "df_summary.groupBy(\"workload\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# By status\n",
    "print(\"\\nðŸ”¸ By Status:\")\n",
    "df_summary.groupBy(\"status\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# Preview vs GA\n",
    "print(\"\\nðŸ”¸ Preview vs GA:\")\n",
    "df_summary.groupBy(\"is_preview\").count().show(truncate=False)\n",
    "\n",
    "# Recent releases (last 30 days)\n",
    "recent_cutoff = datetime.now() - timedelta(days=30)\n",
    "recent_count = df_summary.filter(F.col(\"release_date\") >= recent_cutoff).count()\n",
    "print(f\"\\nðŸ”¸ Features released in last 30 days: {recent_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Transfer Feature Releases Unit - COMPLETED\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
