{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd3012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505200d2",
   "metadata": {},
   "source": [
    "## Step 1: Create `feature_releases_roadmap` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6122dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Creating table: feature_releases_roadmap\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "schema_roadmap = StructType([\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"feature_description\", StringType(), True),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"release_date\", TimestampType(), True),  # Nullable for planned features\n",
    "    StructField(\"release_type\", StringType(), True),\n",
    "    StructField(\"release_status\", StringType(), True),\n",
    "    StructField(\"is_preview\", BooleanType(), False),\n",
    "    StructField(\"is_planned\", BooleanType(), False),\n",
    "    StructField(\"is_shipped\", BooleanType(), False),\n",
    "    StructField(\"last_modified\", TimestampType(), False),\n",
    "    StructField(\"source_url\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"extracted_date\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "df_roadmap = spark.createDataFrame([], schema_roadmap)\n",
    "table_path = \"Tables/feature_releases_roadmap\"\n",
    "df_roadmap.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "\n",
    "print(\"‚úÖ Table created: feature_releases_roadmap\")\n",
    "print(f\"   Schema: {len(schema_roadmap.fields)} columns\")\n",
    "print(\"\\n   üí° Includes planned/future features and historical tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49385cd0",
   "metadata": {},
   "source": [
    "## Step 2: Create `preview_features_active` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dcfc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Creating table: preview_features_active\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "schema_preview_active = StructType([\n",
    "    StructField(\"setting_name\", StringType(), False),\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"similarity_score\", DoubleType(), False),\n",
    "    StructField(\"is_enabled\", BooleanType(), False),\n",
    "    StructField(\"delegate_to_tenant\", BooleanType(), True),\n",
    "    StructField(\"detected_date\", TimestampType(), False),\n",
    "    StructField(\"release_date\", TimestampType(), True),\n",
    "    StructField(\"release_status\", StringType(), True),\n",
    "    StructField(\"source_url\", StringType(), True),\n",
    "    StructField(\"days_since_release\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_preview_active = spark.createDataFrame([], schema_preview_active)\n",
    "table_path = \"Tables/preview_features_active\"\n",
    "df_preview_active.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "\n",
    "print(\"‚úÖ Table created: preview_features_active\")\n",
    "print(f\"   Schema: {len(schema_preview_active.fields)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8337a78",
   "metadata": {},
   "source": [
    "## Step 3: Create `feature_alerts` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66383382",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Creating table: feature_alerts\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "schema_alerts = StructType([\n",
    "    StructField(\"alert_id\", StringType(), False),\n",
    "    StructField(\"feature_id\", StringType(), False),\n",
    "    StructField(\"feature_name\", StringType(), False),\n",
    "    StructField(\"workload\", StringType(), True),\n",
    "    StructField(\"alert_type\", StringType(), False),\n",
    "    StructField(\"severity\", StringType(), False),\n",
    "    StructField(\"message\", StringType(), False),\n",
    "    StructField(\"setting_name\", StringType(), True),\n",
    "    StructField(\"similarity_score\", DoubleType(), True),\n",
    "    StructField(\"days_since_release\", IntegerType(), True),\n",
    "    StructField(\"alert_date\", TimestampType(), False),\n",
    "    StructField(\"acknowledged\", BooleanType(), False),\n",
    "    StructField(\"acknowledged_date\", TimestampType(), True),\n",
    "    StructField(\"acknowledged_by\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_alerts = spark.createDataFrame([], schema_alerts)\n",
    "table_path = \"Tables/feature_alerts\"\n",
    "df_alerts.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "\n",
    "print(\"‚úÖ Table created: feature_alerts\")\n",
    "print(f\"   Schema: {len(schema_alerts.fields)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21110f70",
   "metadata": {},
   "source": [
    "## Step 4: Create Helper SQL Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Creating helper SQL views...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# View 1: Roadmap Upcoming Features\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW vw_roadmap_upcoming AS\n",
    "    SELECT \n",
    "        feature_name,\n",
    "        feature_description,\n",
    "        product_name,\n",
    "        workload,\n",
    "        release_type,\n",
    "        release_status,\n",
    "        release_date,\n",
    "        is_preview,\n",
    "        is_planned,\n",
    "        last_modified,\n",
    "        CASE \n",
    "            WHEN release_date IS NULL THEN NULL\n",
    "            ELSE DATEDIFF(release_date, CURRENT_DATE())\n",
    "        END as days_until_release\n",
    "    FROM feature_releases_roadmap\n",
    "    WHERE is_planned = true\n",
    "      AND (release_date IS NULL OR release_date >= CURRENT_DATE())\n",
    "    ORDER BY release_date ASC NULLS LAST, last_modified DESC\n",
    "\"\"\")\n",
    "print(\"‚úÖ vw_roadmap_upcoming - Planned/upcoming features\")\n",
    "\n",
    "# View 2: Active Preview Features\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW vw_active_preview_features AS\n",
    "    SELECT \n",
    "        feature_name,\n",
    "        workload,\n",
    "        setting_name,\n",
    "        days_since_release,\n",
    "        similarity_score,\n",
    "        release_date,\n",
    "        detected_date,\n",
    "        is_enabled\n",
    "    FROM preview_features_active\n",
    "    WHERE is_enabled = true\n",
    "    ORDER BY detected_date DESC\n",
    "\"\"\")\n",
    "print(\"‚úÖ vw_active_preview_features - Currently enabled previews\")\n",
    "\n",
    "# View 3: Critical Alerts\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW vw_critical_alerts AS\n",
    "    SELECT \n",
    "        alert_id,\n",
    "        feature_name,\n",
    "        workload,\n",
    "        alert_type,\n",
    "        severity,\n",
    "        message,\n",
    "        alert_date,\n",
    "        acknowledged\n",
    "    FROM feature_alerts\n",
    "    WHERE acknowledged = false \n",
    "      AND severity IN ('Critical', 'Warning')\n",
    "    ORDER BY \n",
    "        CASE severity \n",
    "            WHEN 'Critical' THEN 1 \n",
    "            WHEN 'Warning' THEN 2 \n",
    "            ELSE 3 \n",
    "        END,\n",
    "        alert_date DESC\n",
    "\"\"\")\n",
    "print(\"‚úÖ vw_critical_alerts - Unacknowledged critical/warning alerts\")\n",
    "\n",
    "# View 4: Feature Release Timeline\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW vw_feature_timeline AS\n",
    "    SELECT \n",
    "        feature_name,\n",
    "        product_name,\n",
    "        workload,\n",
    "        release_type,\n",
    "        release_status,\n",
    "        is_preview,\n",
    "        is_planned,\n",
    "        is_shipped,\n",
    "        release_date,\n",
    "        CASE \n",
    "            WHEN release_date IS NULL THEN NULL\n",
    "            ELSE DATEDIFF(CURRENT_DATE(), release_date)\n",
    "        END as days_since_release,\n",
    "        last_modified\n",
    "    FROM feature_releases_roadmap\n",
    "    ORDER BY release_date DESC NULLS LAST\n",
    "\"\"\")\n",
    "print(\"‚úÖ vw_feature_timeline - Complete release timeline\")\n",
    "\n",
    "print(\"\\n‚úÖ All SQL views created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af215ea",
   "metadata": {},
   "source": [
    "## ‚úÖ Setup Complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7dc56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ FEATURE TRACKING SETUP COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify tables\n",
    "tables = [\"feature_releases_roadmap\", \"preview_features_active\", \"feature_alerts\"]\n",
    "print(\"\\nüìã Tables created:\")\n",
    "for table in tables:\n",
    "    try:\n",
    "        count = spark.read.format(\"delta\").load(f\"Tables/{table}\").count()\n",
    "        print(f\"  ‚úÖ {table}: {count} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {table}: ERROR - {e}\")\n",
    "\n",
    "# Verify views\n",
    "views = [\"vw_roadmap_upcoming\", \"vw_active_preview_features\", \"vw_critical_alerts\", \"vw_feature_timeline\"]\n",
    "print(\"\\nüìã Views created:\")\n",
    "for view in views:\n",
    "    try:\n",
    "        spark.sql(f\"SELECT * FROM {view} LIMIT 1\")\n",
    "        print(f\"  ‚úÖ {view}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {view}: ERROR - {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìö Next Step:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n  ‚Üí Run 'Load_Feature_Tracking' notebook to populate the tables\")\n",
    "print(\"\\nüí° Schedule Load_Feature_Tracking to run daily for continuous monitoring\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
