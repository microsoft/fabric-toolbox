// KQL script
// Use management commands in this script to configure your database items, such as tables, functions, materialized views, and more.


.create-merge table GatewaysHeartbeat (GatewayId:guid, FileVersion:string, ServerTimestampUTC:datetime, Responding:bool, ProcessName:string, ProductVersion:string, StartTime:datetime) with (folder = "System") 
.create-or-alter table GatewaysHeartbeat ingestion json mapping 'GatewaysHeartbeat_mapping'
```
[{"Properties":{"Path":"$['GatewayId']"},"column":"GatewayId","datatype":""},{"Properties":{"Path":"$['FileVersion']"},"column":"FileVersion","datatype":""},{"Properties":{"Path":"$['ServerTimestampUTC']"},"column":"ServerTimestampUTC","datatype":""},{"Properties":{"Path":"$['Responding']"},"column":"Responding","datatype":""},{"Properties":{"Path":"$['ProcessName']"},"column":"ProcessName","datatype":""},{"Properties":{"Path":"$['ProductVersion']"},"column":"ProductVersion","datatype":""},{"Properties":{"Path":"$['StartTime']"},"column":"StartTime","datatype":""}]
```
.create-merge table ['GatewayReports-Raw'] (logType:string, log:dynamic, logDate:datetime) with (folder = "Raw") 
.create-or-alter table ['GatewayReports-Raw'] ingestion json mapping 'GatewayReports-Raw_mapping'
```
[{"Properties":{"Path":"$['logType']"},"column":"logType","datatype":""},{"Properties":{"Path":"$['log']"},"column":"log","datatype":""},{"Properties":{"Path":"$['logDate']"},"column":"logDate","datatype":""}]
```
.create-merge table SystemCounterAggregationReport (GatewayObjectId:guid, AggregationStartTimeUTC:datetime, AggregationEndTimeUTC:datetime, CounterName:string, Max:real, Min:real, Average:real) with (folder = "System") 
.create-merge table QueryStartReport (GatewayObjectId:guid, RequestId:guid, DataSource:dynamic, QueryTrackingId:guid, QueryExecutionStartTimeUTC:datetime, OperationType:string, QueryText:string, Table:string, EvaluationContext:dynamic, ServiceName:string, CurrentActivityId:guid, ItemId:guid, QueryType:string, RootActivityId:guid, SKU:string, WorkspaceId:guid, HostContextType:string, ConnectionIds:string) with (folder = "Reports") 
.create-merge table QueryExecutionReport (GatewayObjectId:guid, RequestId:guid, DataSource:dynamic, QueryTrackingId:guid, QueryExecutionEndTimeUTC:datetime, QueryExecutionDuration_ms_:long, QueryType:string, DataReadingAndSerializationDuration_ms_:long, SpoolingDiskWritingDuration_ms_:long, SpoolingDiskReadingDuration_ms_:long, SpoolingTotalDataSize_byte_:long, DataProcessingEndTimeUTC:datetime, DataProcessingDuration_ms_:long, Success:string, ErrorMessage:string) with (folder = "Reports") 
.create-merge table OpenConnectionReport (GatewayObjectId:guid, RequestId:guid, DataSource:dynamic, OpenConnectionStartTimeUTC:datetime, OpenConnectionTrackingId:guid, ProviderName:string, OpenConnectionDuration_ms_:long, Success:string, ErrorMessage:string) with (folder = "Reports") 
.create-merge table QueryExecutionAggregationReport (GatewayObjectId:guid, AggregationStartTimeUTC:datetime, AggregationEndTimeUTC:datetime, DataSource:dynamic, Success:string, AverageQueryExecutionDuration_ms_:real, MaxQueryExecutionDuration_ms_:long, MinQueryExecutionDuration_ms_:long, QueryType:string, AverageDataProcessingDuration_ms_:real, MaxDataProcessingDuration_ms_:long, MinDataProcessingDuration_ms_:long, Count:long) with (folder = "Reports") 
.create-merge table QueryStreamingReport (GatewayObjectId:guid, RequestId:guid, DataSource:dynamic, QueryTrackingId:guid, StreamingStartTimeUTC:datetime, SpoolingDiskReadingDuration_ms_:long, StreamingDuration_ms_:long, Success:string, ErrorMessage:string) with (folder = "Reports") 
.create-merge table MashupEvaluationReport (GatewayObjectId:guid, QueryTrackingId:guid, ConnectionId:guid, DataSourceKinds:string, DataSourcePaths:string, EndTimeUTC:datetime, ['AverageCommit byte']:real, AverageIODataBytesPerSecond:real, AveragePercentProcessorTime:real, ['AverageWorkingSet byte']:real, ['MaxCommit byte']:long, MaxIODataBytesPerSecond:long, MaxPercentProcessorTime:real, ['MaxWorkingSet byte']:long, TotalBytes:long, TotalProcessorTime:timespan, TotalRows:real) with (folder = "Reports") 
.create-merge table ConcurrentOperationAggregationReport (GatewayObjectId:guid, AggregationStartTimeUTC:datetime, AggregationEndTimeUTC:datetime, CounterName:string, Max:real, Min:real, Average:real) with (folder = "Reports") 
.create-merge table GatewayNodeInfoRaw (clusterId:guid, clusterName:string, nodeId:guid, machine:string, cloudDatasourceRefresh:bool, contactInformation:string, customConnectors:bool, status:string, type:string, version:string, versionStatus:string, osName:string, osVersion:string, cores:long, logicalCores:long, memoryGb:real, logDate:datetime) with (folder = "Raw") 
.create-merge table CopyExecutionReport (GatewayObjectId:guid, ActivityId:guid, CopyExecutionStartTimeUTC:datetime, CompletionStatus:string, CopyExecutionDuration_ms:decimal, CopyWorkerCPUUtilizationPercentage:decimal, CopyWorkerMemoryUsage_byte:decimal) with (folder = "Reports") 
.create-or-alter function with (folder = "Parsers", skipvalidation = "true") parse_QueryStartReport_csv(T:(GatewayObjectId:guid,RequestId:guid,DataSource:dynamic,QueryTrackingId:guid,QueryExecutionStartTimeUTC:datetime,QueryType:string,QueryText:string,EvaluationContext:string)) {
      T
      | project GatewayObjectId,
            RequestId,
            DataSource=parse_json(DataSource),
            QueryTrackingId,
            QueryExecutionStartTimeUTC,
            OperationType=QueryType,
            QueryText = base64_decode_tostring(QueryText),
            Table = iff(isempty(extract("__AS_Query__ = (.+),",1,base64_decode_tostring(QueryText))),extract("<ccon>(.+)</ccon>",1,base64_decode_tostring(QueryText)),extract("__AS_Query__ = (.+),",1,base64_decode_tostring(QueryText))),
            EvaluationContext=parse_json(iff(isempty(parse_json(EvaluationContext)),'{"serviceTraceContexts":[{"serviceName":"Dataflow Gen1"}]}',EvaluationContext)),
            ServiceTraceContexts=parse_json(iff(isempty(parse_json(EvaluationContext)),'{"serviceTraceContexts":[{"serviceName":"Dataflow Gen1"}]}',EvaluationContext)).serviceTraceContexts
      | mv-expand ServiceTraceContexts
      | extend ServiceName=ServiceTraceContexts.serviceName
      | mv-expand ServiceTraceContexts.traceIds
      | evaluate bag_unpack(ServiceTraceContexts_traceIds) : (
            GatewayObjectId:guid,
            RequestId:guid,
            DataSource:dynamic,
            QueryTrackingId:guid,
            QueryExecutionStartTimeUTC:datetime,
            OperationType:string,
            QueryText:string,
            Table:string,
            EvaluationContext:dynamic,
            ServiceTraceContexts:dynamic,
            ServiceName:string,
            key:string,
            value:string
      )
      | evaluate pivot(key,take_any(value)) : (
            GatewayObjectId:guid,
            RequestId:guid,
            DataSource:dynamic,
            QueryTrackingId:guid,
            QueryExecutionStartTimeUTC:datetime,
            OperationType:string,
            QueryText:string,
            Table:string,
            EvaluationContext:dynamic,
            ServiceName:string,
            CurrentActivityId:guid,
            ["Session Id"]:guid,
            ["Host Context Type"]:string,
            ["Connection Ids"]:string,
            ["Dataflow Id"]:guid,
            ["Workspace Id"]:guid,
            DatasetId:guid,
            QueryType:string,
            RootActivityId:guid,
            SKU:string,
            WorkspaceId:guid
      )
      | project GatewayObjectId
            ,RequestId
            ,DataSource=parse_json((replace_strings(dynamic_to_json(DataSource),dynamic(['["','"]','}","{','\\"']),dynamic(['[',']','},{','"']))))
            ,QueryTrackingId
            ,QueryExecutionStartTimeUTC
            ,OperationType
            ,QueryText
            ,Table
            ,EvaluationContext
            ,ServiceName
            ,CurrentActivityId
            ,ItemId = iff(isempty(DatasetId),["Dataflow Id"],DatasetId)
            ,QueryType
            ,RootActivityId = iff(isempty(RootActivityId),["Session Id"],RootActivityId)
            ,SKU
            ,WorkspaceId = iff(isempty(WorkspaceId),['Workspace Id'],WorkspaceId)
            ,HostContextType=["Host Context Type"]
            ,ConnectionIds=["Connection Ids"]
}
.create-or-alter function with (folder = "UpdatePolicyFunctions", docstring = "Function to extract data from QueryStartReport", skipvalidation = "true") parse_QueryStartReport() {
  ['GatewayReports-Raw']
    | where logType == "QueryStartReport"
    | project log
    | mv-expand log
    | evaluate bag_unpack(log) : (
      GatewayObjectId:guid,
      RequestId:guid,
      DataSource:dynamic,
      QueryTrackingId:guid,
      QueryExecutionStartTimeUTC:datetime,
      QueryType:string,
      QueryText:string,
      EvaluationContext:string
    )
    | invoke parse_QueryStartReport_csv()
}
.create-or-alter function with (folder = "UpdatePolicyFunctions", docstring = "Function to extract data from QueryExecutionReport", skipvalidation = "true") parse_QueryExecutionReport() {
  ['GatewayReports-Raw']
    | where logType == "QueryExecutionReport"
    | project log
    | mv-expand log
    | evaluate bag_unpack(log) : (
      GatewayObjectId:guid,
      RequestId:guid,
      DataSource:dynamic,
      QueryTrackingId:guid,
      QueryExecutionEndTimeUTC:datetime,
      ['QueryExecutionDuration(ms)']:long,
      QueryType:string,
      ['DataReadingAndSerializationDuration(ms)']:long,
      ['SpoolingDiskWritingDuration(ms)']:long,
      ['SpoolingDiskReadingDuration(ms)']:long,
      ['SpoolingTotalDataSize(byte)']:long,
      DataProcessingEndTimeUTC:datetime,
      ['DataProcessingDuration(ms)']:long,
      Success:string,
      ErrorMessage:string
      )
  | project GatewayObjectId,
    RequestId,
    DataSource=parse_json((replace_strings(dynamic_to_json(DataSource),dynamic(['["','"]','\\"']),dynamic(['[',']','"'])))),
    QueryTrackingId,
    QueryExecutionEndTimeUTC,
    ['QueryExecutionDuration(ms)'],
    QueryType,
    ['DataReadingAndSerializationDuration(ms)'],
    ['SpoolingDiskWritingDuration(ms)'],
    ['SpoolingDiskReadingDuration(ms)'],
    ['SpoolingTotalDataSize(byte)'],
    DataProcessingEndTimeUTC,
    ['DataProcessingDuration(ms)'],
    Success,
    ErrorMessage
}
.create-or-alter function with (folder = "UpdatePolicyFunctions", docstring = "Function to extract data from OpenConnectionReport", skipvalidation = "true") parse_OpenConnectionReport() {
  ['GatewayReports-Raw']
    | where logType == "OpenConnectionReport"
    | project log
    | mv-expand log
    | evaluate bag_unpack(log) : (
      GatewayObjectId:guid,
      DataSource:dynamic,
      OpenConnectionStartTimeUTC:datetime,
      OpenConnectionTrackingId:guid,
      ProviderName:string,
      RequestId:guid,
      ["OpenConnectionDuration(ms)"]:long,
      Success:string,
      ErrorMessage:string
      )
  | project GatewayObjectId,
    RequestId,
    DataSource=parse_json((replace_strings(dynamic_to_json(DataSource),dynamic(['["','"]','\\"']),dynamic(['[',']','"'])))),
    OpenConnectionStartTimeUTC,
    OpenConnectionTrackingId,
    ProviderName,
    ["OpenConnectionDuration(ms)"],
    Success,
    ErrorMessage
}
.create-or-alter function with (folder = "UpdatePolicyFunctions", docstring = "Function to extract data from QueryExecutionAggregationReport", skipvalidation = "true") parse_QueryExecutionAggregationReport() {
  ['GatewayReports-Raw']
    | where logType == "QueryExecutionAggregationReport"
    | project log
    | mv-expand log
    | evaluate bag_unpack(log) : (
      GatewayObjectId:guid,
      AggregationStartTimeUTC:datetime,
      AggregationEndTimeUTC:datetime,
      DataSource:dynamic,
      Success:string,
      AverageQueryExecutionDuration_ms_:real,
      MaxQueryExecutionDuration_ms_:long,
      MinQueryExecutionDuration_ms_:long,
      QueryType:string,
      AverageDataProcessingDuration_ms_:real,
      MaxDataProcessingDuration_ms_:long,
      MinDataProcessingDuration_ms_:long,
      Count:long
      )
  | project GatewayObjectId,
      AggregationStartTimeUTC,
      AggregationEndTimeUTC,
      DataSource=parse_json((replace_strings(dynamic_to_json(DataSource),dynamic(['["','"]','\\"']),dynamic(['[',']','"'])))),
      Success,
      AverageQueryExecutionDuration_ms_,
      MaxQueryExecutionDuration_ms_,
      MinQueryExecutionDuration_ms_,
      QueryType,
      AverageDataProcessingDuration_ms_,
      MaxDataProcessingDuration_ms_,
      MinDataProcessingDuration_ms_,
      Count
}
.create-or-alter function with (folder = "UpdatePolicyFunctions", docstring = "Function to extract data from QueryStreamingReport", skipvalidation = "true") parse_QueryStreamingReport() {
  ['GatewayReports-Raw']
    | where logType == "QueryStreamingReport"
    | project log
    | mv-expand log
    | evaluate bag_unpack(log) : (
      GatewayObjectId:guid,
      RequestId:guid,
      DataSource:dynamic,
      QueryTrackingId:guid,
      StreamingStartTimeUTC:datetime,
      SpoolingDiskReadingDuration_ms_:long,
      StreamingDuration_ms_:long,
      Success:string,
      ErrorMessage:string
    )
  | project GatewayObjectId,
      RequestId,
      DataSource=parse_json((replace_strings(dynamic_to_json(DataSource),dynamic(['["','"]','\\"']),dynamic(['[',']','"'])))),
      QueryTrackingId,
      StreamingStartTimeUTC,
      SpoolingDiskReadingDuration_ms_,
      StreamingDuration_ms_,
      Success,
      ErrorMessage
}
.create-or-alter function with (folder = "UpdatePolicyFunctions", docstring = "Function to extract data from MashupEvaluationReport", skipvalidation = "true") parse_MashupEvaluationReport() {
  ['GatewayReports-Raw']
    | where logType == "MashupEvaluationReport"
    | project log
    | mv-expand log
    | evaluate bag_unpack(log) : (
      GatewayObjectId:guid,
      QueryTrackingId:guid,
      ConnectionId:guid,
      DataSourceKinds:string,
      DataSourcePaths:string,
      EndTimeUTC:datetime,
      ['AverageCommit byte']:real ,
      AverageIODataBytesPerSecond:real,
      AveragePercentProcessorTime:real,
      ['AverageWorkingSet byte']:real,
      ['MaxCommit byte']:long,
      MaxIODataBytesPerSecond:long,
      MaxPercentProcessorTime:real,
      ['MaxWorkingSet byte']:long,
      TotalBytes:long,
      TotalProcessorTime:timespan,
      TotalRows:real
    )
}
.create-or-alter function with (folder = "UpdatePolicyFunctions", docstring = "Function to extract data from SystemCounterAggregationReport", skipvalidation = "true") parse_SystemCounterAggregationReport() {
  ['GatewayReports-Raw']
    | where logType == "SystemCounterAggregationReport"
    | project log
    | mv-expand log
    | evaluate bag_unpack(log) : (
      GatewayObjectId:guid ,
      AggregationStartTimeUTC:datetime ,
      AggregationEndTimeUTC:datetime,
      CounterName:string,
      Max:real,
      Min:real,
      Average:real
    )
}
.create-or-alter function with (folder = "UpdatePolicyFunctions", docstring = "Function to extract data from MashupEvaluationReport", skipvalidation = "true") parse_ConcurrentOperationAggregationReport() {
  ['GatewayReports-Raw']
    | where logType == "ConcurrentOperationAggregationReport"
    | project log
    | mv-expand log
    | evaluate bag_unpack(log) : (
      GatewayObjectId:guid,
      AggregationStartTimeUTC:datetime ,
      AggregationEndTimeUTC:datetime,
      CounterName:string,
      Max:real,
      Min:real,
      Average:real
    )
}
.create-or-alter function with (folder = "UpdatePolicyFunctions", docstring = "Function to extract data from GatewayNodeInfo", skipvalidation = "true") parse_GatewayNodeInfoRaw() {
  ['GatewayReports-Raw']
    | where logType == "GatewayNodeInfo"
    | project log,logDate
    | mv-expand log
    | evaluate bag_unpack(log) : (
      clusterId:guid,
      clusterName:string,
      nodeId:guid,
      machine:string, 
      cloudDatasourceRefresh:bool, 
      contactInformation:string, 
      customConnectors:bool,
      status:string,
      type:string,
      version:string,
      versionStatus:string,
      osName:string,
      osVersion:string,
      cores:long,
      logicalCores:long,
      memoryGb:real,
      logDate:datetime 
    )
}
.create-or-alter function with (folder = "UpdatePolicyFunctions", docstring = "Function to extract data from CopyExecutionReport", skipvalidation = "true") parse_CopyExecutionReport() {
  ['GatewayReports-Raw']
    | where logType == "CopyExecutionReport"
    | project log
    | mv-expand log
    | evaluate bag_unpack(log) : (
        GatewayObjectId:guid,
        ActivityId:guid, 
        CopyExecutionStartTimeUTC:datetime, 
        CompletionStatus:string, 
        ['CopyExecutionDuration(ms)']:decimal,    
        ['CopyWorkerCPUUtilization(%)']:decimal, 
        ['CopyWorkerMemoryUsage(byte)']:decimal
    )
    | project-rename ['CopyExecutionDuration_ms']=['CopyExecutionDuration(ms)'], 
    ['CopyWorkerCPUUtilizationPercentage']=['CopyWorkerCPUUtilization(%)'],
    ['CopyWorkerMemoryUsage_byte']=['CopyWorkerMemoryUsage(byte)']
}
.create-or-alter function with (folder = "Functions", docstring = "Function to merge the GatewayNodeInfo and Heartbeat", skipvalidation = "true") GatewayNodeStatusWithInfo() {
    GatewayNodeInfo
    | join kind=fullouter GatewayNodeStatus on nodeId
    | extend Responding=tobool(iff(datetime_diff('minute',now(),lastStatusDate)>1 or isempty(lastStatusDate),false,true))
    | project nodeId,logDate,clusterId,clusterName,machine,cloudDatasourceRefresh,contactInformation,customConnectors,status,type,version,versionStatus,osName,osVersion,cores,logicalCores,memoryGb,lastStatusDate,Responding
}
.create-or-alter function with (folder = "Functions", docstring = "Function to get Query Connections", skipvalidation = "true") QueryConnections(DateSearch:datetime, DaysBack:int, AllGateways:bool, Gateways:dynamic, AllClusters:bool, Clusters:dynamic) {
     let TimeFrom = datetime_add('day', -DaysBack, DateSearch);
     let TimeToStart = datetime_add('hour', 24, DateSearch);
     let TimeToExecute = datetime_add('hour', 30, DateSearch);
     let GatewaysToSeach = GatewayNodeInfo | where AllGateways or machine in(Gateways) | where AllClusters or clusterName in(Clusters) | project nodeId;
     let QueryExecution = QueryExecutionReport
        | where QueryExecutionEndTimeUTC >= TimeFrom and QueryExecutionEndTimeUTC <= TimeToExecute
        | where GatewayObjectId in(GatewaysToSeach);
     let QueryStart = QueryStartReport
        | where QueryExecutionStartTimeUTC >= TimeFrom and QueryExecutionStartTimeUTC <= TimeToStart
        | where GatewayObjectId in(GatewaysToSeach);
     QueryStart
    | mv-expand todynamic(DataSource)
    | extend DataSource=DataSource.kind, Path=DataSource.path
    | project QueryTrackingId, DataSource=tostring(DataSource), Path=tostring(Path)
 }
.create-or-alter function with (folder = "Functions", docstring = "Function to merge the Query Start and Execution", skipvalidation = "true") QueryExecutionUnified(DateSearch:datetime, DaysBack:int, AllGateways:bool, Gateways:dynamic, AllClusters:bool, Clusters:dynamic) {
     let TimeFrom = datetime_add('day', -DaysBack, DateSearch);
     let TimeToStart = datetime_add('hour', 24, DateSearch);
     let TimeToExecute = datetime_add('hour', 36, DateSearch);
     let GatewaysToSeach = GatewayNodeInfo | where AllGateways or machine in(Gateways) | where AllClusters or clusterName in(Clusters) | project nodeId;
     let QueryExecution = QueryExecutionReport
        | where QueryExecutionEndTimeUTC >= TimeFrom and QueryExecutionEndTimeUTC <= TimeToExecute
        | where GatewayObjectId in(GatewaysToSeach);
     let QueryStart = QueryStartReport
        | where QueryExecutionStartTimeUTC >= TimeFrom and QueryExecutionStartTimeUTC <= TimeToStart
        | where GatewayObjectId in(GatewaysToSeach);
     QueryStart
    | join kind=leftouter QueryExecution
        on
        $left.GatewayObjectId == $right.GatewayObjectId,
        $left.RequestId == $right.RequestId,
        $left.QueryTrackingId == $right.QueryTrackingId
    | lookup GatewayNodeInfo on $left.GatewayObjectId == $right.nodeId
    | project
        RootActivityId,
        GatewayObjectId, 
        ClusterName=clusterName,
        NodeName=machine,
        WorkspaceId, 
        ItemId, 
        QueryExecutionStartTimeUTC, 
        QueryExecutionEndTimeUTC,  
        QueryExecutionDuration_ms_,
        DataProcessingEndTimeUTC,
        DataProcessingDuration_ms_,
        RequestId, 
        QueryTrackingId,
        CurrentActivityId,
        Success,
        QueryText,
        Table,
        OperationType,
        QueryType,    
        DataSource,
        ServiceName,
        SKU,
        DataReadingAndSerializationDuration_ms_,
        SpoolingDiskWritingDuration_ms_,
        SpoolingDiskReadingDuration_ms_,
        SpoolingTotalDataSize_byte_,
        ErrorMessage,
        HostContextType,
        ConnectionIds
    | order by
        RootActivityId,
        GatewayObjectId,
        WorkspaceId,
        ItemId,
        QueryExecutionStartTimeUTC desc,
        RequestId,
        QueryTrackingId
 }
.create-or-alter function with (folder = "Functions", docstring = "Function to get SystemCounters", skipvalidation = "true") SystemCounters(DateSearch:datetime, DaysBack:int, AllGateways:bool, Gateways:dynamic, AllClusters:bool, Clusters:dynamic) {
     let TimeFrom = datetime_add('day', -DaysBack, DateSearch);
     let TimeTo = datetime_add('hour', 24, DateSearch);
     let GatewaysToSeach = GatewayNodeInfo
        | where AllGateways or machine in(Gateways)
        | where AllClusters or clusterName in(Clusters)
        | project nodeId;
     let SystemCounter = SystemCounterAggregationReport
        | where AggregationEndTimeUTC >= TimeFrom and AggregationEndTimeUTC <= TimeTo
        | where GatewayObjectId in(GatewaysToSeach);
     SystemCounter
    | lookup GatewayNodeInfo on $left.GatewayObjectId == $right.nodeId
    | project 
        ClusterId=clusterId,
        ClusterName=clusterName,
        NodeId=GatewayObjectId, 
        NodeName=machine,
        AggregationEndTimeUTC,
        CounterName,
        Max,
        Min,
        Average
 }
.create-or-alter materialized-view with (Folder = "Information")  GatewayNodeInfo on table GatewayNodeInfoRaw { GatewayNodeInfoRaw | summarize arg_max(logDate, clusterId,clusterName,machine,cloudDatasourceRefresh,contactInformation,customConnectors,status,type,version,versionStatus,osName,osVersion,cores,logicalCores,memoryGb) by nodeId }
.create-or-alter materialized-view with (Folder = "Information")  GatewayNodeStatus on table GatewaysHeartbeat { GatewaysHeartbeat | project-rename nodeId = GatewayId, lastStatusDate = ServerTimestampUTC | summarize arg_max(lastStatusDate,FileVersion,Responding) by nodeId }
.alter table GatewaysHeartbeat policy retention @'{"SoftDeletePeriod":"7.00:00:00","Recoverability":"Disabled"}'
.alter table GatewaysHeartbeat policy caching hotdata = time(1.00:00:00) hotindex = time(1.00:00:00)
.alter table GatewaysHeartbeat policy streamingingestion "{\"IsEnabled\":true,\"HintAllocatedRate\":null,\"NumberOfRowStores\":null,\"SealIntervalLimit\":null,\"SealThresholdBytes\":null,\"UsageTags\":[],\"IsMaintenanceActive\":false}"
.alter table ['GatewayReports-Raw'] policy retention @'{"SoftDeletePeriod":"1.00:00:00","Recoverability":"Disabled"}'
.alter table ['GatewayReports-Raw'] policy caching hotdata = time(1.00:00:00) hotindex = time(1.00:00:00)
.alter table ['GatewayReports-Raw'] policy streamingingestion "{\"IsEnabled\":true,\"HintAllocatedRate\":null,\"NumberOfRowStores\":null,\"SealIntervalLimit\":null,\"SealThresholdBytes\":null,\"UsageTags\":[],\"IsMaintenanceActive\":false}"
.alter table SystemCounterAggregationReport policy retention @'{"SoftDeletePeriod":"3650.00:00:00","Recoverability":"Enabled"}'
.alter table SystemCounterAggregationReport policy caching hotdata = time(90.00:00:00) hotindex = time(90.00:00:00)
.alter table SystemCounterAggregationReport policy update "[{\"IsEnabled\":true,\"Source\":\"GatewayReports-Raw\",\"Query\":\"parse_SystemCounterAggregationReport()\",\"IsTransactional\":true,\"PropagateIngestionProperties\":false,\"ManagedIdentity\":null}]"
.alter table QueryStartReport policy retention @'{"SoftDeletePeriod":"3650.00:00:00","Recoverability":"Enabled"}'
.alter table QueryStartReport policy caching hotdata = time(90.00:00:00) hotindex = time(90.00:00:00)
.alter table QueryStartReport policy update "[{\"IsEnabled\":true,\"Source\":\"GatewayReports-Raw\",\"Query\":\"parse_QueryStartReport()\",\"IsTransactional\":true,\"PropagateIngestionProperties\":false,\"ManagedIdentity\":null}]"
.alter table QueryExecutionReport policy retention @'{"SoftDeletePeriod":"3650.00:00:00","Recoverability":"Enabled"}'
.alter table QueryExecutionReport policy caching hotdata = time(90.00:00:00) hotindex = time(90.00:00:00)
.alter table QueryExecutionReport policy update "[{\"IsEnabled\":true,\"Source\":\"GatewayReports-Raw\",\"Query\":\"parse_QueryExecutionReport()\",\"IsTransactional\":true,\"PropagateIngestionProperties\":false,\"ManagedIdentity\":null}]"
.alter table OpenConnectionReport policy retention @'{"SoftDeletePeriod":"3650.00:00:00","Recoverability":"Enabled"}'
.alter table OpenConnectionReport policy caching hotdata = time(90.00:00:00) hotindex = time(90.00:00:00)
.alter table OpenConnectionReport policy update "[{\"IsEnabled\":true,\"Source\":\"GatewayReports-Raw\",\"Query\":\"parse_OpenConnectionReport()\",\"IsTransactional\":true,\"PropagateIngestionProperties\":false,\"ManagedIdentity\":null}]"
.alter table QueryExecutionAggregationReport policy retention @'{"SoftDeletePeriod":"3650.00:00:00","Recoverability":"Enabled"}'
.alter table QueryExecutionAggregationReport policy caching hotdata = time(90.00:00:00) hotindex = time(90.00:00:00)
.alter table QueryExecutionAggregationReport policy update "[{\"IsEnabled\":true,\"Source\":\"GatewayReports-Raw\",\"Query\":\"parse_QueryExecutionAggregationReport()\",\"IsTransactional\":true,\"PropagateIngestionProperties\":false,\"ManagedIdentity\":null}]"
.alter table QueryStreamingReport policy retention @'{"SoftDeletePeriod":"3650.00:00:00","Recoverability":"Enabled"}'
.alter table QueryStreamingReport policy caching hotdata = time(90.00:00:00) hotindex = time(90.00:00:00)
.alter table QueryStreamingReport policy update "[{\"IsEnabled\":true,\"Source\":\"GatewayReports-Raw\",\"Query\":\"parse_QueryStreamingReport()\",\"IsTransactional\":true,\"PropagateIngestionProperties\":false,\"ManagedIdentity\":null}]"
.alter table MashupEvaluationReport policy retention @'{"SoftDeletePeriod":"3650.00:00:00","Recoverability":"Enabled"}'
.alter table MashupEvaluationReport policy caching hotdata = time(90.00:00:00) hotindex = time(90.00:00:00)
.alter table MashupEvaluationReport policy update "[{\"IsEnabled\":true,\"Source\":\"GatewayReports-Raw\",\"Query\":\"parse_MashupEvaluationReport()\",\"IsTransactional\":true,\"PropagateIngestionProperties\":false,\"ManagedIdentity\":null}]"
.alter table ConcurrentOperationAggregationReport policy retention @'{"SoftDeletePeriod":"3650.00:00:00","Recoverability":"Enabled"}'
.alter table ConcurrentOperationAggregationReport policy caching hotdata = time(90.00:00:00) hotindex = time(90.00:00:00)
.alter table ConcurrentOperationAggregationReport policy update "[{\"IsEnabled\":true,\"Source\":\"GatewayReports-Raw\",\"Query\":\"parse_ConcurrentOperationAggregationReport()\",\"IsTransactional\":true,\"PropagateIngestionProperties\":false,\"ManagedIdentity\":null}]"
.alter table GatewayNodeInfoRaw policy retention @'{"SoftDeletePeriod":"3650.00:00:00","Recoverability":"Enabled"}'
.alter table GatewayNodeInfoRaw policy caching hotdata = time(90.00:00:00) hotindex = time(90.00:00:00)
.alter table GatewayNodeInfoRaw policy update "[{\"IsEnabled\":true,\"Source\":\"GatewayReports-Raw\",\"Query\":\"parse_GatewayNodeInfoRaw()\",\"IsTransactional\":true,\"PropagateIngestionProperties\":false,\"ManagedIdentity\":null}]"
.alter table CopyExecutionReport policy retention @'{"SoftDeletePeriod":"3650.00:00:00","Recoverability":"Enabled"}'
.alter table CopyExecutionReport policy caching hotdata = time(90.00:00:00) hotindex = time(90.00:00:00)
.alter table CopyExecutionReport policy update "[{\"IsEnabled\":true,\"Source\":\"GatewayReports-Raw\",\"Query\":\"parse_CopyExecutionReport()\",\"IsTransactional\":true,\"PropagateIngestionProperties\":false,\"ManagedIdentity\":null}]"
