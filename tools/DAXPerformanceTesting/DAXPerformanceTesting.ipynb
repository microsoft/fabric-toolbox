{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAX Performance Testing\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook is designed to measure DAX query timings under different cache states (cold, warm, and hot).\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **DAX Queries from Excel or YAML**  \n",
    "   - You must provide an Excel file containing the DAX queries in a table you wish to test.  \n",
    "   - For each query, a column needs align with the `runQueryType` used for a given `queryId`.  \n",
    "   - This notebook reads those queries and executes them on one or more Power BI/Fabric models.\n",
    "   - For Yaml files please refer to the `sample-yaml-dax-query-file.yaml` file in the media folder\n",
    "\n",
    "\n",
    "2. **Lakehouse Logging**  \n",
    "   - You also must attach the appropriate Lakehouse in Fabric so that logs can be saved (both in a table and as files if you choose).  \n",
    "\n",
    "3. **Capacity Pause/Resume**  \n",
    "   - In some scenarios (e.g., simulating a \"cold\" cache on DirectQuery or Import models), the code pauses and resumes capacities.  \n",
    "   - **Warning**: Pausing a capacity will interrupt any running workloads on that capacity. Resuming will take time and resources, and can affect other workspaces assigned to the same capacity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the latest Semantic Link Labs package\n",
    "\n",
    "Check [here](https://pypi.org/project/semantic-link-labs/) to see the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the library and necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import time\n",
    "import itertools\n",
    "import functools\n",
    "import builtins\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from threading import local\n",
    "from typing import Any, Callable, Generator, Type\n",
    "from contextlib import contextmanager\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pyspark.sql.functions import col, sum as _sum, when, countDistinct\n",
    "\n",
    "# Local Application/Library-specific Imports\n",
    "import sempy.fabric as fabric\n",
    "import sempy_labs as labs\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global configurations & variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique run ID for this test run\n",
    "run_id = str(uuid4())\n",
    "\n",
    "query_file_path = \"Files/DAXQueries.xlsx\"  # Path to the query file relative to the mount\n",
    "query_file_mount_path = \"/default\"              # Mount location where the file is stored\n",
    "query_worksheet_name = \"DAXQueries\"          # Worksheet name (for Excel files)\n",
    "\n",
    "# Define models and their configurations for testing\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"Model Name\", # The name of the semantic model\n",
    "        \"storageMode\": \"DirectLake\",  # Import, DirectQuery, or DirectLake\n",
    "        \"cache_types\": [\"cold\", \"warm\", \"hot\"], # List of cache types to be run (hot, warm, and cold)\n",
    "        \"model_workspace_name\": \"Model Workspace Name\", # The workspace name of the semantic model\n",
    "        \"database_name\": \"Lakehouse Name\",  # Only needed for cold cache queries for Import and DirectQuery\n",
    "        \"database_workspace_name\": \"Lakehouse Workspace Name\",  # Only needed for cold cache queries for Import and DirectQuery\n",
    "        \"runQueryType\": \"query\", # The name of the column in your DAX Excel file contains the query to be run\n",
    "    },\n",
    "]\n",
    "\n",
    "# Only needed for cold cache queries for Import and DirectQuery\n",
    "workspace_capacities = {\n",
    "    \"Workspace Name\": {\n",
    "        \"capacity_name\": \"Testing Capacity Name\",\n",
    "        \"alt_capacity_name\": \"Alternate Capacity Name\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Additional arguments controlling the behavior of query execution and logging\n",
    "additional_arguments = {\n",
    "    \"roundNumber\": 1, # The current round of DAX testing. Will be considered when determine if maxNumberPerQuery is met or not\n",
    "    \"onlyRunNewQueries\": True, # Will determine if queries will stop being tested after maxNumberPerQuery is met\n",
    "    \"maxNumberPerQuery\": 1, # The max number of queries to capture per round, queryId, model and cache type\n",
    "    \"maxFailuresBeforeSkipping\": 5, # The number of failed query attempts per round, queryId, model and cache type before skipping\n",
    "    \"numberOfRunsPerQueryId\": 15, # The number of times to loop over each queryId. If all combos have met maxNumberPerQuery, the loop will break\n",
    "    \"stopQueryIdsAt\": 99, # Allows you to stop the queryId loop at a certain number, even if there are more queries present, i.e., there are queryIds 1-20 but stop at 5\n",
    "    \"forceStartQueriesAt1\": False, # If set to False, testing will stop at the first incomplete queryId instead of starting at queryId 1  \n",
    "    \"logTableName\": \"DAXTestingLogTableName\", # The name of the table in the attached lakehouse to save the performance logs to\n",
    "    \"clearAllLogs\": False, # Will drop the existing logs table before starting testing\n",
    "    \"clearCurrentRoundLogs\": False, # Will delete the logs associated with the current roundNumber before starting testing\n",
    "    \"randomizeRuns\": True, # Will randomize the model and cache type combos when testing\n",
    "    \"skipSettingHotCache\": False, # Should be False if randomizing the runs. If the runs are randomized, the previous warm cache run will set the hot cache\n",
    "    \"pauseAfterSettingCache\": 5, # The number of seconds to wait after setting the cache\n",
    "    \"pauseAfterRunningQuery\": 5, # The number of second to wait before writing the logs to the log table\n",
    "    \"pauseBetweenRuns\": 30, # The number of seconds to wait before starting the next query\n",
    "}\n",
    "\n",
    "# Define the expected schema for DAX trace log events\n",
    "event_schema = {\n",
    "    \"DirectQueryBegin\": [\n",
    "        \"EventClass\", \"CurrentTime\", \"TextData\", \"StartTime\", \n",
    "        \"EndTime\", \"Duration\", \"CpuTime\", \"Success\", \"SessionID\"\n",
    "    ],\n",
    "    \"DirectQueryEnd\": [\n",
    "        \"EventClass\", \"CurrentTime\", \"TextData\", \"StartTime\", \n",
    "        \"EndTime\", \"Duration\", \"CpuTime\", \"Success\", \"SessionID\"\n",
    "    ],\n",
    "    \"VertiPaqSEQueryBegin\": [\n",
    "        \"EventClass\", \"EventSubclass\", \"CurrentTime\", \n",
    "        \"TextData\", \"StartTime\", \"SessionID\"\n",
    "    ],\n",
    "    \"VertiPaqSEQueryEnd\": [\n",
    "        \"EventClass\", \"EventSubclass\", \"CurrentTime\", \"TextData\", \n",
    "        \"StartTime\", \"EndTime\", \"Duration\", \"CpuTime\", \"Success\", \"SessionID\"\n",
    "    ],\n",
    "    \"VertiPaqSEQueryCacheMatch\": [\n",
    "        \"EventClass\", \"EventSubclass\", \"CurrentTime\", \"TextData\", \"SessionID\"\n",
    "    ],\n",
    "    \"QueryBegin\": [\n",
    "        \"EventClass\", \"EventSubclass\", \"CurrentTime\", \"TextData\", \n",
    "        \"StartTime\", \"ConnectionID\", \"SessionID\", \"RequestProperties\"\n",
    "    ],\n",
    "    \"QueryEnd\": [\n",
    "        \"EventClass\", \"EventSubclass\", \"CurrentTime\", \"TextData\", \n",
    "        \"StartTime\", \"EndTime\", \"Duration\", \"CpuTime\", \"Success\", \n",
    "        \"ConnectionID\", \"SessionID\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Dictionary to track if a capacity pause is needed for each model during testing\n",
    "model_pause_capacity_needed = {}\n",
    "\n",
    "# Variables for Pausing/Resuming Capacities: credentials and configuration parameters for Azure Key Vault and resource management\n",
    "resource_group_name = \"\"\n",
    "subscription_id = \"\"\n",
    "key_vault_uri_secret_name = \"\"\n",
    "key_vault_client_id_secret_name = \"\"\n",
    "key_vault_tenant_id_secret_name = \"\"\n",
    "key_vault_client_secret_secret_name = \"\"\n",
    "\n",
    "# Enforce case-sensitivity in Spark to ensure column name matching is exact\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging & Retry Decorators, Basic Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread-local storage variable to manage indentation for logging.\n",
    "_thread_local = local()\n",
    "\n",
    "@contextmanager\n",
    "def dynamic_indented_print() -> Generator[None, None, None]:\n",
    "    \"\"\"\n",
    "    A context manager that overrides the built-in print function to automatically\n",
    "    indent messages based on the current call depth. This helps visualize nested logs.\n",
    "    \"\"\"\n",
    "    original_print = builtins.print\n",
    "\n",
    "    def custom_print(*args: Any, **kwargs: Any) -> None:\n",
    "        # Use current call depth (default 0) to set indentation.\n",
    "        depth = getattr(_thread_local, \"call_depth\", 0)\n",
    "        indent = \"    \" * depth\n",
    "        original_print(indent + \" \".join(map(str, args)), **kwargs)\n",
    "\n",
    "    builtins.print = custom_print\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        builtins.print = original_print\n",
    "\n",
    "def log_function_calls(func: Callable) -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator that logs the start and end of a function call.\n",
    "    It increases the indentation for nested function calls for better readability.\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args: Any, **kwargs: Any) -> Any:\n",
    "        if not hasattr(_thread_local, \"call_depth\"):\n",
    "            _thread_local.call_depth = 0\n",
    "\n",
    "        with dynamic_indented_print():\n",
    "            print(f\"‚úÖ {func.__name__} - Starting\")\n",
    "            _thread_local.call_depth += 1  # Increase indentation level for inner calls\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "            finally:\n",
    "                _thread_local.call_depth -= 1  # Decrease indentation level when done\n",
    "                print(f\"‚úÖ {func.__name__} - Ending\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def retry(exceptions: tuple[Type[Exception], ...],\n",
    "          tries: int = 3,\n",
    "          delay: int = 5,\n",
    "          backoff: int = 2,\n",
    "          logger: Callable = print) -> Callable:\n",
    "    \"\"\"\n",
    "    A decorator for retrying a function call with exponential backoff.\n",
    "    This is useful for transient errors (e.g., waiting for a capacity status update).\n",
    "    \"\"\"\n",
    "    def decorator_retry(func: Callable) -> Callable:\n",
    "        @functools.wraps(func)\n",
    "        def wrapper_retry(*args: Any, **kwargs: Any) -> Any:\n",
    "            _tries, _delay = tries, delay\n",
    "            while _tries > 1:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except exceptions as e:\n",
    "                    with dynamic_indented_print():\n",
    "                        print(f\"‚ö†Ô∏è {func.__name__} failed with {e}, retrying in {_delay} seconds...\")\n",
    "                    time.sleep(_delay)\n",
    "                    _tries -= 1\n",
    "                    _delay *= backoff  # Increase the delay for each retry\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper_retry\n",
    "    return decorator_retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_camel_case(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a string to camel case.\n",
    "    For example, 'Event Class' becomes 'eventClass'.\n",
    "    \"\"\"\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    return words[0].lower() + ''.join(word.capitalize() for word in words[1:])\n",
    "\n",
    "@log_function_calls\n",
    "def trace_started(traces, trace_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a specific trace (by name) has started by inspecting the traces DataFrame.\n",
    "    \"\"\"\n",
    "    return traces.loc[traces[\"Name\"] == trace_name].shape[0] > 0\n",
    "\n",
    "@log_function_calls\n",
    "@retry(Exception, tries=10, delay=2, backoff=2, logger=print)\n",
    "def wait_for_trace_start(trace_connection, trace_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Polls until the trace with the specified name is detected as started.\n",
    "    Raises an exception if not started, triggering a retry.\n",
    "    \"\"\"\n",
    "    if not trace_started(trace_connection.list_traces(), trace_name):\n",
    "        raise Exception(\"Trace has not started yet\")\n",
    "    return True\n",
    "\n",
    "def extract_session_id_from_query_begin(logs) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the Session ID from QueryBegin events that include 'SemPy' in their Request Properties.\n",
    "    This is used to match corresponding QueryEnd events.\n",
    "    \"\"\"\n",
    "    query_begin_events = logs[logs[\"Event Class\"] == \"QueryBegin\"]\n",
    "    for idx, row in query_begin_events.iterrows():\n",
    "        req_props = row.get(\"Request Properties\")\n",
    "        if req_props and \"SemPy\" in req_props:\n",
    "            try:\n",
    "                root = ET.fromstring(req_props)\n",
    "                for child in root:\n",
    "                    if child.tag.endswith(\"SspropInitAppName\") and child.text == \"SemPy\":\n",
    "                        return row[\"Session ID\"]\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing Request Properties XML: {e}\")\n",
    "    return None\n",
    "\n",
    "@log_function_calls\n",
    "@retry(Exception, tries=60, delay=3, backoff=1, logger=print)\n",
    "def wait_for_query_end_event(trace) -> tuple:\n",
    "    \"\"\"\n",
    "    Waits until a QueryBegin event with 'SemPy' is detected and its corresponding QueryEnd event is present.\n",
    "    Returns a tuple of (session_id, logs).\n",
    "    \"\"\"\n",
    "    logs = trace.get_trace_logs()\n",
    "    session_id = extract_session_id_from_query_begin(logs)\n",
    "    if not session_id:\n",
    "        raise Exception(\"QueryBegin event with SspropInitAppName 'SemPy' not found yet\")\n",
    "    \n",
    "    query_end_events = logs[(logs[\"Event Class\"] == \"QueryEnd\") & (logs[\"Session ID\"] == session_id)]\n",
    "    if query_end_events.empty:\n",
    "        raise Exception(\"QueryEnd event for Session ID not collected yet\")\n",
    "    return session_id, logs\n",
    "\n",
    "@log_function_calls\n",
    "def collect_filtered_trace_logs(trace):\n",
    "    \"\"\"\n",
    "    Waits for the QueryEnd event, stops the trace, and filters the logs for the relevant Session ID.\n",
    "    This returns only the logs related to the current query execution.\n",
    "    \"\"\"\n",
    "    session_id, _ = wait_for_query_end_event(trace)\n",
    "    logs = trace.stop()\n",
    "    filtered_logs = logs[logs[\"Session ID\"] == session_id]\n",
    "    return filtered_logs\n",
    "\n",
    "@log_function_calls\n",
    "@retry(Exception, tries=30, delay=5, backoff=1, logger=print)\n",
    "def check_model_online(model: dict) -> None:\n",
    "    \"\"\"\n",
    "    Checks if the given model is online by executing a simple DAX query.\n",
    "    If the query fails, an exception is raised.\n",
    "    \"\"\"\n",
    "    dax_query_eval_1(model)\n",
    "\n",
    "@log_function_calls\n",
    "def get_capacity_status(_capacity_name):\n",
    "    \"\"\"\n",
    "    Retrieves the current status of a capacity by its name.\n",
    "    \"\"\"\n",
    "    capacity_info = labs.list_capacities()\n",
    "    return capacity_info.loc[\n",
    "        capacity_info[\"Display Name\"] == _capacity_name, \"State\"\n",
    "    ].iloc[0]\n",
    "\n",
    "@log_function_calls\n",
    "@retry(Exception, tries=30, delay=2, backoff=2, logger=print)\n",
    "def wait_for_capacity_status(_capacity_name, target_status):\n",
    "    \"\"\"\n",
    "    Polls until the capacity status matches the target status.\n",
    "    \"\"\"\n",
    "    current_status = get_capacity_status(_capacity_name)\n",
    "    \n",
    "    if current_status != target_status:\n",
    "        raise Exception(\"Capacity status not updated yet\")\n",
    "    return current_status\n",
    "\n",
    "def dax_query_eval_1(model: dict) -> None:\n",
    "    \"\"\"\n",
    "    Executes a simple DAX query (\"EVALUATE {1}\") to verify connectivity and responsiveness.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fabric.evaluate_dax(\n",
    "            model[\"name\"], \"EVALUATE {1}\", workspace=model[\"model_workspace_name\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Failed to query model\") from e\n",
    "\n",
    "@log_function_calls\n",
    "def wait_for_model_to_come_online(model: dict) -> None:\n",
    "    \"\"\"\n",
    "    Waits until the model is online by calling check_model_online.\n",
    "    Raises an exception if the model fails to come online.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        check_model_online(model)\n",
    "        print(\"‚úÖ Model is online\")\n",
    "    except Exception as e:\n",
    "        raise Exception(\"‚ùå Model failed to come online\") from e\n",
    "\n",
    "@log_function_calls\n",
    "def load_dax_queries(file_path: str, mount_path: str, worksheet_name: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the DAX queries from the given file. Supports Excel and YAML formats.\n",
    "    The first column must be 'queryId' and additional columns should contain variants of the DAX query.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Relative path to the query file.\n",
    "        mount_path (str): The mount path where the file is stored.\n",
    "        worksheet_name (str, optional): Worksheet name for Excel files.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the DAX queries.\n",
    "    \"\"\"\n",
    "    file_type = file_path.split('.')[-1].lower()\n",
    "    full_path = f\"{notebookutils.fs.getMountPath(mount_path)}/{file_path}\"\n",
    "    \n",
    "    if file_type == 'xlsx':\n",
    "        # Use the worksheet_name if provided, defaulting to the first sheet otherwise.\n",
    "        return pd.read_excel(full_path, sheet_name=worksheet_name)\n",
    "    elif file_type in ['yml', 'yaml']:\n",
    "        with open(full_path, 'r') as f:\n",
    "            data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pause & Resume Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def update_model_pause_status(event: str, model: dict = None, workspace: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Updates the dictionary that tracks if a capacity pause is needed for each model.\n",
    "    Different events trigger different updates:\n",
    "      - \"initialize\": Set default based on storageMode.\n",
    "      - \"model_queried\": Mark model as needing a pause for Import/DirectQuery.\n",
    "      - \"capacity_paused\": Reset the pause flag after a capacity operation.\n",
    "    \"\"\"\n",
    "    global model_pause_capacity_needed\n",
    "\n",
    "    if event == \"initialize\":\n",
    "        for m in models:\n",
    "            model_pause_capacity_needed[m[\"name\"]] = False if m[\"storageMode\"] == \"DirectLake\" else True\n",
    "\n",
    "    elif event == \"model_queried\" and model is not None:\n",
    "        if model[\"storageMode\"] == \"Import\":\n",
    "            model_pause_capacity_needed[model[\"name\"]] = True\n",
    "        elif model[\"storageMode\"] == \"DirectQuery\":\n",
    "            target_db = model[\"database_name\"]\n",
    "            target_db_workspace = model[\"database_workspace_name\"]\n",
    "            for m in models:\n",
    "                if (\n",
    "                    m[\"storageMode\"] == \"DirectQuery\" and \n",
    "                    m[\"database_name\"] == target_db and \n",
    "                    m[\"database_workspace_name\"] == target_db_workspace\n",
    "                ):\n",
    "                    model_pause_capacity_needed[m[\"name\"]] = True\n",
    "\n",
    "    elif event == \"capacity_paused\" and workspace is not None:\n",
    "        # Reset pause flags after capacity operations complete\n",
    "        for m in models:\n",
    "            if m[\"model_workspace_name\"] == workspace and m[\"storageMode\"] == \"Import\":\n",
    "                model_pause_capacity_needed[m[\"name\"]] = False\n",
    "        for m in models:\n",
    "            if m[\"storageMode\"] == \"DirectQuery\" and m[\"database_workspace_name\"] == workspace:\n",
    "                model_pause_capacity_needed[m[\"name\"]] = False\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Unknown event '{event}' or missing required parameter(s).\")\n",
    "\n",
    "    print(\"üìù Updated model_pause_capacity_needed\")\n",
    "\n",
    "@log_function_calls\n",
    "@retry(Exception, tries=10, delay=5, backoff=2, logger=print)\n",
    "def pause_resume_capacity(_capacity_name, _action, _simplify_logs=False):\n",
    "    \"\"\"\n",
    "    Pauses or resumes a capacity based on the _action parameter.\n",
    "    Uses Semantic Link Labs functions along with key vault authentication.\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ {_action.title()} capacity '{_capacity_name}': Attempting\")\n",
    "\n",
    "    # Check the current status of the capacity.\n",
    "    current_status = get_capacity_status(_capacity_name)\n",
    "    \n",
    "    # Mapping for actions with expected and target statuses.\n",
    "    action_options = {\n",
    "        \"pause\": {\n",
    "            \"expected_status\": \"Active\",  # Can only pause if capacity is active.\n",
    "            \"target_status\": \"Suspended\",\n",
    "            \"action_function\": labs.suspend_fabric_capacity,\n",
    "        },\n",
    "        \"resume\": {\n",
    "            \"expected_status\": \"Suspended\",  # Can only resume if capacity is suspended.\n",
    "            \"target_status\": \"Active\",\n",
    "            \"action_function\": labs.resume_fabric_capacity,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    if current_status == action_options[_action][\"expected_status\"]:\n",
    "        print(f\"üõ†Ô∏è {_action.title()} capacity '{_capacity_name}': Requesting action\")\n",
    "        \n",
    "        # Authenticate using service principal details from the key vault.\n",
    "        with labs.service_principal_authentication(\n",
    "            key_vault_uri=key_vault_uri_secret_name,\n",
    "            key_vault_tenant_id=key_vault_tenant_id_secret_name,\n",
    "            key_vault_client_id=key_vault_client_id_secret_name,\n",
    "            key_vault_client_secret=key_vault_client_secret_secret_name\n",
    "        ):\n",
    "            # Execute the pause or resume action.\n",
    "            action_options[_action][\"action_function\"](\n",
    "                capacity_name=_capacity_name,\n",
    "                azure_subscription_id=subscription_id,\n",
    "                resource_group=resource_group_name\n",
    "            )\n",
    "        \n",
    "        # Wait for the capacity status to reach the target status.\n",
    "        try:\n",
    "            wait_for_capacity_status(_capacity_name, action_options[_action][\"target_status\"])\n",
    "            print(f\"‚úÖ {_action.title()} capacity '{_capacity_name}': Action successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è {_action.title()} capacity '{_capacity_name}': Timeout waiting for target status. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è {_action.title()} capacity '{_capacity_name}': Already '{current_status}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache-setting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "@retry(exceptions=(Exception,), tries=5, delay=5, backoff=2)\n",
    "def clear_vertipaq_cache(model: dict) -> None:\n",
    "    \"\"\"\n",
    "    Clears the VertiPaq cache by calling labs.clear_cache.\n",
    "    Verifies that the cache clear succeeded by evaluating a simple DAX query.\n",
    "    \"\"\"\n",
    "    print(\"üßπ Clearing VertiPaq cache\")\n",
    "    wait_for_model_to_come_online(model)\n",
    "    try:\n",
    "        labs.clear_cache(model[\"name\"], workspace=model[\"model_workspace_name\"])\n",
    "        dax_query_eval_1(model)\n",
    "        print(\"‚úÖ Clear VertiPaq cache successful\")\n",
    "    except Exception as e:\n",
    "        print(\"üîÑ Clearing VertiPaq cache failed; retrying...\")\n",
    "        fabric.refresh_tom_cache(model[\"model_workspace_name\"])\n",
    "        raise e\n",
    "    time.sleep(5)\n",
    "\n",
    "@log_function_calls\n",
    "def set_hot_cache(model: dict, expression: str, successful_query_count_goal: int = 1) -> bool:\n",
    "    \"\"\"\n",
    "    Primes the hot cache by executing the given query multiple times.\n",
    "    Returns True if the required number of successful executions is reached.\n",
    "    \"\"\"\n",
    "    print(\"üî• Setting Hot Cache\")\n",
    "    successful_query_count = 0\n",
    "    # Define how many attempts to make based on the goal\n",
    "    number_of_query_attempts = successful_query_count_goal * 5 if successful_query_count_goal > 1 else 1\n",
    "\n",
    "    if additional_arguments[\"skipSettingHotCache\"]:\n",
    "        # Skip actual execution if flag is set\n",
    "        successful_query_count = successful_query_count_goal\n",
    "    else:\n",
    "        for _ in range(number_of_query_attempts):\n",
    "            # Clear any existing traces before running the query\n",
    "            fabric.create_trace_connection(model[\"name\"], model[\"model_workspace_name\"]).drop_traces()\n",
    "            trace_name = f\"Cache Trace {uuid4()}\"\n",
    "            with fabric.create_trace_connection(model[\"name\"], model[\"model_workspace_name\"]) as trace_conn:\n",
    "                with trace_conn.create_trace(event_schema, trace_name) as trace:\n",
    "                    print(\"üîç Starting trace for hot cache\")\n",
    "                    trace.start()\n",
    "                    wait_for_trace_start(trace_conn, trace_name)\n",
    "                    try:\n",
    "                        print(\"‚ö° Executing DAX query for hot cache\")\n",
    "                        fabric.evaluate_dax(model[\"name\"], expression, workspace=model[\"model_workspace_name\"])\n",
    "                        successful_query_count += 1\n",
    "                        print(\"‚úÖ DAX query succeeded for hot cache\")\n",
    "                    except Exception as e:\n",
    "                        print(\"‚ùå DAX query failed for hot cache:\", e)\n",
    "                    print(\"üìú Collecting trace logs for hot cache\")\n",
    "                    collect_filtered_trace_logs(trace)\n",
    "            # Exit early if enough successful queries have been executed\n",
    "            if successful_query_count >= successful_query_count_goal:\n",
    "                break\n",
    "\n",
    "    print(f\"‚úÖ Hot cache set; goal: {successful_query_count_goal} successful queries\")\n",
    "    return successful_query_count >= successful_query_count_goal\n",
    "\n",
    "@log_function_calls\n",
    "def set_warm_cache(model: dict, expression: str) -> bool:\n",
    "    \"\"\"\n",
    "    Sets a warm cache by first priming the hot cache then clearing the VertiPaq cache.\n",
    "    For DirectQuery models, it calls set_cold_cache.\n",
    "    \"\"\"\n",
    "    print(\"üî• Setting Warm Cache\")\n",
    "    if model[\"storageMode\"] == \"DirectQuery\":\n",
    "        set_cold_cache(model)\n",
    "    hot_cache_successful = set_hot_cache(model, expression)\n",
    "    clear_vertipaq_cache(model)\n",
    "    print(\"‚úÖ Warm cache set\")\n",
    "    return hot_cache_successful\n",
    "\n",
    "@retry(exceptions=(Exception,), tries=5, delay=5, backoff=2)\n",
    "def _refresh_tom_cache(workspace_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Refreshes the TOM cache for the specified workspace.\n",
    "    \"\"\"\n",
    "    print(f\"‚åõ Refreshing TOM cache for workspace '{workspace_name}'\")\n",
    "    fabric.refresh_tom_cache(workspace_name)\n",
    "    print(\"‚úÖ TOM cache refreshed successfully\")\n",
    "\n",
    "@retry(exceptions=(Exception,), tries=30, delay=3, backoff=2)\n",
    "def _wait_for_refresh_to_complete(model: dict, refresh_id: str) -> None:\n",
    "    \"\"\"\n",
    "    Waits for a dataset refresh to complete or fail by checking its status.\n",
    "    \"\"\"\n",
    "    status = fabric.get_refresh_execution_details(\n",
    "        model[\"name\"],\n",
    "        refresh_id,\n",
    "        workspace=model[\"model_workspace_name\"],\n",
    "    ).status\n",
    "\n",
    "    if status not in [\"Completed\", \"Failed\"]:\n",
    "        raise Exception(f\"Refresh status is '{status}'; not done yet.\")\n",
    "    print(f\"‚úÖ Refresh status: '{status}' - finishing polling.\")\n",
    "\n",
    "@log_function_calls\n",
    "def set_cold_cache(model: dict) -> None:\n",
    "    \"\"\"\n",
    "    Sets a cold cache by ensuring that any previous cache is cleared.\n",
    "    For non-DirectLake models, it pauses/resumes capacities to simulate a cold state.\n",
    "    For DirectLake, it performs a clear and full refresh.\n",
    "    \"\"\"\n",
    "    print(\"‚ùÑÔ∏è Setting Cold Cache\")\n",
    "    if model[\"storageMode\"] != \"DirectLake\":\n",
    "        if model_pause_capacity_needed.get(model[\"name\"], False):\n",
    "            ws_caps = workspace_capacities[model[\"model_workspace_name\"]]\n",
    "            print(f\"üîÑ Assigning alternate capacity for workspace '{model['model_workspace_name']}'\")\n",
    "            labs.assign_workspace_to_capacity(ws_caps[\"alt_capacity_name\"], model[\"model_workspace_name\"])\n",
    "            print(f\"‚úÖ Alternate capacity assigned: {ws_caps['alt_capacity_name']}\")\n",
    "\n",
    "            pause_resume_capacity(ws_caps[\"capacity_name\"], \"pause\")\n",
    "            pause_resume_capacity(ws_caps[\"capacity_name\"], \"resume\")\n",
    "\n",
    "            print(f\"üîÑ Reassigning primary capacity for workspace '{model['model_workspace_name']}'\")\n",
    "            labs.assign_workspace_to_capacity(ws_caps[\"capacity_name\"], model[\"model_workspace_name\"])\n",
    "            print(f\"‚úÖ Primary capacity assigned: {ws_caps['capacity_name']}\")\n",
    "\n",
    "            _refresh_tom_cache(model[\"model_workspace_name\"])\n",
    "            wait_for_model_to_come_online(model)\n",
    "            update_model_pause_status(\"capacity_paused\", workspace=model[\"model_workspace_name\"])\n",
    "            time.sleep(30)\n",
    "            clear_vertipaq_cache(model)\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Performing clear refresh for cold cache\")\n",
    "        refresh_status_clear = fabric.refresh_dataset(\n",
    "            model[\"name\"],\n",
    "            refresh_type=\"clearValues\",\n",
    "            workspace=model[\"model_workspace_name\"],\n",
    "        )\n",
    "        _wait_for_refresh_to_complete(model, refresh_status_clear)\n",
    "        print(\"‚úÖ Clear refresh completed; performing full refresh\")\n",
    "        refresh_status_full = fabric.refresh_dataset(\n",
    "            model[\"name\"],\n",
    "            refresh_type=\"full\",\n",
    "            workspace=model[\"model_workspace_name\"],\n",
    "        )\n",
    "        _wait_for_refresh_to_complete(model, refresh_status_full)\n",
    "        clear_vertipaq_cache(model)\n",
    "    print(\"‚úÖ Cold cache set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-table helpers & query checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def get_log_table(table_name: str):\n",
    "    \"\"\"\n",
    "    Retrieves the log table as a Spark DataFrame and applies filters based on the current round and event class.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw_table = spark.table(table_name)\n",
    "        base_filters = (\n",
    "            (col(\"roundNumber\") == additional_arguments[\"roundNumber\"]) &\n",
    "            (col(\"eventClass\") == \"QueryEnd\")\n",
    "        )\n",
    "        return raw_table.filter(base_filters)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "@log_function_calls\n",
    "def max_queries_met(check_logs: bool, log_table, model_cache_combo: dict, queryId: int) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if the maximum number of queries (successful or failed) has been met for the given model, cache type, and queryId.\n",
    "    \"\"\"\n",
    "    if check_logs and log_table is not None:\n",
    "        base_filters = (\n",
    "            (col(\"modelName\") == model_cache_combo[\"model\"][\"name\"]) &\n",
    "            (col(\"queryId\") == queryId) &\n",
    "            (col(\"cacheType\") == model_cache_combo[\"cache_type\"])\n",
    "        )\n",
    "        \n",
    "        success_count = log_table.filter(base_filters & (col(\"success\") == \"Success\")).count()\n",
    "        failure_count = log_table.filter(base_filters & (col(\"success\") == \"Failure\")).count()\n",
    "\n",
    "        result = (success_count >= additional_arguments[\"maxNumberPerQuery\"] or \n",
    "                  failure_count >= additional_arguments[\"maxFailuresBeforeSkipping\"])\n",
    "        print(f\"üìä {'Skipping' if result else 'Continuing'} queries (Success: {success_count}, Failure: {failure_count})\")\n",
    "        return result\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "@log_function_calls\n",
    "def get_starting_query_id(log_table, additional_args: dict) -> int:\n",
    "    \"\"\"\n",
    "    Determines the next queryId to start from based on the existing logs.\n",
    "    If no logs are present, starts at 1.\n",
    "    \"\"\"\n",
    "    print(\"üîç Determining starting query ID\")\n",
    "    if log_table is not None:\n",
    "        success_failure_counts = log_table.groupBy(\"modelName\", \"cacheType\", \"queryId\").agg(\n",
    "            _sum(when(col(\"success\") == \"Success\", 1).otherwise(0)).alias(\"success_count\"),\n",
    "            _sum(when(col(\"success\") == \"Failure\", 1).otherwise(0)).alias(\"failure_count\")\n",
    "        )\n",
    "\n",
    "        valid_queries = success_failure_counts.filter(\n",
    "            (col(\"success_count\") >= additional_args[\"maxNumberPerQuery\"]) |\n",
    "            (col(\"failure_count\") >= additional_args[\"maxFailuresBeforeSkipping\"])\n",
    "        )\n",
    "\n",
    "        distinct_combos_count = valid_queries.select(\"modelName\", \"cacheType\").distinct().count()\n",
    "\n",
    "        valid_query_ids = valid_queries.groupBy(\"queryId\").agg(\n",
    "            countDistinct(\"modelName\", \"cacheType\").alias(\"valid_combinations\")\n",
    "        ).filter(col(\"valid_combinations\") == distinct_combos_count)\n",
    "\n",
    "        query_id_list = sorted([row[\"queryId\"] for row in valid_query_ids.select(\"queryId\").collect()])\n",
    "        max_query_id = 0\n",
    "        for i, qid in enumerate(query_id_list):\n",
    "            if qid != i + 1:\n",
    "                break\n",
    "            max_query_id = qid\n",
    "\n",
    "        starting_query_id = 1 if max_query_id == 0 else max_query_id + 1\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Log table {additional_args['logTableName']} does not exist\")\n",
    "        starting_query_id = 1\n",
    "\n",
    "    print(f\"‚úÖ Starting query ID set to {starting_query_id}\")\n",
    "    return starting_query_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main DAX testing orchestration functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def run_dax_query_and_collect_logs(model_cache_combo: dict, dax_query: pd.Series, log_table) -> str:\n",
    "    \"\"\"\n",
    "    Runs a single DAX query based on the provided model, cache type, and queryId.\n",
    "    Handles cache setup, execution, trace logging, and appends the results to the log table.\n",
    "    Returns \"Ran\" if the query executed or \"Skipped\" if conditions indicate to skip it.\n",
    "    \"\"\"\n",
    "    model = model_cache_combo[\"model\"]\n",
    "    sent_dax_expression = dax_query[model[\"runQueryType\"]]\n",
    "    query_run_name = f\"Model: {model['name']}, QueryId: {dax_query['queryId']}, Cache Type: {model_cache_combo['cache_type']}\"\n",
    "    valid_cache_type_for_model = model_cache_combo[\"cache_type\"] in model[\"cache_types\"]\n",
    "    \n",
    "    print(f\"üöÄ Starting query: {query_run_name}\")\n",
    "    \n",
    "    # Check if this query should be run based on log history and cache type validity.\n",
    "    if (not max_queries_met(additional_arguments[\"onlyRunNewQueries\"], log_table, model_cache_combo, dax_query[\"queryId\"])\n",
    "        and valid_cache_type_for_model):\n",
    "        set_cache_start_time = datetime.now().isoformat()\n",
    "        wait_for_model_to_come_online(model)\n",
    "        \n",
    "        # Set the cache according to the type specified.\n",
    "        if model_cache_combo[\"cache_type\"] == \"cold\":\n",
    "            set_cold_cache(model)\n",
    "            cache_set = True\n",
    "        elif model_cache_combo[\"cache_type\"] == \"warm\":\n",
    "            cache_set = set_warm_cache(model, sent_dax_expression)\n",
    "        else:  # hot cache\n",
    "            cache_set = set_hot_cache(model, sent_dax_expression)\n",
    "        \n",
    "        time.sleep(additional_arguments[\"pauseAfterSettingCache\"])\n",
    "        set_cache_end_time = datetime.now().isoformat()\n",
    "        update_model_pause_status(\"model_queried\", model=model)\n",
    "        query_start_time = datetime.now().isoformat()\n",
    "        \n",
    "        # Start a trace for the DAX query execution.\n",
    "        fabric.create_trace_connection(model[\"name\"], model[\"model_workspace_name\"]).drop_traces()\n",
    "        trace_name = f\"Simple DAX Trace {uuid4()}\"\n",
    "        \n",
    "        with fabric.create_trace_connection(model[\"name\"], model[\"model_workspace_name\"]) as trace_conn:\n",
    "            with trace_conn.create_trace(event_schema, trace_name) as trace:\n",
    "                print(\"üîç Starting trace for DAX query\")\n",
    "                trace.start()\n",
    "                wait_for_trace_start(trace_conn, trace_name)\n",
    "                \n",
    "                dax_query_result = \"Success\"\n",
    "                try:\n",
    "                    print(\"‚ö° Executing DAX query\")\n",
    "                    fabric.evaluate_dax(model[\"name\"], sent_dax_expression, workspace=model[\"model_workspace_name\"])\n",
    "                    print(\"‚úÖ DAX query executed successfully\")\n",
    "                except Exception as e:\n",
    "                    dax_query_result = str(e)\n",
    "                    print(\"‚ùå DAX query execution failed:\", e)\n",
    "                \n",
    "                print(\"üìú Collecting trace logs\")\n",
    "                filtered_logs = collect_filtered_trace_logs(trace)\n",
    "                \n",
    "                time.sleep(additional_arguments[\"pauseAfterRunningQuery\"])\n",
    "                query_end_time = datetime.now().isoformat()\n",
    "\n",
    "                # Capture request properties from the QueryBegin event.\n",
    "                query_begin_rows = filtered_logs[filtered_logs[\"Event Class\"] == \"QueryBegin\"]\n",
    "                request_properties_value = query_begin_rows.iloc[0][\"Request Properties\"] if not query_begin_rows.empty else None\n",
    "                filtered_logs[\"Request Properties\"] = request_properties_value\n",
    "\n",
    "                # Convert all column names to camelCase.\n",
    "                filtered_logs.columns = [to_camel_case(col) for col in filtered_logs.columns]\n",
    "                \n",
    "                # Append metadata columns to the trace logs.\n",
    "                filtered_logs = filtered_logs.assign(\n",
    "                    sentExpression=sent_dax_expression,\n",
    "                    setCacheStartTime=set_cache_start_time,\n",
    "                    setCacheEndTime=set_cache_end_time,\n",
    "                    queryStartTime=query_start_time,\n",
    "                    queryEndTime=query_end_time,\n",
    "                    modelName=model[\"name\"],\n",
    "                    queryId=dax_query[\"queryId\"],\n",
    "                    runQueryType=model[\"runQueryType\"],\n",
    "                    cacheType=model_cache_combo[\"cache_type\"],\n",
    "                    queryResult=dax_query_result,\n",
    "                    storageMode=model[\"storageMode\"],\n",
    "                    roundNumber=additional_arguments[\"roundNumber\"],\n",
    "                )\n",
    "                \n",
    "                if not cache_set:\n",
    "                    print(\"‚ùå Cache was not set properly; marking query as failed\")\n",
    "                    filtered_logs = filtered_logs.assign(success=\"Failure\")\n",
    "        \n",
    "        # Convert the pandas DataFrame to a Spark DataFrame and append to the log table.\n",
    "        filtered_logs_df = spark.createDataFrame(filtered_logs)\n",
    "        \n",
    "        print(\"üíæ Appending logs to table\")\n",
    "        filtered_logs_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(additional_arguments[\"logTableName\"])\n",
    "        print(\"‚úÖ Logs appended to table\")\n",
    "        print(\"‚ÑπÔ∏è Pausing between runs\")\n",
    "        time.sleep(additional_arguments[\"pauseBetweenRuns\"])\n",
    "        \n",
    "        return \"Ran\"\n",
    "    else:\n",
    "        print(\"‚è≠Ô∏è Query skipped (logs exist or invalid cache type)\")\n",
    "        return \"Skipped\"\n",
    "\n",
    "@log_function_calls\n",
    "def run_dax_queries() -> None:\n",
    "    \"\"\"\n",
    "    Main entry point for running all DAX queries from the Excel file.\n",
    "    Manages the log table, capacity checks, and iterates over all queries and their combinations.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting all DAX queries\")\n",
    "\n",
    "    # Load the DAX queries using the configuration parameters.\n",
    "    dax_queries = load_dax_queries(query_file_path, query_file_mount_path, query_worksheet_name)\n",
    "    \n",
    "    # Initialize pause status flags for all models.\n",
    "    update_model_pause_status(\"initialize\")\n",
    "\n",
    "    # Manage log table cleanup based on additional arguments.\n",
    "    if additional_arguments[\"clearCurrentRoundLogs\"]:\n",
    "        print(f\"üóëÔ∏è Dropping round {additional_arguments['roundNumber']} logs from {additional_arguments['logTableName']}\")\n",
    "        spark.sql(f\"DELETE FROM {additional_arguments['logTableName']} WHERE roundNumber = {additional_arguments['roundNumber']}\")\n",
    "    if additional_arguments[\"clearAllLogs\"]:\n",
    "        print(f\"üóëÔ∏è Dropping entire table {additional_arguments['logTableName']}\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {additional_arguments['logTableName']}\")\n",
    "        startQueryIdsAt = 1\n",
    "        log_table = None\n",
    "    else:\n",
    "        print(f\"üîç Retrieving table {additional_arguments['logTableName']}\")\n",
    "        log_table = get_log_table(additional_arguments[\"logTableName\"])\n",
    "        startQueryIdsAt = (1 if additional_arguments[\"clearCurrentRoundLogs\"] or additional_arguments[\"forceStartQueriesAt1\"] or log_table is None\n",
    "                           else get_starting_query_id(log_table, additional_arguments))\n",
    "\n",
    "    # Check if capacity pause/resume logic is required (for Import/DirectQuery with cold cache)\n",
    "    include_pause_resume_logic = any(\n",
    "        model[\"storageMode\"] in [\"Import\", \"DirectQuery\"] and \"cold\" in model[\"cache_types\"]\n",
    "        for model in models\n",
    "    )\n",
    "\n",
    "    if include_pause_resume_logic:\n",
    "        # Validate that all necessary workspaces are defined in the workspace_capacities\n",
    "        for m in models:\n",
    "            ws = m[\"model_workspace_name\"]\n",
    "            if ws not in workspace_capacities:\n",
    "                raise ValueError(f\"The workspace '{ws}' (in model '{m['name']}') is not defined in workspace_capacities.\")\n",
    "        # Resume capacities before starting queries\n",
    "        for ws, caps in workspace_capacities.items():\n",
    "            pause_resume_capacity(caps[\"capacity_name\"], \"resume\", _simplify_logs=True)\n",
    "            pause_resume_capacity(caps[\"alt_capacity_name\"], \"resume\", _simplify_logs=True)\n",
    "            print(f\"‚úÖ Assigning primary capacity '{caps['capacity_name']}' to workspace '{ws}'\")\n",
    "            labs.assign_workspace_to_capacity(caps[\"capacity_name\"], ws)\n",
    "\n",
    "    # Loop through each DAX query from the Excel file\n",
    "    for _, dax_query in dax_queries.iterrows():\n",
    "        if dax_query[\"queryId\"] <= additional_arguments[\"stopQueryIdsAt\"] and dax_query[\"queryId\"] >= startQueryIdsAt:\n",
    "            for _ in range(additional_arguments[\"numberOfRunsPerQueryId\"]):\n",
    "                total_query_count = 0\n",
    "                skipped_query_count = 0\n",
    "                # Randomize the run order if specified\n",
    "                if additional_arguments[\"randomizeRuns\"]:\n",
    "                    print(\"üîÄ Randomizing run order of (model, cache_type)\")\n",
    "                    model_cache_combo = (\n",
    "                        pd.DataFrame(\n",
    "                            itertools.product(models, [\"cold\", \"warm\", \"hot\"]),\n",
    "                            columns=[\"model\", \"cache_type\"],\n",
    "                        )\n",
    "                        .sample(frac=1)\n",
    "                        .reset_index(drop=True)\n",
    "                    )\n",
    "                else:\n",
    "                    df = pd.DataFrame(\n",
    "                        itertools.product(models, [\"cold\", \"warm\", \"hot\"]),\n",
    "                        columns=[\"model\", \"cache_type\"],\n",
    "                    )\n",
    "                    df[\"model_name\"] = df[\"model\"].apply(lambda m: m[\"name\"])\n",
    "                    df[\"cache_order\"] = pd.Categorical(df[\"cache_type\"], categories=[\"cold\", \"warm\", \"hot\"], ordered=True)\n",
    "                    model_cache_combo = (\n",
    "                        df.sort_values(by=[\"model_name\", \"cache_order\"])\n",
    "                        .drop([\"model_name\", \"cache_order\"], axis=1)\n",
    "                        .reset_index(drop=True)\n",
    "                    )\n",
    "\n",
    "                # Execute the query for each model and cache type combination.\n",
    "                for _, current_combo in model_cache_combo.iterrows():\n",
    "                    total_query_count += 1\n",
    "                    if include_pause_resume_logic:\n",
    "                        update_model_pause_status(\"model_queried\", model=current_combo[\"model\"])\n",
    "                    run_status = run_dax_query_and_collect_logs(current_combo, dax_query, log_table)\n",
    "                    if run_status == \"Skipped\":\n",
    "                        skipped_query_count += 1\n",
    "\n",
    "                print(f\"üîÑ Refreshing log table from {additional_arguments['logTableName']}\")\n",
    "                log_table = get_log_table(additional_arguments[\"logTableName\"])\n",
    "\n",
    "                if total_query_count == skipped_query_count:\n",
    "                    print(\"‚ÑπÔ∏è No new queries; skipping additional runs for this query group\")\n",
    "                    break\n",
    "\n",
    "    if include_pause_resume_logic:\n",
    "        for ws, caps in workspace_capacities.items():\n",
    "            pause_resume_capacity(caps[\"alt_capacity_name\"], \"pause\", _simplify_logs=True)\n",
    "    print(\"‚úÖ All queries complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute main flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dax_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
