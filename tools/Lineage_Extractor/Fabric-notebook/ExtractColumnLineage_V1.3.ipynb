{"cells":[{"cell_type":"markdown","source":["This workbook does 4 things:\n","\n","1. first it provides some setup cells, to enable authentication and create storage dataframes for the results\n","\n","2. it explores all workspaces to which the provided user/service principal has access and extracts:\n","\n","    - Tables with their columns (this uses a SQL query, therefore it can be performed also from a SQL client application external to Fabric)\n","\n","    - Reports with their source tables (using Semantic Labs lib, therefore this can only be performed within Fabric, unless you leverage the low-level APIs which are used by SemanticLink labs)\n","\n","    - Details of Copy pipelines with column-level mappings (the source json is extracted via Semantic Labs lib)\n","\n","3. eventually all metadata is uploaded into MS Purview Data Governance, and lineage links with column mappings are created for the discovered artifacts above\n","\n","4. some examples are provided for the use of Semantic Link Lab, Scanner APIs, etc. for extracting Fabric metadata and lineage to external sources e.g. Azure Databricks. \n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"07c84ced-5b15-4329-8a3d-be3de9666d2f"},{"cell_type":"markdown","source":["**1. Setup**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8ae47336-d99d-448d-bc03-d6d8d3004673"},{"cell_type":"code","source":["#first let's install needed packages\n","\n","%pip install semantic-link-labs pyapacheatlas"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4a812563-9084-4466-b916-3d28b90feff4"},{"cell_type":"code","source":["# Enter your SERVICE PRINCIPAL credentials here, or better use an Azure Keyvault\n","# the clientid, clientsecret, tenantid variables are used to access one or more MS Fabric Workspaces\n","# the clientid4purview, clientsecret4purview, tenantid4purview variables are used to access a Purview Data Governance instance \n","# where the metadata (tables, columns, reports, pipelines) will be uploaded\n","# The demo_workspace and demo_workspace_id can be used for demo/test purposes in order to show/scan a single workspace\n","\n","tenant_id=\"<your tenant id>\"\n","tenantid4purview=tenant_id\n","client_id=\"<the client ID of the Service Principal>\"\n","clientid4purview=client_id\n","client_secret=\"<the client secret, only for non production. Preferably use a KeyVault >\"\n","clientsecret4purview=client_secret\n","PurviewAccount_name=\"<your Purview Account name>\"\n","# you may want to limit extraction to just one workspace for a quick try. in this case uncomment lines 17 and 18 in cell named \"Extract metadata from workspaces\"\n","demo_workspace=\"<the workspace name>\"\n","demo_workspace_id=\"<the workspace id>\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70683a44-1ad6-41a1-9973-d05d6e1c74f3"},{"cell_type":"code","source":["import sempy_labs as labs\n","import sempy_labs.report as rep\n","from sempy_labs.report import ReportWrapper\n","\n","import requests\n","import pyodbc\n","import json"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1638b317-68a6-4be6-affb-72ce4c5a45fe"},{"cell_type":"code","source":["# First authenticate and create an access token\n","base_url_auth = 'https://login.microsoftonline.com'\n","relative_url_auth=f'/{tenant_id}/oauth2/v2.0/token'\n","url_auth= f'{base_url_auth}/{relative_url_auth}'\n","data_auth = f'client_id={client_id}\\n&client_secret={client_secret}\\n&grant_type=client_credentials\\n&scope=https://api.fabric.microsoft.com/.default'\n","header_auth={\"Content-Type\": \"application/x-www-form-urlencoded\"}\n","\n","authentication_response = requests.post(url_auth, data = data_auth, headers=header_auth)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7ad06465-4b80-4abb-9262-c3f4fa8e9e27"},{"cell_type":"code","source":["#extract auth token to use in Fabric API requests\n","\n","auth_token=json.loads(authentication_response.text)[\"access_token\"]\n","bearer=f'Bearer {auth_token}'"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"25e5824b-fa85-4fa0-83c8-c3b0b9254a22"},{"cell_type":"code","source":["# these are the SQL queries used to scan tables and fields, they can be used also from an external SQL client \n","# instead of a notebook inside Fabric\n","\n","service_principal_id = f\"{client_id}@{tenant_id}\" # this is a very important pattern client_id@tenant_id\n","\n","odbc_connection_string_template= (\n","    f\"DRIVER={{ODBC Driver 18 for SQL Server}};\"\n","    f\"UID={service_principal_id};\"\n","    f\"PWD={client_secret};\"\n","    f\"Authentication=ActiveDirectoryServicePrincipal;\"\n",")\n","# the \"DATABASE={database_name}\" and SERVER=\"{Fabric SQL connection string}\" part will be added later, \n","# for each workspace being scanned\n","\n","query_tables='SELECT name, object_id, create_date, modify_date, type, type_desc from sys.tables'\n","query_columns_template1=(\n","    f\"SELECT c.name AS column_name \"  \n","    f\",c.column_id \"  \n","    f\",SCHEMA_NAME(t.schema_id) AS type_schema \"  \n","    f\",t.name AS type_name \"  \n","    f\",t.is_user_defined \"  \n","    f\",t.is_assembly_type \"  \n","    f\",c.max_length \"  \n","    f\",c.precision \"  \n","    f\",c.scale \"  \n","    f\"FROM sys.columns AS c \"\n","    f\"JOIN sys.types AS t ON c.user_type_id=t.user_type_id \"  \n","    f\"WHERE c.object_id = OBJECT_ID('\"\n",")\n","query_columns_template2=\"') ORDER BY c.column_id\" "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21cc7830-507a-4ff2-961e-81d09c8a86f1"},{"cell_type":"code","source":["# we create some container dataframes to hold the metadata we gather\n","\n","from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n","# record template for table columns metadata\n","df_columns_datarow={\"WorkspaceName\": \"dummy_column1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"WarehouseName\": \"\", \\\n","        \"WarehouseID\": \"\", \\\n","        \"LakehouseName\": \"\", \\\n","        \"LakehouseID\": \"\", \\\n","        \"TableName\": \"\", \\\n","        \"TableID\": \"\", \\\n","        \"ColumnName\": \"\", \\\n","        \"ColumnType\": \"\", \\\n","        \"ColumnID\":\"\"}\n","\n","schema_df_columns = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"WarehouseName\",StringType(),True), \\\n","    StructField(\"WarehouseID\",StringType(),True), \\\n","    StructField(\"LakehouseName\",StringType(),True), \\\n","    StructField(\"LakehouseID\",StringType(),True), \\\n","    StructField(\"TableName\",StringType(),True), \\\n","    StructField(\"TableID\", StringType(), True), \\\n","    StructField(\"ColumnName\", StringType(), True), \\\n","    StructField(\"ColumnType\", StringType(), True), \\\n","    StructField(\"ColumnID\", StringType(), True) \\\n","  ])\n"," \n","df_columns = spark.createDataFrame(data=[],schema=schema_df_columns)\n","#df_columns.show()\n","\n","# record template for table metadata\n","# table type in this df is for tables/views/storedprocs...\n","# currently only tables are exctracted\n","df_tables_datarow={\"WorkspaceName\": \"dummy_table_1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"WarehouseName\": \"\", \\\n","        \"WarehouseID\": \"\", \\\n","        \"LakehouseName\": \"\", \\\n","        \"LakehouseID\": \"\", \\\n","        \"TableName\": \"\", \\\n","        \"TableType\": \"\", \\\n","        \"TableID\": \"\", \\\n","        \"PurviewGUID\":\"\", \\\n","        \"PurviewFQN\":\"\"}\n"," \n","\n","schema_df_tables = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"WarehouseName\",StringType(),True), \\\n","    StructField(\"WarehouseID\",StringType(),True), \\\n","    StructField(\"LakehouseName\",StringType(),True), \\\n","    StructField(\"LakehouseID\",StringType(),True), \\\n","    StructField(\"TableName\",StringType(),True), \\\n","    StructField(\"TableID\", StringType(), True), \\\n","    StructField(\"PurviewGUID\", StringType(), True), \\\n","    StructField(\"PurviewFQN\", StringType(), True)\n","    ])\n"," \n","df_tables = spark.createDataFrame(data=[],schema=schema_df_tables)\n","\n","# record template for report metadata\n","# source can be a table or a view. Only tables are currently extracted\n","df_reports_datarow={\"WorkspaceName\": \"dummy_report_1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"ReportName\": \"\", \\\n","        \"ReportID\": \"\", \\\n","        \"ColumnName\": \"\", \\\n","        \"SourceName\": \"\", \\\n","        \"SourceType\": \"\", \\\n","        \"SourceID\":\"\", \\\n","        \"PurviewGUID\":\"\", \\\n","        \"PurviewFQN\":\"\"}\n","\n","schema_df_reports = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"ReportName\",StringType(),True), \\\n","    StructField(\"ReportID\", StringType(), True), \\\n","    StructField(\"ColumnName\", StringType(), True), \\\n","    StructField(\"SourceName\", StringType(), True), \\\n","    StructField(\"SourceType\", StringType(), True), \\\n","    StructField(\"SourceID\", StringType(), True), \\\n","    StructField(\"PurviewGUID\", StringType(), True), \\\n","    StructField(\"PurviewFQN\", StringType(), True)\n","  ])\n"," \n","df_reports = spark.createDataFrame(data=[],schema=schema_df_reports)\n","#df_reports.show()\n","\n","# record template for DataPipelines metadata\n","# source can be a table or a view. Only tables are currently extracted\n","df_pipelines_datarow={\"WorkspaceName\": \"dummy_lineage_1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"PipelineID\": \"\", \\\n","        \"PipelineName\": \"\", \\\n","        \"ActivityName\": \"\", \\\n","        \"ActivityType\": \"\", \\\n","        \"SourceName\": \"\", \\\n","        \"SourceType\": \"\", \\\n","        \"SourceContainerName\": \"\", \\\n","        \"SourceContainerID\": \"\", \\\n","        \"SinkType\": \"\", \\\n","        \"SinkName\": \"\", \\\n","        \"SinkContainer Name\": \"\", \\\n","        \"SinkContainer ID\": \"\", \\\n","        \"ColumnMappings\": \"\", \\\n","        \"PurviewGUID\":\"\", \\\n","        \"PurviewFQN\":\"\"}\n","\n","schema_df_pipelines = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"PipelineID\",StringType(),True), \\\n","    StructField(\"PipelineName\",StringType(),True), \\\n","    StructField(\"ActivityName\", StringType(), True), \\\n","    StructField(\"ActivityType\", StringType(), True), \\\n","    StructField(\"SourceName\",StringType(),True), \\\n","    StructField(\"SourceType\", StringType(), True), \\\n","    StructField(\"SourceContainerName\", StringType(), True), \\\n","    StructField(\"SourceContainerID\", StringType(), True), \\\n","    StructField(\"SinkName\",StringType(),True), \\\n","    StructField(\"SinkType\", StringType(), True), \\\n","    StructField(\"SinkContainerName\", StringType(), True), \\\n","    StructField(\"SinkContainerID\", StringType(), True), \\\n","    StructField(\"ColumnMappings\", StringType(), True), \\\n","    StructField(\"PurviewGUID\", StringType(), True), \\\n","    StructField(\"PurviewFQN\", StringType(), True)\n","  ])\n"," \n","df_pipelines = spark.createDataFrame(data=[],schema=schema_df_pipelines)\n","display(df_pipelines)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"012cc47d-d101-4363-a7ac-20b1ea271faa"},{"cell_type":"markdown","source":["**2. Start of main metadata extraction process**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cc5ebfe7-3731-4c83-bd98-3bc0228eff5f"},{"cell_type":"markdown","source":["**Extract metadata from workspaces**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"350e2802-d322-4e3c-8e44-e1cc9583b09a"},{"cell_type":"code","source":["# this is the actual metadata extraction process\n","# first get the list of workspaces accessible by the Service Principal we authenticated with\n","get_workspaces_url = f'https://api.fabric.microsoft.com/v1/workspaces'\n","header_token={\"Authorization\": bearer, 'Content-Type': 'application/json'}\n","data_dummy={\"dummy\":\"true\"}\n","workspaces_response=requests.get(get_workspaces_url,headers=header_token, data=data_dummy)\n","print(\"List of workspaces that can be accessed for metadata extraction:\\n\",workspaces_response.text)\n","print()\n","\n","# loop through the workspaces and extract their items\n","wkspaces=json.loads(workspaces_response.text)[\"value\"]\n","for wkspc in wkspaces:\n","    \n","    current_wkspace_id=wkspc[\"id\"]\n","    current_wkspace_name=wkspc[\"displayName\"]\n","    ## hack for demo: if you need to just display one workspace (for test or demo) uncomment these 2 lines\n","    #if current_wkspace_id != demo_workspace_id:\n","    #  continue\n","    \n","    print(f'Processing workspace:  {current_wkspace_name} ({current_wkspace_id})\\n')\n","    # add a new row to the list of tables to track the workspaceID\n","    new_row = df_tables_datarow.copy()\n","    new_row.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id})\n","    df_tables = df_tables.union(spark.createDataFrame(data=[new_row],schema=schema_df_tables))\n","    #df_tables.show()\n","    \n","    # get the items within the current workspace\n","    get_items_url = f'https://api.fabric.microsoft.com/v1/workspaces/{current_wkspace_id}/items'\n","    items_response=requests.get(get_items_url,headers=header_token, data=data_dummy)\n","    #print(items_response.text)\n","    \n","    # loop through the items to extract the ones we can manage\n","    items=json.loads(items_response.text)[\"value\"]\n","    for item in items:\n","      item_name=item[\"displayName\"]\n","      item_type=item[\"type\"]\n","      item_id=item[\"id\"]\n","      print(f'\\n{item_name} is of type {item_type} and has id={item[\"id\"]}')\n","      if item_type == \"Warehouse\":\n","          print(f\"Exploring {item_name}\")\n","          database_name=item_name\n","          # get the SERVER part of the SQL connection string\n","          # https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses/{warehouseId} \n","          get_SQLString_url = f'https://api.fabric.microsoft.com/v1/workspaces/{current_wkspace_id}/warehouses/{item_id}'\n","          sql_url_response=requests.get(get_SQLString_url,headers=header_token, data=data_dummy)\n","          fabric_SQL_connection_string=json.loads(sql_url_response.text)[\"properties\"][\"connectionString\"]\n","          print(f\"SQLconn_string for {item_name} is {fabric_SQL_connection_string}\")\n","          odbc_conn_string=f'{odbc_connection_string_template}SERVER={fabric_SQL_connection_string};DATABASE={database_name}'\n","          conn = pyodbc.connect(odbc_conn_string)\n","          # Execute the query to extract the tables\n","          cursor = conn.cursor()\n","          cursor.execute(query_tables)\n","          tableList = cursor.fetchall()\n","          table_name=\"\"\n","          for row in tableList:\n","              #print(\"Found table: \",row)\n","              table_name=row[0]\n","\n","              # save table name in dataframe for tables\n","              new_dict=df_tables_datarow.copy()\n","              new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","              \"WarehouseName\": f\"{database_name}\", \\\n","              \"WarehouseID\": f\"{item_id}\", \\\n","              \"TableName\": f'{table_name}', \\\n","              \"TableType\": 'Table', \\\n","              \"TableID\": f'{row[1]}'})\n","              #print(f'new_dict={new_dict}')\n","              new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_tables)\n","              #print(f'new_dict={new_row}')\n","              df_tables = df_tables.union(new_row)\n","              #df_tables.show()\n","\n","              #extract  details of table columns\n","              #\n","              \n","              query_columns=f'{query_columns_template1}{table_name}{query_columns_template2}'\n","              #print(query_columns,\"\\n\")\n","              cursor.execute(query_columns)\n","              columnList=cursor.fetchall()\n","              for column in columnList:\n","                # extract  column_name,column_id,type_name\n","                # save column name in dataframe for columns\n","                new_dict=df_columns_datarow.copy()\n","                new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                \"WarehouseName\": f\"{database_name}\", \\\n","                \"WarehouseID\": f\"{item_id}\", \\\n","                \"TableName\": f'{table_name}', \\\n","                \"TableID\": f'{row[1]}', \\\n","                \"ColumnName\": f'{column[0]}', \\\n","                \"ColumnID\": f'{column[1]}', \\\n","                \"ColumnType\": f'{column[3]}'})\n","                #print(f'new_dict={new_dict}')\n","                new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_columns)\n","                #print(f'new_dict={new_row}')\n","                df_columns = df_columns.union(new_row)\n","              \n","          #close correctly\n","          cursor.close()\n","          conn.close()\n","          \n","\n","      if item_type == \"Lakehouse\":\n","          print(f\"Exploring {item_name}\")\n","          database_name=item_name\n","          # get the SERVER part of the SQL connection string\n","          # https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses/{warehouseId} \n","          get_SQLString_url = f'https://api.fabric.microsoft.com/v1/workspaces/{current_wkspace_id}/lakehouses/{item_id}'\n","          sql_url_response=requests.get(get_SQLString_url,headers=header_token, data=data_dummy)\n","          fabric_SQL_connection_string=json.loads(sql_url_response.text)[\"properties\"][\"sqlEndpointProperties\"][\"connectionString\"]\n","          print(f\"SQLconn_string for {item_name} is {fabric_SQL_connection_string}\")\n","          odbc_conn_string=f'{odbc_connection_string_template}SERVER={fabric_SQL_connection_string};DATABASE={database_name}'\n","          conn = pyodbc.connect(odbc_conn_string)\n","          # Execute the query to extract tables\n","          cursor = conn.cursor()\n","          cursor.execute(query_tables)\n","          tableList = cursor.fetchall()\n","          table_name=\"\"\n","          for row in tableList:\n","              print(\"Found table: \",row)\n","              table_name=row[0]\n","              \n","              # save table name in dataframe for tables\n","              new_dict=df_tables_datarow.copy()\n","              new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","              \"LakehouseName\": f\"{database_name}\", \\\n","              \"LakehouseID\": f\"{item_id}\", \\\n","              \"TableName\": f'{table_name}', \\\n","              \"TableType\": 'Table', \\\n","              \"TableID\": f'{row[1]}'})\n","              #print(f'new_dict={new_dict}')\n","              new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_tables)\n","              #print(f'new_dict={new_row}')\n","              df_tables = df_tables.union(new_row)\n","              #df_tables.show()\n","\n","              #extract  details of table columns\n","              #\n","              \n","              query_columns=f'{query_columns_template1}{table_name}{query_columns_template2}'\n","              #print(query_columns,\"\\n\")\n","              cursor.execute(query_columns)\n","              columnList=cursor.fetchall()\n","              for column in columnList:\n","                # extract  column_name,column_id,type_name\n","                # save column name in dataframe for columns\n","                new_dict=df_columns_datarow.copy()\n","                new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                \"LakehouseName\": f\"{database_name}\", \\\n","                \"LakehouseID\": f\"{item_id}\", \\\n","                \"TableName\": f'{table_name}', \\\n","                \"TableID\": f'{row[1]}', \\\n","                \"ColumnName\": f'{column[0]}', \\\n","                \"ColumnID\": f'{column[1]}', \\\n","                \"ColumnType\": f'{column[3]}'})\n","                #print(f'new_dict={new_dict}')\n","                new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_columns)\n","                #print(f'new_dict={new_row}')\n","                df_columns = df_columns.union(new_row)\n","          #close correctly\n","          cursor.close()\n","          conn.close()\n","          \n","\n","      if item_type == \"Report\":\n","          print(f\"Exploring {item_name}\")\n","          # use semantic toolkit library for reports\n","          # catch exceptions if the report is not in the new PBIP/PBIR format, because only the new format can be parsed\n","          try:\n","                rpt = ReportWrapper(report=item_name, workspace=current_wkspace_name)\n","                pandadf_report_lineage=rpt.list_semantic_model_objects()\n","                #print(pandadf_report_lineage)\n","                for row in pandadf_report_lineage.itertuples():\n","                    #print(\"pandas row=\",row)\n","                    #print(row[2])\n","                    if row[3] == \"Column\":\n","                        #\n","                        # save report lineage item in dataframe for reports\n","                        new_dict=df_reports_datarow.copy()\n","                        new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                        \"ReportName\": f'{item_name}', \\\n","                        \"ReportID\": f'{item_id}', \\\n","                        \"ColumnName\": f'{row[2]}', \\\n","                        \"SourceName\": f'{row[1]}', \\\n","                        #\"Source ID\": f'{column[1]}', \\\n","                        \"SourceType\": \"Table\"}) #only tables so far\n","                        #print(f'new_dict={new_dict}')\n","                        new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_reports)\n","                        #print(f'new_dict={new_row}')\n","                        df_reports = df_reports.union(new_row)\n","                        #\n","          except ValueError as verr:\n","            print(\"WARNING: report not valid, you must use the new PBIR/PBIP format in PowerBI Desktop\")\n","            print(\"error=\",verr)\n","          except Exception as generic_ex:\n","            print(\"WARNING: generic exception (if reported errorCode is 'ItemHasProtectedLabel' then consider raising privileges of your Service Principal )\")\n","            print(\"error=\",generic_ex)\n","\n","                \n","     \n","      if item_type == \"DataPipeline\":\n","          print(f\"Exploring {item_name}\")\n","          # use semantic toolkit for pipelines, to extract the JSON definition of activities\n","          try:\n","                pipeline_definition=labs.get_data_pipeline_definition(item_name,current_wkspace_id,decode=True)\n","                # check which type, and if we can manage this kind of pipeline\n","                # only pipelines with copy activity are managed so far\n","                all_activities=json.loads(pipeline_definition)[\"properties\"][\"activities\"]\n","                \n","                for activity in all_activities:\n","                        #print(\"activity \",activity)\n","                        activity_type=activity[\"type\"]\n","                        if activity_type == \"Copy\":\n","                            #gather metadata\n","                            activity_name=activity[\"name\"]\n","                            source_type = activity[\"typeProperties\"][\"source\"][\"type\"]\n","                            source_container=activity[\"typeProperties\"][\"source\"][\"datasetSettings\"][\"linkedService\"][\"name\"]\n","                            source_container_id=activity[\"typeProperties\"][\"source\"][\"datasetSettings\"][\"linkedService\"][\"properties\"][\"typeProperties\"][\"artifactId\"]\n","                            source_name=activity[\"typeProperties\"][\"source\"][\"datasetSettings\"][\"typeProperties\"][\"table\"]\n","                            sink_type=activity[\"typeProperties\"][\"sink\"][\"type\"]\n","                            sink_container=activity[\"typeProperties\"][\"sink\"][\"datasetSettings\"][\"linkedService\"][\"name\"]\n","                            sink_container_id=activity[\"typeProperties\"][\"sink\"][\"datasetSettings\"][\"linkedService\"][\"properties\"][\"typeProperties\"][\"artifactId\"]\n","                            sink_name=activity[\"typeProperties\"][\"sink\"][\"datasetSettings\"][\"typeProperties\"][\"table\"]\n","                            source_sink_mappings=activity[\"typeProperties\"][\"translator\"][\"mappings\"]\n","                            mappings_list=[]\n","                            for mapping_item in source_sink_mappings:\n","                                mappings_list.append({\"Source\": f'{mapping_item[\"source\"][\"name\"]} ({mapping_item[\"source\"][\"type\"]} {mapping_item[\"source\"][\"physicalType\"]})', \\\n","                                \"Sink\": f'{mapping_item[\"source\"][\"name\"]} ({mapping_item[\"source\"][\"physicalType\"]})'})\n","                            mappings_string=json.dumps(mappings_list)\n","                            print(\"mappings to string\",mappings_string)\n","                            # save metadata in dataframe\n","                            new_dict=df_pipelines_datarow.copy()\n","                            new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                            \"PipelineName\": item_name, \\\n","                            \"PipelineID\": item_id, \\\n","                            \"ActivityName\": activity_name, \\\n","                            \"ActivityType\": activity_type, \\\n","                            \"SourceName\": source_name, \\\n","                            \"SourceType\": source_type, \\\n","                            \"SourceContainerName\": source_container, \\\n","                            \"SourceContainerID\": source_container_id, \\\n","                            \"SinkType\": sink_type, \\\n","                            \"SinkName\": sink_name, \\\n","                            \"SinkContainerName\": sink_container, \\\n","                            \"SinkContainerID\": sink_container_id, \\\n","                            \"ColumnMappings\": f'{mappings_string}', \\\n","                            \"PurviewFQN\":\"\"})\n","                            \n","                            #print(f'new_dict={new_dict}')\n","                            new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_pipelines)\n","                            #print(f'new_dict={new_row}')\n","                            df_pipelines = df_pipelines.union(new_row)\n","                            break\n","          except KeyError as keyerr:\n","            print(\"Error parsing DataPipelines definition: \", keyerr)\n","            #print(\"error parsing activities: \",all_activities)\n","            print(\"error details \", keyerr)\n","          except NameError as namerr:\n","            #print(\"error parsing activities: \",all_activities)\n","            print(\"Error parsing DataPipelines definition: \", namerr)\n","          except Exception as generic_ex:\n","            print(\"WARNING: generic exception (if reported errorCode is 'ItemHasProtectedLabel' then consider raising privileges of your Service Principal )\")\n","            print(\"error=\",generic_ex)\n","                    \n","\n","              \n","              \n","print(\"\\n\\nRecap of Metadata found:\")\n","\n","print(\"\\nFound Tables (includes partial rows for Lakehouses and Warehouses):\")\n","display(df_tables)\n","#df_tables.show()\n","\n","print(\"\\n\\nFound Table columns (for all tables) found:\")\n","#df_columns.show()\n","display(df_columns)\n","\n","print(\"\\n\\nFound PBIR Reports lineage to tables/views:\")\n","display(df_reports)\n","#df_reports.show()\n","\n","print(\"\\n\\nFound Copy DataPipelines, with lineage to tables/views:\")\n","display(df_pipelines)\n","\n","\n","print(\"\\nMetadata Extraction Completed.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f3709664-6259-4004-97b2-f0efb7fd7c50"},{"cell_type":"markdown","source":["**3. Upload metadata into Purview Data Governance**\n","\n","Code for upload is taken from Pyapacheatlas samples by Will Johnson ( https://github.com/wjohnson/pyapacheatlas )"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"71999375-cd28-4380-8179-0268dce09cd6"},{"cell_type":"code","source":["import json\n","import pyapacheatlas\n","# PyApacheAtlas packages are used to upload the metadata into MS Purview Data Governance\n","# Connect to Atlas (MS Purview DG) via a Service Principal\n","from pyapacheatlas.auth import ServicePrincipalAuthentication\n","from pyapacheatlas.core import PurviewClient, AtlasEntity, AtlasProcess\n","from pyapacheatlas.core.typedef import EntityTypeDef, AtlasAttributeDef\n","from pyapacheatlas.core.util import GuidTracker\n","\n","from pyspark.sql import Row\n","from pyspark.sql.functions import col, when\n","import pyspark.sql"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4465fa93-6736-46e4-ae7f-11856f3225c6"},{"cell_type":"code","source":["# enter Service principal id and secret to access the MS Purview account APIs\n","# (by default the code reuses the same Svc Principal used for accessing Fabric, feel free to modify the variables in this cell to use a different Svc Principal)\n","oauth = ServicePrincipalAuthentication(\n","        tenant_id=tenantid4purview,\n","        client_id=clientid4purview,\n","        client_secret=clientsecret4purview\n","    )\n","client = PurviewClient(\n","        account_name=PurviewAccount_name,\n","        authentication=oauth\n","    )\n","created_GUIDs=[] # used to keep track of newly created Purview items, in case we want to move such items to a specific collection   "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70fd7857-43db-446f-a95a-e612e89edc87"},{"cell_type":"code","source":["# We need a custom process entity type that contains the definition for\n","# a columnMapping attribute to store column-level details.\n","# This cell should preferably be run only once (further runs would only overwrite the entity type)\n","procType = EntityTypeDef(\n","    \"Lineage_Extractor_Process\",\n","    superTypes=[\"Process\"],\n","    attributeDefs = [\n","        AtlasAttributeDef(\"columnMapping\")\n","    ]\n",")\n","\n","# Upload the new asset type definition\n","type_results = client.upload_typedefs(entityDefs=[procType], force_update=True)\n","\n","created_GUIDs.append(type_results[\"entityDefs\"][0][\"guid\"])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"57604287-caa9-4c8c-9cdb-8f43d9ec21b7"},{"cell_type":"markdown","source":["**Upload tables and their columns**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a8637cac-0f5f-4d52-9259-6cfbd6083512"},{"cell_type":"code","source":["# create a new spark dataframe (df for short), where we will update the Purview fullyQualifiedName and GUID\n","df_tables_registered=spark.createDataFrame(data=[],schema=schema_df_tables)\n","\n","\n","# create Table nodes\n","gt = GuidTracker()\n","for table in df_tables.collect():\n","    table_dict=table.asDict()\n","    #print(\"table_dict=\",table_dict)\n","    tablename=table_dict[\"TableName\"]\n","    \n","    if tablename:\n","        if table_dict[\"WarehouseName\"]:\n","            artifactName=table_dict[\"WarehouseName\"]\n","            artifactType=\"warehouse\"\n","            artifactID=table_dict[\"WarehouseID\"]\n","        else:\n","            artifactName=table_dict[\"LakehouseName\"]\n","            artifactType=\"lakehouse\"\n","            artifactID=table_dict[\"LakehouseID\"]\n","        table_entity = AtlasEntity(\n","            name=tablename,\n","            typeName=\"DataSet\",\n","            qualified_name=f'LineageExtractor://workspace/{table_dict[\"WorkspaceID\"]}/{artifactType}/{artifactID}/table/{table_dict[\"TableID\"]}',\n","            guid=gt.get_guid(),\n","            attributes={\n","                \"userDescription\": f'<div>Fabric Table<p> {tablename} is a Table contained within {artifactType} {artifactName} in Workspace {table_dict[\"WorkspaceName\"]}</p>'\n","            },\n","            # This custom attribute flips a switch inside of the Purview UI to render\n","            # the rich text description.\n","            customAttributes={\n","                \"microsoft_isDescriptionRichText\": \"true\"\n","            }\n","        )\n","        #print(table_entity)\n","        #print(table_entity.guid)\n","        #print(table_entity.qualifiedName)\n","        results = client.upload_entities([table_entity])\n","        \n","        table_dict[\"PurviewGUID\"] = results[\"guidAssignments\"][str(table_entity.guid)]    \n","        table_dict[\"PurviewFQN\"] = str(table_entity.qualifiedName)\n","        #print(f'new guid, new fqn={table_dict[\"PurviewGUID\"]}, {table_dict[\"PurviewFQN\"]}')\n","        \n","        created_GUIDs.append(table_dict[\"PurviewGUID\"])\n","\n","        new_dict=df_tables_datarow.copy()\n","        for key in table_dict.keys():\n","            #print(\"key=\",key)\n","            new_dict[key]=table_dict[key]\n","        #print(\"new_dict=\",new_dict)\n","        new_row=spark.createDataFrame(data=[new_dict],schema=schema_df_tables)\n","        \n","        df_tables_registered=df_tables_registered.union(new_row)\n","          \n","        #df_tables.union(table)\n","        #df_tables=df_tables.withColumn(\"PurviewGUID\", when(col(\"TableName\") == tablename, createdPurviewGUID))\n","        #df_tables=df_tables.withColumn(\"PurviewFQN\", when(col(\"TableName\") == tablename, createdPurviewFQN))\n","\n","display(df_tables_registered)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b54b3937-e95f-4623-8bf9-3df0847f2d37"},{"cell_type":"markdown","source":["**Upload reports and their lineage**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a2b446ff-5da1-4c04-9211-c7e857946ba3"},{"cell_type":"code","source":["#new df where we will update the Purview fullyQualifiedName and GUID\n","df_reports_registered=df_reports.alias(\"df_reports_registered\")\n","#generates an empty copy of the Dataframe\n","\n","\n","gt = GuidTracker()\n","# create Reports nodes and lineage\n","#first find all detected reports IDs\n","report_distinctIds=df_reports.select('ReportID').distinct().collect()\n","#print(\"\\nreport_distinctIds:\")\n","#display(report_distinctIds)\n","#print()\n","\n","\n","for report_id in report_distinctIds:\n","    rep_id_dict=report_id.asDict()\n","    #print(\"\\nrep_id_dict\",rep_id_dict)\n","    current_report_id=rep_id_dict[\"ReportID\"]\n","    report_rows=df_reports_registered.filter(col(\"ReportID\")==current_report_id)\n","    report_sources=report_rows.collect()\n","    #print(\"report sources=\",report_sources)\n","    #print()\n","\n","    #for each report register it, and create lineage with tables\n","    #first register it if not already scanned by Purview\n","    report_metadata=report_sources[0].asDict()\n","    report_guid=report_metadata[\"PurviewGUID\"]\n","    report_fqn=report_metadata[\"PurviewFQN\"]\n","    report_workspace_id=report_metadata[\"WorkspaceID\"]\n","    if report_guid == \"\":\n","        # try to find whether a report was scanned by Purview, use its FQN \n","        scanned_report_FQN=f'https://app.powerbi.com/groups/{report_workspace_id}/reports/{current_report_id}'\n","        response_if_any_report_entity = client.get_entity(typeName=\"powerbi_report\",qualifiedName=scanned_report_FQN)\n","        if len(response_if_any_report_entity)>0 :\n","            scanned_report=response_if_any_report_entity[\"entities\"][0]\n","            report_guid = scanned_report['guid']    \n","            report_fqn = scanned_report['attributes']['qualifiedName']\n","            print(f\"Found scanned report: {report_metadata['ReportName']} ; Purview GUID={report_guid} ; Purview QualifiedName={report_fqn}\")            \n","        else:\n","            # register a new one\n","            report_entity = AtlasEntity(\n","                name=report_metadata[\"ReportName\"],\n","                typeName=\"DataSet\",\n","                qualified_name=f'LineageExtractor://workspace/{report_metadata[\"WorkspaceID\"]}/report/{current_report_id}',\n","                guid=gt.get_guid(),\n","                attributes={\n","                    \"userDescription\": f'<div>Fabric Report<p>in PBIR format</p><p>contained within {report_metadata[\"WorkspaceName\"]}</p><p>Fabric ID is {current_report_id}</p>'\n","                },\n","                # This custom attribute flips a switch inside of the Purview UI to render\n","                # the rich text description.\n","                customAttributes={\n","                    \"microsoft_isDescriptionRichText\": \"true\"\n","                }\n","            )\n","            #print(report_entity)\n","            #print(report_entity.guid)\n","            #print(report_entity.qualifiedName)\n","            results = client.upload_entities([report_entity])\n","            #print(\"\\nrisultato upload=\",json.dumps(results))\n","\n","            report_guid = results[\"guidAssignments\"][str(report_entity.guid)]    \n","            report_fqn = str(report_entity.qualifiedName)\n","            #print(f'\\nnew guid, new fqn={report_guid}, {report_fqn}')\n","            \n","            created_GUIDs.append(report_guid)\n","        #change all row occurrences of the report by updating FQN and GUID\n","        df_reports_registered=df_reports_registered.withColumn(\"PurviewGUID\", when(col(\"ReportID\") == current_report_id, report_guid).otherwise(col(\"PurviewGUID\")))\n","        df_reports_registered=df_reports_registered.withColumn(\"PurviewFQN\", when(col(\"ReportID\") == current_report_id, report_fqn).otherwise(col(\"PurviewFQN\")))       \n","        #refresh report_rows\n","        report_rows=df_reports_registered.filter(col(\"ReportID\")==current_report_id)\n","        \n","    #we use a join between the source table name here in the df of reports and the df of tables, then for each row we create a lineage link\n","    reports_short=report_rows.select('ReportID','SourceName',\"ColumnName\",\"PurviewFQN\").withColumnRenamed(\"PurviewFQN\",\"ReportFQN\")\n","    #print(\"\\nreports_short:\")\n","    #print(reports_short)\n","    #display(reports_short)\n","    #print()\n","\n","    lineage_links=reports_short.join(df_tables_registered.select(\"TableName\",\"PurviewFQN\"),reports_short[\"SourceName\"] == df_tables_registered[\"TableName\"],\"inner\").orderBy(\"TableName\").collect()\n","    #print(\"\\nlinks=\",lineage_links)\n","    #display(lineage_links)\n","\n","    # loop through source tables to create lineage columns mappings\n","    column_mapping_total=[]\n","    column_mapping_partial=[]\n","    inputs=[]\n","    sourceFQN=\"\"\n","    sinkFQN=\"\"\n","    current_table=lineage_links[0][\"TableName\"]\n","    previous_table=current_table\n","    #print(f'\\nprevious table={previous_table}, current table={current_table}')\n","    for link in lineage_links:\n","        link_dict=link.asDict()\n","        #print(\"\\nlink_dict=\",link_dict)\n","        current_table=link_dict[\"TableName\"]\n","        if previous_table == current_table:\n","            # just add another mapping entry\n","            # print(\"column in same table as previous loop\")\n","            columnname=link_dict[\"ColumnName\"]\n","            column_mapping_partial.append({\"Source\": columnname,\"Sink\":\"Visual\"})\n","            sourceFQN=link_dict[\"PurviewFQN\"]\n","            sinkFQN=link_dict[\"ReportFQN\"]\n","        else:\n","            # pack info for current table\n","            # print(f'\\nprevious table={previous_table}, current table={current_table}')\n","            # print(f'different tables from previous loop')\n","\n","            column_mapping_total.append(\n","                # This object defines the column lineage between table souces and the current report\n","                {\"ColumnMapping\": column_mapping_partial,\n","                \"DatasetMapping\": {\n","                        \"Source\": sourceFQN, \"Sink\": sinkFQN}\n","                }\n","            )\n","            column_mapping_partial=[]\n","            registered_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=sourceFQN)[\"entities\"][0]\n","            #print(\"found table entity =\",registered_table_entity)\n","            inputs.append(registered_table_entity)\n","            # take new values\n","            columnname=link_dict[\"ColumnName\"]\n","            column_mapping_partial.append({\"Source\": columnname,\"Sink\":\"Visual\"})\n","            sourceFQN=link_dict[\"PurviewFQN\"]\n","            sinkFQN=link_dict[\"ReportFQN\"]\n","            previous_table=current_table\n","    # process and close last table column\n","    # print(\"\\nlast\")\n","    registered_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=sourceFQN)[\"entities\"][0]\n","    #print(\"found table entity =\",registered_table_entity)\n","    inputs.append(registered_table_entity)\n","    # take new values\n","    columnname=link_dict[\"ColumnName\"]\n","    #column_mapping_partial.append({\"Source:\": columnname,\"Sink:\":\"Visual\"})\n","    \n","    column_mapping_total.append(\n","                # This object defines the column lineage between table sources and the current report\n","                {\"ColumnMapping\": column_mapping_partial,\n","                \"DatasetMapping\": {\n","                        \"Source\": sourceFQN, \"Sink\": sinkFQN}\n","                }\n","            )\n","\n","    registered_report_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=report_fqn)[\"entities\"][0]\n","\n","    print('\\n\\n\\ncolumn_mapping_total=', json.dumps(column_mapping_total))\n","    lineage_link_obj = AtlasProcess(\n","        name=f'Report Lineage for {report_metadata[\"ReportName\"]}',\n","        typeName=\"Lineage_Extractor_Process\",\n","        qualified_name=f'LineageExtractor://workspace/{report_metadata[\"WorkspaceID\"]}/report/{current_report_id}/ReportLineage',\n","        inputs=inputs,\n","        outputs=[registered_report_entity],\n","        guid=gt.get_guid(),\n","        attributes={\n","            \"columnMapping\": json.dumps(column_mapping_total),\n","            \"userDescription\": f'<div>Fabric Report Mapping<p> for Report {report_metadata[\"ReportName\"]} contained within workspace {report_metadata[\"WorkspaceName\"]}</p> <p> Report Id is {current_report_id}</p>'\n","        },\n","        # This custom attribute flips a switch inside of the Purview UI to render\n","        # the rich text description.\n","        customAttributes={\n","            \"microsoft_isDescriptionRichText\": \"true\"\n","        }\n","    )\n","    #print(\"\\ncreated guids=\",created_GUIDs)\n","    #print(\"\\nentity to be created=\",lineage_link_obj)\n","    results = client.upload_entities([lineage_link_obj])\n","    #print(\"\\n upload result=\",json.dumps(results))\n","    created_GUIDs.append(results[\"guidAssignments\"][str(lineage_link_obj.guid)])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9fbbcf6d-0dac-48df-b0b7-dc19a2d39aa6"},{"cell_type":"markdown","source":[" **Upload pipelines and their lineage**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2be18311-7afa-4b6b-8046-d12f0ef67d89"},{"cell_type":"code","source":["df_pipelines_registered=df_pipelines.alias(\"df_pipelines_registered\")\n","\n","gt = GuidTracker()\n","\n","# create Reports nodes and lineage\n","#first find all found reports IDs\n","pipelines_list=df_pipelines_registered.collect()\n","print(\"\\npipelines_list:\")\n","display(pipelines_list)\n","print()\n","\n","\n","for pipeline in pipelines_list:\n","    source_table=df_tables_registered.filter((col(\"TableName\")== pipeline[\"SourceName\"]) & (col(\"WorkspaceID\")==pipeline[\"WorkspaceID\"]) & ((col(\"LakehouseID\")==pipeline[\"SourceContainerID\"]) | (col(\"WarehouseID\")==pipeline[\"SourceContainerID\"]))).first()\n","    source_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=source_table[\"PurviewFQN\"])[\"entities\"][0]\n","    #print(\"found source table entity =\",source_table_entity)\n","    inputs=[source_table_entity]\n","\n","    sink_table=df_tables_registered.filter((col(\"TableName\")== pipeline[\"SinkName\"]) & (col(\"WorkspaceID\")==pipeline[\"WorkspaceID\"]) & ((col(\"LakehouseID\")==pipeline[\"SinkContainerID\"]) | (col(\"WarehouseID\")==pipeline[\"SinkContainerID\"]))).first()\n","    sink_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=sink_table[\"PurviewFQN\"])[\"entities\"][0]\n","    #print(\"found sink table entity =\",sink_table_entity)\n","    outputs=[sink_table_entity]\n","    \n","    column_mapping_partial=json.loads(pipeline[\"ColumnMappings\"])\n","    column_mapping_total=[{\"ColumnMapping\": column_mapping_partial,\n","        \"DatasetMapping\": {\"Source\": source_table[\"PurviewFQN\"], \"Sink\": sink_table[\"PurviewFQN\"]}\n","        }]\n","\n","\n","    pipeline_entity=AtlasProcess(\n","        name=f'DataPipeline {pipeline[\"PipelineName\"]}',\n","        typeName=\"Lineage_Extractor_Process\",\n","        qualified_name=f'DataPipeline://workspace/{pipeline[\"WorkspaceID\"]}/pipeline/{pipeline[\"PipelineID\"]}/CopyActivity',\n","        inputs=inputs,\n","        outputs=outputs,\n","        guid=gt.get_guid(),\n","        attributes={\n","            \"columnMapping\": json.dumps(column_mapping_total),\n","            \"userDescription\": f'<div>Fabric DataPipeline Mapping<p> contained within {pipeline[\"WorkspaceName\"]}</p> <p> </p>'\n","        },\n","        # This custom attribute flips a switch inside of the Purview UI to render\n","        # the rich text description.\n","        customAttributes={\n","            \"microsoft_isDescriptionRichText\": \"true\"\n","        }\n","    )\n","    results = client.upload_entities([pipeline_entity])\n","    #print(results)\n","    #print(\"\\nupload result=\",json.dumps(results))\n","    created_GUIDs.append(results[\"guidAssignments\"][str(pipeline_entity.guid)])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8e989797-cc57-4237-86c7-5b0ef5b720c7"},{"cell_type":"markdown","source":["**The End**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2533b769-2a88-4260-a0f4-fedc7e2b3b34"},{"cell_type":"code","source":["# The list of newly created entities in Purview DG can be used to move them to a specific Collection using the Move Entity API\n","created_GUIDs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd608fb6-1069-4184-a8e7-186c6b0c3571"},{"cell_type":"markdown","source":["**APPENDIX**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1ed5061b-d901-4a04-aeb5-f209c534681f"},{"cell_type":"markdown","source":[" **Example: Scanner API call sequence to extract metadata from Fabric**\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"057f4fed-f9a9-421c-96e6-811195cee9c3"},{"cell_type":"code","source":["# Extract list of Workspaces accessible by the Service Principal\n","#\n","# IMPORTANT NOTE: The Svc Principal must NOT have \"_itemType_.ReadAll\" and \"_itemType_.ReadWriteAll\" authorizations \n","# for Tables, DataPipelines, Workspaces, etc. when used with Scanner APIs\n","#\n","# This is just an example/demo cell and does not interfere with the metadata extraction in the previous cells\n","#\n","# this part uses the API Call: GET https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified\n","get_modified_wkspc_url = f'https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified'\n","header_token={\"Authorization\": bearer, 'Content-Type': 'application/json'}\n","data_dummy={\"dummy\":\"true\"}\n","\n","response_modified_wkspc_url=requests.get(get_modified_wkspc_url,headers=header_token, data=data_dummy)\n","print(\"List of accessible workspaces that have been recently modified: \\n\",response_modified_wkspc_url.text)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d1e69c49-34dc-4a9f-8a36-2d5dfc0a106d"},{"cell_type":"code","source":["# Submit a metadata request\n","#\n","# this is just an example/demo cell and does not interfere with the metadata extraction in the previous cells\n","# you can skip it\n","#\n","get_scanner_url = f'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo?reportObjects=True&getTridentArtifacts=True&getArtifactUsers=true&lineage=True&datasourceDetails=True&datasetSchema=True&datasetExpressions=True'\n","header_token={\"Authorization\": bearer, 'Content-Type': 'application/json'}\n","\n","list_of_workspaces=f\"{{'workspaces' : [ '{demo_workspace_id}']}}\"\n","#print(\"list=\", list_of_workspaces)\n","\n","response_scanner_url=requests.post(get_scanner_url,headers=header_token, data=list_of_workspaces)\n","print(\"response=\",response_scanner_url.text)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"27b8620a-a192-4ce4-be8e-5988ea7d7e1f"},{"cell_type":"code","source":["# Check status for the metadata request, this is needed because the server may take some time to accumulate the resulting metadata: the scan is done asynchronously\n","# this part uses the API Call: GET https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/{scanId}\n","\n","scanid=response_scanner_url.json().get('id')\n","print(\"scan_id=\",scanid)\n","\n","get_scanstatus_url = f'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/{scanid}'\n","header_token={\"Authorization\": bearer, 'Content-Type': 'application/json'}\n","\n","response_scanstatus_url=requests.get(get_scanstatus_url,headers=header_token)\n","print(\"scan status is:\\n\",response_scanstatus_url.text)\n","# check the \"status\" to be \"Succeeded\" before running the next cell"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1fb3f226-9c31-4404-9a45-b53c2ce04f40"},{"cell_type":"code","source":["#4 Finally get metadata\n","#this part uses the API Call: GET https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/{scanId}\n","# this is just an example/demo cell and does not interfere with the metadata extraction in the following cells\n","# you can skip it\n","get_scanresult_url = f'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/{scanid}'\n","header_token={\"Authorization\": bearer, 'Content-Type': 'application/json'}\n","\n","workspaces_response=requests.get(get_scanresult_url,headers=header_token)\n","print(\"workspaces_response:\\n\",workspaces_response.text)\n","# The output can be quite bulky, so I suggest to explore it via a good JSON editor or specific json queries."],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ac98db01-c133-4da5-8e60-dab0e2e76b42"},{"cell_type":"markdown","source":[" **Example: Metadata extraction using SQL**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"308aa746-663f-451e-ab98-b6f2238f966b"},{"cell_type":"code","source":["#EXTRACT TABLES USING SQL\n","# this is provided just as an example \n","df2 = spark.sql(\"SELECT name, object_id, create_date, modify_date, type, type_desc from sys.tables\")\n","display(df2)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f02dd016-026e-4ed1-91ef-cccd2dad7ae0"},{"cell_type":"markdown","source":[" **Example: use of Semantic Link Library**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"44d2cb68-1256-4e3f-8a0a-5c1a7fb4e6ab"},{"cell_type":"code","source":["# Example Use of Semantic Link Labs\n","# For getting report source tables and columns in Fabric\n","#\n","# this is just an example/demo cell and does not interfere with the metadata extraction in the previous cells\n","# \n","report_name = '<Name of report here>' # Enter the report name\n","report_workspace = demo_workspace # Enter the workspace in which the report exists\n","rpt = ReportWrapper(report=report_name, workspace=report_workspace)\n","\n","rpt.list_semantic_model_objects()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"985f0ec5-80a5-4957-9674-081b6fb6a08d"},{"cell_type":"code","source":["# Example Use of Semantic Link Labs\n","# For getting source tables and columns for Fabric report whose sources are in Azure Databricks\n","#\n","# this is just an example/demo cell and does not interfere with the metadata extraction in the previous cells\n","# \n","import sempy.fabric as sdtfabric\n","import sempy_labs as labs\n","dataset = '<Name of report here>' # Enter report name with Databricks source tables\n","workspace = None # Enter workspace name\n"," \n","df = sdtfabric.list_partitions(dataset=dataset, workspace=workspace)\n","display(df)\n","# use the M code in the \"Query\" column in the result to get source tables and fields from non-Fabric sources e.g. Azure Databricks"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b7a6cadd-873a-4338-97c6-10c51074be16"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"7200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"37cdc82e-9d7c-4406-8670-6bf5dbeacb66","default_lakehouse_name":"lakehouse_for_lineage","default_lakehouse_workspace_id":"6f587540-db76-497f-acf8-170bc6d88580"}}},"nbformat":4,"nbformat_minor":5}