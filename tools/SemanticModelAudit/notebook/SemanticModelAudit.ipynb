{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527f53b1-6a78-4d95-9e2c-a27a85aea446",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Fabric Semantic Model Audit\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook is designed to perform a comprehensive audit of Fabric semantic models by collecting and tracking logs and metadata over time. It supports ongoing evaluation of model performance, usage patterns, and metadata changes, which can help you:\n",
    "\n",
    "- **Identify Unused or Obsolete Columns:** Determine which columns may be removed from the model or underlying Delta tables.\n",
    "- **Monitor Performance:** Evaluate DAX query performance over time.\n",
    "- **Track Model Usage:** Collect historical query logs and usage statistics.\n",
    "\n",
    "### Key Components and Functionality\n",
    "\n",
    "1. **Initial Setup and Requirements:**\n",
    "   - **Workspace Monitoring:** The notebook requires that Workspace Monitoring is enabled. See [this blog post](https://blog.fabric.microsoft.com/blog/announcing-public-preview-of-workspace-monitoring) for guidance.\n",
    "   - **Scheduled Execution:** To capture detailed usage statistics, it is recommended to schedule this notebook to run multiple times per day (e.g., 6 times per day).\n",
    "   - **Configure Run Parameters:** Configure the run parameters at the top of the notebook based on your models and other requirements.\n",
    "   - **Logging Datastore:** Attach a lakehouse so log tables can be saved. \n",
    "\n",
    "1. **Core Functionality:**\n",
    "   - **Metadata Capture:** Functions to retrieve and save semantic model objects (columns and measures) along with dependencies using Fabric API calls.\n",
    "   - **Query Log Collection:** Modules to capture query counts and detailed logs, which help track model usage and performance over specified time intervals.\n",
    "   - **Unused Columns and Source Mapping:** Compares lakehouse/warehouse metadata with model usage to detect columns that are no longer utilized.\n",
    "   - **Cold Cache Performance:** Deploys a cloned version of the model to measure cold-cache performance via parallel DAX queries and trace log analysis.\n",
    "   - **Resident Statistics:** Captures statistics about column residency (e.g., memory load, sizes) to further evaluate model performance.\n",
    "\n",
    "1. **Star Schema Generation:**\n",
    "   - The notebook constructs several star schema tables for in-depth analysis:\n",
    "      - **DIM_ModelObject:** Latest definitions for columns, measures, and unused columns.\n",
    "      - **DIM_Model:** Basic model details.\n",
    "      - **DIM_Report:** Report details.\n",
    "      - **DIM_User:** Standardized user info from logs.\n",
    "      - **FACT_ModelObjectQueryCount:** Ties query counts to model objects and their dependencies.\n",
    "      - **FACT_ModelLogs:** Detailed logs for performance tracking.\n",
    "      - **FACT_ModelObjectStatistics:** Combines daily statistics such as cold cache performance and memory size for columns.\n",
    "\n",
    "1. **Orchestration and Execution:**\n",
    "   - The main orchestration function (`collect_model_statistics`) processes each model sequentially, performing all capture steps (metadata, logs, unused columns, cold cache, resident statistics) and finally marking each run as completed or failed.\n",
    "   - The notebook concludes by writing the star schema tables to Delta format, ready for import into a Fabric semantic model for further analysis.\n",
    "\n",
    "### Usage Notes\n",
    "\n",
    "- **Scheduling and Monitoring:** To capture granular historical data, consider scheduling this notebook to run at regular intervals throughout the day.\n",
    "- **Configuration:** Adjust the parameters (e.g., `max_queries_daily`, `max_workers`) to suit your environment and workload.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74d5d8-2ab6-4114-af39-6549df850d59",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Install the Semantic Link Labs package\n",
    "Check [here](https://pypi.org/project/semantic-link-labs/) to see the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c6da9-1527-414a-9130-ee52a2f805ed",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad99cd-1e49-401b-970c-bf0c00edf2a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5bfeed-3de0-4c21-931d-2ed258587773",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import builtins\n",
    "import functools\n",
    "import math\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from uuid import uuid4\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "# Local Project-Specific Modules\n",
    "import sempy.fabric as fabric\n",
    "import sempy_labs as labs\n",
    "\n",
    "# Third-Party Libraries\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.functions import col, lit, collect_set, udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868df2b0-07c5-4bca-84a5-4452fc6d5781",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Set Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d52b16f-b4c3-4bb3-a50c-d6153fb3d882",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Models to collect statistics for\n",
    "models = [\n",
    "    {\n",
    "        \"model_name\": \"Semantic Model Name\", # The name of the target model\n",
    "        \"model_workspace_name\": \"Semantic Model Workspace Name\", # The name of the target model workspace\n",
    "        \"datastore_name\": \"Semantic Model Datastore Name\", # Either a Fabric Lakehouse or Warehouse - Only needed for Direct Lake models. Will also work with a Fabric warehouse\n",
    "        \"datastore_workspace_name\": \"Semantic Model Lakehouse Workspace Name\", # Either a Fabric Lakehouse or Warehouse - Only needed for Direct Lake models. Will also work with a Fabric warehouse\n",
    "        \"log_analytics_kusto_uri\": \"\",     # Optional: provide your own Kusto URI or leave as empty string (leaving blank will result in using the get_workspace_monitoring_info function)\n",
    "        \"log_analytics_kusto_database_uuid\": \"\", # Optional: provide your own Kusto DB Uuid or leave as empty string (leaving blank will result in using the get_workspace_monitoring_info function)\n",
    "    },\n",
    "]\n",
    "\n",
    "# Settings for collecting cold cache performance measurements.\n",
    "collect_cold_cache_measurements = True # Only recommended for Direct Lake or small Import models\n",
    "max_queries_daily = 1                  # Maximum cold cache performance queries per column per day\n",
    "max_workers = 50                       # Number of concurrent column queries\n",
    "\n",
    "# Determine target workspace for the cloned model (used in cold cache measurements)\n",
    "cold_cache_target_workspace_name = fabric.resolve_workspace_name()\n",
    "\n",
    "# Settings for model query collection (how many days back to collect data)\n",
    "max_days_ago_to_collect = 30  # Collect data from 1 to 30 days ago (only days with no data are collected)\n",
    "\n",
    "# Adjustment for recent logs: exclude intervals within this many hours of the as-of datetime for the most recent day\n",
    "min_hours_before_current = 3\n",
    "\n",
    "# Principal name collection mode:\n",
    "#   0 = Keep original ExecutingUser,\n",
    "#   1 = Anonymize using historical mapping,\n",
    "#   2 = Always set to \"Masked\"\n",
    "collect_principal_names = 0\n",
    "mask_principal_names_after_days = 30  # Set to 0 if masking is not required\n",
    "\n",
    "# Define user groups to bucket executing users found in the object count and detailed logs.\n",
    "user_groups = {  \n",
    "    \"Engineers Example\": [\n",
    "        \"engineer1@microsoft.com\",\n",
    "        \"engineer2@microsoft.com\",\n",
    "    ],\n",
    "    \"Project Managers Example\": [\n",
    "        \"pm1@microsoft.com\",\n",
    "        \"pm2@microsoft.com\",\n",
    "    ],\n",
    "}\n",
    "default_user_group = \"Other Users\"\n",
    "\n",
    "# Delta table names for historical data (new records are appended each run)\n",
    "historical_table_names = {\n",
    "    \"run_history\": \"run_history\",\n",
    "    \"model_columns\": \"model_columns\",\n",
    "    \"model_measures\": \"model_measures\",\n",
    "    \"object_query_count\": \"model_object_query_count\",\n",
    "    \"detailed_logs\": \"model_detailed_logs\",\n",
    "    \"object_mapping\": \"model_object_mapping\",\n",
    "    \"dependencies\": \"model_dependencies\",\n",
    "    \"unused_columns\": \"unused_delta_table_columns\",\n",
    "    \"source_mapping\": \"model_column_source_mapping\",\n",
    "    \"cold_cache_measurements\": \"model_column_cold_cache_measurement\",\n",
    "    \"resident_statistics\": \"model_column_resident_statistics\",\n",
    "    \"source_reports\": \"source_reports\",\n",
    "    \"source_app_reports\": \"source_app_reports\",\n",
    "}\n",
    "\n",
    "# Delta table names for star schema (these tables are overwritten each run)\n",
    "star_schema_table_names = {\n",
    "    \"dim_model_object\": \"DIM_ModelObject\",\n",
    "    \"dim_model\": \"DIM_Model\",\n",
    "    \"dim_report\": \"DIM_Report\",\n",
    "    \"dim_user\": \"DIM_User\",\n",
    "    \"fact_model_object_query_count\": \"FACT_ModelObjectQueryCount\",\n",
    "    \"fact_detailed_logs\": \"FACT_ModelLogs\",\n",
    "    \"fact_model_statistics\": \"FACT_ModelObjectStatistics\",\n",
    "}\n",
    "\n",
    "# Flags for table management\n",
    "force_delete_historical_tables = False\n",
    "force_delete_incomplete_runs = True\n",
    "\n",
    "# Abfss base path\n",
    "abfss_base_path = \"onelake.dfs.fabric.microsoft.com\"\n",
    "\n",
    "# Ensure Spark uses case-sensitive SQL\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749dae3e-8429-435c-b927-b741cfe30592",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Helper Functions: Logging, Retry, and Saving DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6fee32-f67f-4c5c-9377-02beeb956858",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Thread-local storage to track call depth (used for logging indentation)\n",
    "# This allows each thread to maintain its own \"call depth\" counter independently.\n",
    "_thread_local = threading.local()\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def indented_print(indent_level: int):\n",
    "    \"\"\"\n",
    "    A context manager that temporarily replaces the built-in print function.\n",
    "    It prepends a specific indent (based on indent_level) to every print output,\n",
    "    which makes nested function calls easier to trace visually.\n",
    "    \"\"\"\n",
    "    # Save the original print function so it can be restored later.\n",
    "    original_print = builtins.print\n",
    "\n",
    "    def custom_print(*args, **kwargs):\n",
    "        # Create an indent by repeating four spaces per indent level.\n",
    "        indent = \"    \" * indent_level\n",
    "        # Call the original print with the indented message.\n",
    "        original_print(indent + \" \".join(map(str, args)), **kwargs)\n",
    "\n",
    "    # Replace the built-in print with our custom_print.\n",
    "    builtins.print = custom_print\n",
    "    try:\n",
    "        # Yield control back to the caller.\n",
    "        yield\n",
    "    finally:\n",
    "        # Restore the original print function after exiting the block.\n",
    "        builtins.print = original_print\n",
    "\n",
    "\n",
    "def log_function_calls(func):\n",
    "    \"\"\"\n",
    "    Decorator that logs the start and end of a function call using indented printing.\n",
    "    It uses a thread-local counter to indent log messages, so nested calls are visually offset.\n",
    "    \n",
    "    Example:\n",
    "        @log_function_calls\n",
    "        def my_func():\n",
    "            ...\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Initialize call_depth for the thread if not already set.\n",
    "        if not hasattr(_thread_local, \"call_depth\"):\n",
    "            _thread_local.call_depth = 0\n",
    "\n",
    "        # Capture the current call depth to determine the indent.\n",
    "        indent = _thread_local.call_depth\n",
    "\n",
    "        # Log the start message using the indented_print context manager.\n",
    "        with indented_print(indent):\n",
    "            print(f\"âœ… {func.__name__} - Starting\")\n",
    "\n",
    "        # Increase the call depth as we enter the function.\n",
    "        _thread_local.call_depth += 1\n",
    "        try:\n",
    "            # Log any output inside the function with increased indentation.\n",
    "            with indented_print(_thread_local.call_depth):\n",
    "                result = func(*args, **kwargs)\n",
    "        finally:\n",
    "            # Decrease the call depth on function exit.\n",
    "            _thread_local.call_depth -= 1\n",
    "            with indented_print(_thread_local.call_depth):\n",
    "                print(f\"âœ… {func.__name__} - Ending\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def retry(exceptions, num_retries=3, initial_delay=5, backoff_factor=2, logger=None):\n",
    "    \"\"\"\n",
    "    Decorator factory that returns a decorator to automatically retry a function call if it raises\n",
    "    one of the specified exceptions. It uses exponential backoff between retries.\n",
    "    \n",
    "    Parameters:\n",
    "        exceptions (tuple or Exception): Exception(s) that trigger a retry.\n",
    "        num_retries (int): Number of retry attempts before giving up.\n",
    "        initial_delay (int): Initial delay in seconds before the first retry.\n",
    "        backoff_factor (int): Factor by which the delay is multiplied after each retry.\n",
    "        logger (callable, optional): Logger function for reporting retries (defaults to print).\n",
    "    \n",
    "    Usage:\n",
    "        @retry((ValueError,), num_retries=3, initial_delay=2, backoff_factor=2)\n",
    "        def my_func():\n",
    "            ...\n",
    "    \"\"\"\n",
    "    def decorator_retry(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper_retry(*args, **kwargs):\n",
    "            attempts, delay = num_retries, initial_delay\n",
    "            # Retry loop: try the function until attempts are exhausted.\n",
    "            while attempts > 1:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except exceptions as e:\n",
    "                    msg = f\"âš ï¸ {func.__name__} failed with {e}, retrying in {delay} seconds...\"\n",
    "                    if logger:\n",
    "                        logger(msg)\n",
    "                    else:\n",
    "                        print(msg)\n",
    "                    # Pause execution for 'delay' seconds before retrying.\n",
    "                    time.sleep(delay)\n",
    "                    attempts -= 1\n",
    "                    # Increase the delay for the next attempt.\n",
    "                    delay *= backoff_factor\n",
    "            # Final attempt: if previous retries failed, let any exception propagate.\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper_retry\n",
    "    return decorator_retry\n",
    "\n",
    "\n",
    "@log_function_calls\n",
    "def save_dataframe_to_delta_table(data, table_name: str, context: dict, **extra_columns) -> None:\n",
    "    \"\"\"\n",
    "    Appends a pandas or Spark DataFrame to a Delta table with additional contextual columns.\n",
    "    Extra columns provided as keyword arguments are added to the DataFrame before writing.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pandas.DataFrame or pyspark.sql.DataFrame): The input data.\n",
    "        table_name (str): The target Delta table name.\n",
    "        context (dict): A context dictionary that must contain keys:\n",
    "            - 'as_of_datetime'\n",
    "            - 'as_of_date'\n",
    "            - 'run_uuid'\n",
    "            - 'source_model_uuid'\n",
    "        **extra_columns: Any additional columns to add to the DataFrame.\n",
    "    \n",
    "    The function converts a pandas DataFrame to a Spark DataFrame if necessary and ensures\n",
    "    that column names have no spaces. It then writes the DataFrame to the specified Delta table.\n",
    "    \"\"\"\n",
    "    # Default columns added to every record from the context.\n",
    "    default_cols = {\n",
    "        \"AsOfDateTime\": context[\"as_of_datetime\"],\n",
    "        \"AsOfDate\": context[\"as_of_date\"],\n",
    "        \"RunUuid\": context[\"run_uuid\"],\n",
    "        \"ModelUuid\": context[\"source_model_uuid\"],\n",
    "    }\n",
    "    # Merge default columns with any extra columns provided.\n",
    "    all_extra_cols = {**default_cols, **extra_columns}\n",
    "\n",
    "    def add_columns(df, cols: dict):\n",
    "        \"\"\"\n",
    "        Helper function to add extra columns to a DataFrame.\n",
    "        Works for both pandas and Spark DataFrames.\n",
    "        \"\"\"\n",
    "        for col_name, value in cols.items():\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                # Direct assignment for pandas DataFrame.\n",
    "                df[col_name] = value\n",
    "            else:\n",
    "                # For Spark DataFrame, use the withColumn method and lit() to add constant columns.\n",
    "                df = df.withColumn(col_name, lit(value))\n",
    "        return df\n",
    "\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # For pandas DataFrame, remove spaces in column names for consistency.\n",
    "        data.columns = data.columns.str.replace(\" \", \"\", regex=True)\n",
    "        data = add_columns(data, all_extra_cols)\n",
    "        # Convert the cleaned pandas DataFrame into a Spark DataFrame.\n",
    "        spark_df = spark.createDataFrame(data)\n",
    "    elif isinstance(data, pyspark.sql.DataFrame):\n",
    "        # For Spark DataFrame, rename columns by removing any spaces.\n",
    "        for c in data.columns:\n",
    "            data = data.withColumnRenamed(c, c.replace(\" \", \"\"))\n",
    "        spark_df = add_columns(data, all_extra_cols)\n",
    "    else:\n",
    "        # Raise error if data is not a recognized DataFrame type.\n",
    "        raise TypeError(\"âŒ Unsupported data type. Expected pandas or Spark DataFrame.\")\n",
    "\n",
    "    try:\n",
    "        # Write the DataFrame to the specified Delta table.\n",
    "        # The \"mergeSchema\" option allows the schema to evolve if needed.\n",
    "        spark_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n",
    "        print(f\"âœ… Table `{table_name}` updated successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to save table `{table_name}`. Error: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bef74-dd30-42ef-a177-16c1963355bb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Run History & Cleanup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30690f3b-be06-4ec8-8674-8664501c0b71",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def record_run_start(context: dict) -> None:\n",
    "    \"\"\"\n",
    "    Records the start of a run by inserting a new record into the run_history table.\n",
    "    \n",
    "    It creates a DataFrame that includes the current start time, a placeholder for the end time,\n",
    "    and a status of 'started'. Additional context data (except for keys that are handled elsewhere)\n",
    "    is included to help identify the run.\n",
    "    \"\"\"\n",
    "    # Filter out keys that are specific to run identification and handled separately,\n",
    "    # ensuring that the DataFrame only includes the general context information.\n",
    "    context_filtered = {\n",
    "        k: v for k, v in context.items() if k not in [\"run_uuid\", \"source_model_uuid\"]\n",
    "    }\n",
    "    # Construct a pandas DataFrame with a single row representing the run start.\n",
    "    # Both StartTime and EndTime are set to the current timestamp; EndTime will be updated upon completion.\n",
    "    run_start_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                **context_filtered,\n",
    "                \"StartTime\": datetime.now(),  # Capture the current time as the start time.\n",
    "                \"EndTime\": datetime.now(),    # Placeholder for end time; to be updated later.\n",
    "                \"Status\": \"started\",          # Set initial status as 'started'.\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    # Write the run start DataFrame to the Delta table designated for run history.\n",
    "    save_dataframe_to_delta_table(\n",
    "        data=run_start_df,\n",
    "        table_name=historical_table_names[\"run_history\"],\n",
    "        context=context,\n",
    "    )\n",
    "    # Log a confirmation message including the run's unique identifier.\n",
    "    print(f\"âœ… Recorded run start for UUID: {context['run_uuid']}\")\n",
    "\n",
    "\n",
    "@log_function_calls\n",
    "def record_run_completion(context: dict, status: str) -> None:\n",
    "    \"\"\"\n",
    "    Updates the run_history table to mark the run as completed or failed.\n",
    "    \n",
    "    It sets the EndTime to the current timestamp and updates the run's Status.\n",
    "    The function ensures that the run_uuid is present and safely escapes it for SQL usage.\n",
    "    \"\"\"\n",
    "    # Retrieve the unique run identifier from the context.\n",
    "    run_uuid = context.get(\"run_uuid\")\n",
    "    if not run_uuid:\n",
    "        raise ValueError(\"âŒ 'run_uuid' missing from context.\")\n",
    "    # Escape single quotes in the run_uuid to prevent SQL injection or syntax issues.\n",
    "    escaped_uuid = run_uuid.replace(\"'\", \"''\")\n",
    "    # Format the current datetime as a string suitable for SQL TIMESTAMP.\n",
    "    end_time_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Construct the SQL update query to set EndTime and Status for the given run.\n",
    "    update_query = f\"\"\"\n",
    "        UPDATE {historical_table_names[\"run_history\"]}\n",
    "        SET EndTime = CAST('{end_time_str}' AS TIMESTAMP),\n",
    "            Status = '{status}'\n",
    "        WHERE RunUuid = '{escaped_uuid}'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Execute the SQL update query using Spark's SQL interface.\n",
    "        spark.sql(update_query)\n",
    "        # Log a success message indicating the run has been updated.\n",
    "        print(f\"âœ… Run UUID: {run_uuid} updated with status '{status}'.\")\n",
    "    except Exception as e:\n",
    "        # Log the error if the update fails and re-raise the exception.\n",
    "        print(f\"âŒ Failed to update run UUID: {run_uuid}. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@log_function_calls\n",
    "def cleanup_incomplete_runs() -> None:\n",
    "    \"\"\"\n",
    "    Removes records associated with runs that did not complete successfully.\n",
    "    \n",
    "    The function performs two main operations:\n",
    "      1. Deletes records from all historical tables (except run_history) corresponding to incomplete runs.\n",
    "      2. Updates the run_history table to mark those incomplete runs as 'removed'.\n",
    "      \n",
    "    This cleanup helps maintain data consistency by removing or flagging partially recorded runs.\n",
    "    \"\"\"\n",
    "    # Check if the run_history table exists; if not, there is nothing to clean up.\n",
    "    if not spark.catalog.tableExists(historical_table_names[\"run_history\"]):\n",
    "        print(\"âœ… run_history table does not exist yet. No cleanup necessary.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Retrieve all run_uuids from run_history where the Status is not 'completed' or 'removed'.\n",
    "        incomplete_df = (\n",
    "            spark.table(historical_table_names[\"run_history\"])\n",
    "            .filter(~col(\"Status\").isin(\"completed\", \"removed\"))\n",
    "            .select(\"RunUuid\")\n",
    "        )\n",
    "        # Collect the run uuids from the DataFrame to a list.\n",
    "        incomplete_uuids = [row[\"RunUuid\"] for row in incomplete_df.collect()]\n",
    "\n",
    "        # If there are no incomplete runs, log the info and exit.\n",
    "        if not incomplete_uuids:\n",
    "            print(\"âœ… No incomplete runs to clean.\")\n",
    "            return\n",
    "\n",
    "        print(f\"âœ… Found {len(incomplete_uuids)} incomplete run(s); proceeding with cleanup.\")\n",
    "        # Escape each run_uuid for safe SQL query usage.\n",
    "        escaped_uuids = [uuid.replace(\"'\", \"''\") for uuid in incomplete_uuids]\n",
    "        # Create a comma-separated string of escaped run_uuids for use in SQL IN clause.\n",
    "        uuid_list_str = \", \".join(f\"'{uuid}'\" for uuid in escaped_uuids)\n",
    "\n",
    "        # Iterate over each historical table (except run_history) to remove incomplete run records.\n",
    "        for logical_name, table in historical_table_names.items():\n",
    "            if logical_name == \"run_history\":\n",
    "                continue  # Skip the run_history table in this deletion loop.\n",
    "            try:\n",
    "                # Only attempt deletion if the table exists.\n",
    "                if not spark.catalog.tableExists(table):\n",
    "                    print(f\"âœ… Table {table} not found. Skipping deletion.\")\n",
    "                    continue\n",
    "                # Construct and execute the deletion query for the current table.\n",
    "                delete_query = f\"DELETE FROM {table} WHERE RunUuid IN ({uuid_list_str})\"\n",
    "                spark.sql(delete_query)\n",
    "                print(f\"âœ… Deleted records in table {table} for incomplete runs.\")\n",
    "            except Exception as e:\n",
    "                # Log the error for the current table and continue with the next one.\n",
    "                print(f\"âŒ Failed to clean table {table}. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # After cleaning other tables, update the run_history table to mark incomplete runs as 'removed'.\n",
    "        update_query = f\"\"\"\n",
    "            UPDATE {historical_table_names[\"run_history\"]}\n",
    "            SET Status = 'removed'\n",
    "            WHERE RunUuid IN ({uuid_list_str})\n",
    "        \"\"\"\n",
    "        spark.sql(update_query)\n",
    "        print(f\"âœ… Marked {len(incomplete_uuids)} incomplete run(s) as removed.\")\n",
    "    except Exception as e:\n",
    "        # Log and re-raise any exception encountered during the cleanup process.\n",
    "        print(f\"âŒ Cleanup failed. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def drop_historical_tables() -> None:\n",
    "    \"\"\"\n",
    "    Drops all historical Delta tables.\n",
    "    \n",
    "    Use this function with caution as it permanently deletes all historical audit data\n",
    "    stored in the tables defined in the 'historical_table_names' mapping.\n",
    "    \"\"\"\n",
    "    # Loop through each historical table and attempt to drop it.\n",
    "    for logical_name, table in historical_table_names.items():\n",
    "        try:\n",
    "            print(f\"ðŸ—‘ï¸ Dropping table: {table}\")\n",
    "            # Execute the DROP TABLE command; IF EXISTS ensures no error is thrown if the table doesn't exist.\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "            print(f\"âœ… Dropped table: {table}\")\n",
    "        except Exception as e:\n",
    "            # Log any failure to drop a table.\n",
    "            print(f\"âŒ Failed to drop table `{table}`. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4866699-d2e4-4452-a14c-4c5a915dfeb0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Capturing Semantic Model Objects & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3ef49-801a-4761-ac06-a7015eee2f8f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def capture_semantic_model_objects(context: dict) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Retrieves columns and measures for the specified semantic model.\n",
    "\n",
    "    It uses Fabric API calls to capture:\n",
    "      - Columns (with extended metadata) from the model.\n",
    "      - Measures from the model.\n",
    "    The captured data is saved to Delta tables for historical tracking.\n",
    "\n",
    "    Returns:\n",
    "      A tuple (model_columns, model_measures) as pandas DataFrames.\n",
    "    \"\"\"\n",
    "    # Check that the context contains the required keys for API calls.\n",
    "    for key in [\"source_model_uuid\", \"source_model_workspace_uuid\"]:\n",
    "        if key not in context:\n",
    "            raise KeyError(f\"âŒ Missing context key: '{key}'\")\n",
    "    try:\n",
    "        # Refresh the Table Object Model (TOM) cache to ensure up-to-date metadata.\n",
    "        fabric.refresh_tom_cache(context[\"source_model_workspace_uuid\"])\n",
    "\n",
    "        # Retrieve model columns with extended metadata via Sempy.\n",
    "        model_columns = fabric.list_columns(\n",
    "            dataset=context[\"source_model_uuid\"],\n",
    "            extended=True,\n",
    "            workspace=context[\"source_model_workspace_uuid\"],\n",
    "        )\n",
    "        # Clean column names by removing spaces for consistency.\n",
    "        model_columns.columns = model_columns.columns.str.replace(\" \", \"\", regex=True)\n",
    "        # Save the captured columns to the Delta table for historical tracking.\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=model_columns,\n",
    "            table_name=historical_table_names[\"model_columns\"],\n",
    "            context=context,\n",
    "        )\n",
    "\n",
    "        # Similarly, capture model measures from Sempy.\n",
    "        model_measures = fabric.list_measures(\n",
    "            dataset=context[\"source_model_uuid\"],\n",
    "            workspace=context[\"source_model_workspace_uuid\"],\n",
    "        )\n",
    "        # Remove spaces from measure column names.\n",
    "        model_measures.columns = model_measures.columns.str.replace(\" \", \"\", regex=True)\n",
    "        # Save the captured measures to the corresponding Delta table.\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=model_measures,\n",
    "            table_name=historical_table_names[\"model_measures\"],\n",
    "            context=context,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"âŒ Failed to capture model objects for model `{context['source_model_uuid']}`. Error: {e}\"\n",
    "        )\n",
    "        raise\n",
    "    return model_columns, model_measures\n",
    "\n",
    "\n",
    "def find_dependencies_recursive(\n",
    "    dependencies: pd.DataFrame,\n",
    "    root_table: str,\n",
    "    root_object: str,\n",
    "    ref_table: str,\n",
    "    ref_object: str,\n",
    "    level: int,\n",
    "    accum: list,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Recursively finds dependencies for a measure.\n",
    "\n",
    "    It traverses the dependency DataFrame (obtained from a DAX query) to:\n",
    "      - Append each dependency with the current recursion level.\n",
    "      - Recursively follow further dependencies if the referenced object is a measure.\n",
    "\n",
    "    Parameters:\n",
    "      dependencies (pd.DataFrame): DataFrame containing dependency data.\n",
    "      root_table (str): The table name of the original measure.\n",
    "      root_object (str): The measure name for which dependencies are being traced.\n",
    "      ref_table (str): The table name of the current referenced object.\n",
    "      ref_object (str): The name of the current referenced object.\n",
    "      level (int): The current recursion depth (starting at 1).\n",
    "      accum (list): List used to accumulate dependency entries.\n",
    "    \"\"\"\n",
    "    # Filter the dependency DataFrame to only include rows matching the current reference.\n",
    "    refs = dependencies[\n",
    "        (dependencies[\"[TABLE]\"] == ref_table) &\n",
    "        (dependencies[\"[OBJECT]\"] == ref_object)\n",
    "    ]\n",
    "    for _, row in refs.iterrows():\n",
    "        # Append dependency details to the accumulator.\n",
    "        accum.append(\n",
    "            {\n",
    "                \"ObjectType\": \"MEASURE\",  # All entries here are measures.\n",
    "                \"TableName\": root_table,  # Original measure's table.\n",
    "                \"ObjectName\": root_object,  # Original measure's name.\n",
    "                \"ReferencedObjectType\": row[\"[REFERENCED_OBJECT_TYPE]\"],\n",
    "                \"ReferencedTableName\": row[\"[REFERENCED_TABLE]\"],\n",
    "                \"ReferencedObjectName\": row[\"[REFERENCED_OBJECT]\"],\n",
    "                \"Level\": level,  # Record the current recursion level.\n",
    "            }\n",
    "        )\n",
    "        # If the referenced object is itself a measure, continue recursion.\n",
    "        if row[\"[REFERENCED_OBJECT_TYPE]\"] == \"MEASURE\":\n",
    "            find_dependencies_recursive(\n",
    "                dependencies,\n",
    "                root_table,\n",
    "                root_object,\n",
    "                row[\"[REFERENCED_TABLE]\"],\n",
    "                row[\"[REFERENCED_OBJECT]\"],\n",
    "                level + 1,  # Increase recursion level.\n",
    "                accum,\n",
    "            )\n",
    "\n",
    "\n",
    "@log_function_calls\n",
    "def capture_semantic_model_dependencies(\n",
    "    context: dict, model_measures: pd.DataFrame\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Captures dependencies for model measures.\n",
    "\n",
    "    It runs a DAX query to retrieve dependency data for measures,\n",
    "    then recursively traces dependencies for each measure using the provided model_measures DataFrame.\n",
    "    The complete dependency mapping is saved to the Delta table for dependencies.\n",
    "    \"\"\"\n",
    "    # Execute a DAX query to fetch dependency data for measures.\n",
    "    model_measure_deps = fabric.evaluate_dax(\n",
    "        dataset=context[\"source_model_uuid\"],\n",
    "        workspace=context[\"source_model_workspace_uuid\"],\n",
    "        dax_string=\"\"\"\n",
    "            EVALUATE\n",
    "            FILTER(\n",
    "                INFO.CALCDEPENDENCY(\"OBJECT_TYPE\", \"MEASURE\"),\n",
    "                [REFERENCED_OBJECT_TYPE] IN { \"COLUMN\", \"MEASURE\" }\n",
    "            )\n",
    "        \"\"\",\n",
    "    )\n",
    "    accum = []  # Initialize an empty list to accumulate dependency entries.\n",
    "    # Iterate over each measure to start the recursive dependency search.\n",
    "    for _, row in model_measures.iterrows():\n",
    "        # Begin recursion for each measure using its own table and name as the root.\n",
    "        find_dependencies_recursive(\n",
    "            dependencies=model_measure_deps,\n",
    "            root_table=row[\"TableName\"],\n",
    "            root_object=row[\"MeasureName\"],\n",
    "            ref_table=row[\"TableName\"],\n",
    "            ref_object=row[\"MeasureName\"],\n",
    "            level=1,  # Start at level 1.\n",
    "            accum=accum,\n",
    "        )\n",
    "    # Convert the accumulated dependency records to a Spark DataFrame and save to Delta table.\n",
    "    save_dataframe_to_delta_table(\n",
    "        data=spark.createDataFrame(accum),\n",
    "        table_name=historical_table_names[\"dependencies\"],\n",
    "        context=context,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a47ef-833f-4b9c-814c-2d2c14e099b4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Processing Semantic Model Objects & Saving Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dffae9-b257-4d94-bbac-dad9d7913302",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def process_semantic_model_objects(\n",
    "    model_objects: pd.DataFrame, object_type: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes the metadata for model objects (columns or measures) into a mapping DataFrame.\n",
    "\n",
    "    For columns, it produces multiple formatting variants (e.g., quoted, unquoted, bracketed)\n",
    "    to facilitate later matching. For measures, only a single representation is generated.\n",
    "\n",
    "    Returns:\n",
    "      A DataFrame with standardized object mapping information, including:\n",
    "         - TableName: Original table name.\n",
    "         - ObjectName: The base name (column or measure name).\n",
    "         - ObjectType: 'COLUMN' or 'MEASURE'.\n",
    "         - ModelObject: The variant string representation for matching.\n",
    "    \"\"\"\n",
    "\n",
    "    def map_row(row: pd.Series) -> list:\n",
    "        # Process based on whether the object is a column or a measure.\n",
    "        if object_type == \"COLUMN\":\n",
    "            tbl = row[\"TableName\"]\n",
    "            col_name = row[\"ColumnName\"]\n",
    "            # Generate different string variants for the column.\n",
    "            quoted_variant = f\"'{tbl}'[{col_name}]\"   # e.g., 'TableName'[ColumnName]\n",
    "            unquoted_variant = quoted_variant.replace(\"'\", \"\")  # Remove quotes, e.g., TableName[ColumnName]\n",
    "            bracket_variant = f\"[{tbl}].[{col_name}]\"   # e.g., [TableName].[ColumnName]\n",
    "            # If the table name includes spaces, avoid the unquoted variant.\n",
    "            variants = (\n",
    "                [quoted_variant, bracket_variant]\n",
    "                if \" \" in tbl\n",
    "                else [quoted_variant, unquoted_variant, bracket_variant]\n",
    "            )\n",
    "            obj_name = col_name\n",
    "        else:\n",
    "            # For measures, generate only one variant.\n",
    "            measure = row[\"MeasureName\"]\n",
    "            variants = [f\"[{measure}]\"]  # Format measure as [MeasureName]\n",
    "            obj_name = measure\n",
    "\n",
    "        # Base dictionary holds common mapping fields.\n",
    "        base = {\n",
    "            \"TableName\": row[\"TableName\"],\n",
    "            \"ObjectName\": obj_name,\n",
    "            \"ObjectType\": object_type,\n",
    "        }\n",
    "        # Return a list of dictionaries, one for each variant.\n",
    "        return [{**base, \"ModelObject\": variant} for variant in variants]\n",
    "\n",
    "    # For each row in the input DataFrame, apply map_row to produce a list of mapping dictionaries.\n",
    "    mapped = [item for _, row in model_objects.iterrows() for item in map_row(row)]\n",
    "    # Convert the list of mapping dictionaries into a pandas DataFrame.\n",
    "    return pd.DataFrame(mapped)\n",
    "\n",
    "\n",
    "@log_function_calls\n",
    "def save_report_measure_mappings(distinct_objects: set, context: dict) -> None:\n",
    "    \"\"\"\n",
    "    Saves new mappings for REPORT MEASURE objects.\n",
    "\n",
    "    It parses each report measure string using regular expressions to extract:\n",
    "      - The table name (from content within square brackets or single quotes).\n",
    "      - The measure name (from a specific DAX pattern).\n",
    "    These mappings are then saved to the object_mapping Delta table.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    # Regular expression to capture table names enclosed in either square brackets or single quotes.\n",
    "    table_pattern = re.compile(r\"(?:\\[(?P<name_bracket>[^\\]]+)\\]|'(?P<name_quote>[^']+)')\")\n",
    "    \n",
    "    # Regular expression to capture the measure name from the expression pattern.\n",
    "    measure_pattern = re.compile(r\"\\[([^\\]]+)\\]\\s*=\\s*\\(\\/\\* USER DAX BEGIN \\*\\/\")\n",
    "\n",
    "    # Iterate over each report measure string in the distinct_objects set.\n",
    "    for model_object in distinct_objects:\n",
    "        tbl_match = table_pattern.search(model_object)\n",
    "        measure_match = measure_pattern.search(model_object)\n",
    "        # Extract the table name from the regex groups if a match is found; otherwise, set as None.\n",
    "        if tbl_match:\n",
    "            table_name = tbl_match.group(\"name_bracket\") or tbl_match.group(\"name_quote\")\n",
    "        else:\n",
    "            table_name = None\n",
    "\n",
    "        # Append a mapping dictionary with the extracted values.\n",
    "        rows.append(\n",
    "            {\n",
    "                \"TableName\": table_name,\n",
    "                \"ObjectName\": measure_match.group(1) if measure_match else None,\n",
    "                \"ModelObject\": model_object,\n",
    "                \"ObjectType\": \"REPORT MEASURE\",\n",
    "            }\n",
    "        )\n",
    "    # If mappings were found, convert them to a DataFrame and save to the Delta table.\n",
    "    if rows:\n",
    "        df_report = pd.DataFrame(rows)\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=df_report,\n",
    "            table_name=historical_table_names[\"object_mapping\"],\n",
    "            context=context,\n",
    "        )\n",
    "        print(f\"âœ… Saved {len(rows)} REPORT MEASURE mappings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b992608d-fb74-4acd-a519-75c02f04a275",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Capture and Process Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99024f82-68b9-426d-9192-ff1c27f67a2f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "\n",
    "def find_starting_index(block_hours: float) -> int:\n",
    "    \"\"\"\n",
    "    Finds the starting index for a given block length (in hours) based on a descending range.\n",
    "\n",
    "    This function iterates over the numbers 24 down to 1 and returns the first index\n",
    "    for which the provided block_hours is greater than or equal to the hour value.\n",
    "    If no matching value is found, it returns 23 as a fallback.\n",
    "\n",
    "    Args:\n",
    "        block_hours (float): The length of the block in hours.\n",
    "\n",
    "    Returns:\n",
    "        int: The index corresponding to the block_hours.\n",
    "    \"\"\"\n",
    "    # Loop through hours from 24 to 1 (inclusive), keeping track of the index.\n",
    "    for i, hrs in enumerate(range(24, 0, -1)):\n",
    "        # If the current block_hours is at least as large as the current hour value...\n",
    "        if block_hours >= hrs:\n",
    "            # ...return this index.\n",
    "            return i\n",
    "    # If no hour in the loop was less than or equal to block_hours, return the last index as fallback.\n",
    "    return 23  \n",
    "\n",
    "\n",
    "def group_missing_hours(missing_hours: list) -> list:\n",
    "    \"\"\"\n",
    "    Groups contiguous missing hours into intervals.\n",
    "\n",
    "    This function takes a list of missing hour values and groups them into continuous intervals.\n",
    "    Each interval is returned as a tuple where the second value is one greater than the last hour\n",
    "    (to represent an exclusive end).\n",
    "\n",
    "    Args:\n",
    "        missing_hours (list): A list of integer hour values that are missing.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, each representing an interval (start_hour, end_hour_exclusive).\n",
    "    \"\"\"\n",
    "    if not missing_hours:\n",
    "        # If there are no missing hours, return an empty list.\n",
    "        return []\n",
    "    intervals = []\n",
    "    # Initialize start and end with the first missing hour.\n",
    "    start = missing_hours[0]\n",
    "    end = missing_hours[0]\n",
    "    # Loop through the remaining missing hours.\n",
    "    for h in missing_hours[1:]:\n",
    "        # If the current hour continues the sequence...\n",
    "        if h == end + 1:\n",
    "            # ...extend the current interval.\n",
    "            end = h\n",
    "        else:\n",
    "            # Otherwise, append the current interval (with end as exclusive) and start a new one.\n",
    "            intervals.append((start, end + 1))\n",
    "            start = h\n",
    "            end = h\n",
    "    # Append the final interval.\n",
    "    intervals.append((start, end + 1))\n",
    "    return intervals\n",
    "\n",
    "\n",
    "def get_missing_hours_for_day(log_table, day_date) -> list:\n",
    "    \"\"\"\n",
    "    Retrieves the missing hours for a given day from the log table.\n",
    "\n",
    "    It collects existing hour entries for the given day from the log table and returns a list\n",
    "    of hour values (0 to 23) that are missing.\n",
    "\n",
    "    Args:\n",
    "        log_table: A Spark DataFrame representing the log table.\n",
    "        day_date: The date for which missing hours should be identified.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of hour values (0-23) that are not present in the log_table.\n",
    "    \"\"\"\n",
    "    existing = set()\n",
    "    if log_table is not None:\n",
    "        try:\n",
    "            # Filter the log table for the specified day and extract distinct hours.\n",
    "            existing = {\n",
    "                row[\"AsOfHour\"] for row in log_table.filter(col(\"AsOfDate\") == day_date)\n",
    "                .select(\"AsOfHour\").distinct().collect()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Print a warning if there is an error during retrieval.\n",
    "            print(f\"âš ï¸ Error retrieving existing hours for {day_date}: {e}\")\n",
    "    # Return all hours that are not in the existing set.\n",
    "    return [hour for hour in range(24) if hour not in existing]\n",
    "\n",
    "\n",
    "def format_datetime(dt: datetime) -> str:\n",
    "    \"\"\"\n",
    "    Formats a datetime object into a specific string format.\n",
    "\n",
    "    The formatted string follows the pattern \"datetime(YYYY-MM-DDTHH:MM:SS)\".\n",
    "\n",
    "    Args:\n",
    "        dt (datetime): The datetime object to format.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted datetime string.\n",
    "    \"\"\"\n",
    "    return dt.strftime(\"datetime(%Y-%m-%dT%H:%M:%S)\")\n",
    "\n",
    "\n",
    "def build_user_groups():\n",
    "    \"\"\"\n",
    "    Constructs dynamic 'let' statements and 'case' conditions for user groups.\n",
    "\n",
    "    The function iterates over a globally defined 'user_groups' dictionary (assumed to be defined\n",
    "    elsewhere) and generates:\n",
    "      - A list of 'let' statements that create dynamic arrays for each group.\n",
    "      - A list of 'case' conditions to be used in KQL queries for mapping executing users to groups.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two strings, one with all let statements joined by newlines, and another with the case conditions joined by commas and newlines.\n",
    "    \"\"\"\n",
    "    let_statements = []\n",
    "    case_conditions = []\n",
    "    # Iterate over each user group and its associated email list.\n",
    "    for group, emails in user_groups.items():\n",
    "        # Create a variable-friendly group name (remove spaces/dashes and convert to lowercase).\n",
    "        grp_var = group.replace(\" \", \"\").replace(\"-\", \"\").lower()\n",
    "        # Join the email addresses into a single string, each email wrapped in single quotes.\n",
    "        emails_str = \", \".join(f\"'{email}'\" for email in emails)\n",
    "        # Append the let statement for the current group.\n",
    "        let_statements.append(f\"let {grp_var} = dynamic([{emails_str}]);\")\n",
    "        # Append the corresponding case condition to check if the executing user belongs to this group.\n",
    "        case_conditions.append(f'ExecutingUser in ({grp_var}), \"{group}\"')\n",
    "    # Return the complete let statements and case conditions as joined strings.\n",
    "    return \"\\n\".join(let_statements), \",\\n\".join(case_conditions)\n",
    "\n",
    "\n",
    "def get_log_table(table_key: str, filter_expr):\n",
    "    \"\"\"\n",
    "    Retrieve a Spark table using a table key and apply a filter expression.\n",
    "\n",
    "    The function attempts to load a table (using a global mapping 'historical_table_names' with table_key)\n",
    "    and apply the provided filter. If the table is not accessible, it returns None.\n",
    "\n",
    "    Args:\n",
    "        table_key (str): The key to look up the table name in historical_table_names.\n",
    "        filter_expr: A Spark SQL filter expression to apply to the table.\n",
    "\n",
    "    Returns:\n",
    "        The filtered Spark DataFrame if accessible, otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to load the Spark table and apply the filter.\n",
    "        table = spark.table(table_key).filter(filter_expr)\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        # Print a warning if the table cannot be accessed.\n",
    "        print(f\"âš ï¸ Table {table_key} not accessible; proceeding without prior data.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Base Query Handler Interface\n",
    "class BaseQueryHandler:\n",
    "    \"\"\"\n",
    "    Abstract base class for query handlers.\n",
    "\n",
    "    This class defines the interface for any query handler, requiring the implementation of\n",
    "    methods to generate test queries, main queries, and process query results.\n",
    "    \"\"\"\n",
    "    def generate_test_query(self, start_ts: datetime, end_ts: datetime) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate_main_query(self, start_ts: datetime, end_ts: datetime) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def process_result(self, main_result, start_ts: datetime) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# QueryLogCollector Class\n",
    "class QueryLogCollector:\n",
    "    \"\"\"\n",
    "    Handles the collection and processing of log queries over time intervals.\n",
    "\n",
    "    This class divides a given time range into intervals, executes queries for each interval,\n",
    "    and processes the results. It supports retries with smaller intervals in case of mismatches.\n",
    "    \"\"\"\n",
    "    ALLOWED_INTERVALS = list(range(24, 0, -1))  # Allowed query interval lengths (hours)\n",
    "\n",
    "    def __init__(self, context: dict):\n",
    "        # Store the execution context which contains configurations and connection details.\n",
    "        self.context = context\n",
    "\n",
    "    @staticmethod\n",
    "    @log_function_calls\n",
    "    @retry(exceptions=(Exception,), num_retries=2, initial_delay=30, backoff_factor=2, logger=print)\n",
    "    def execute_query(context: dict, kql_query: str):\n",
    "        \"\"\"\n",
    "        Executes a Kusto Query Language (KQL) query using the provided context.\n",
    "\n",
    "        The function retrieves an access token, builds a Spark DataFrame query using the KQL query,\n",
    "        and returns the result. It prints success or failure messages accordingly.\n",
    "\n",
    "        Args:\n",
    "            context (dict): Context containing Kusto connection details.\n",
    "            kql_query (str): The KQL query string to execute.\n",
    "\n",
    "        Returns:\n",
    "            The Spark DataFrame containing the query result.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Retrieve the access token for authentication.\n",
    "            access_token = mssparkutils.credentials.getToken(context[\"log_analytics_kusto_uri\"])\n",
    "            # Build the Spark DataFrame reader for the Kusto data source with required options.\n",
    "            result = (spark.read.format(\"com.microsoft.kusto.spark.synapse.datasource\")\n",
    "                      .option(\"accessToken\", access_token)\n",
    "                      .option(\"kustoCluster\", context[\"log_analytics_kusto_uri\"])\n",
    "                      .option(\"kustoDatabase\", context[\"log_analytics_kusto_database\"])\n",
    "                      .option(\"kustoQuery\", kql_query)\n",
    "                      .load())\n",
    "            print(\"âœ… KQL query executed successfully.\")\n",
    "            return result\n",
    "        except Exception:\n",
    "            print(\"âŒ Failed to execute KQL query.\")\n",
    "            raise\n",
    "\n",
    "    def process_sub_intervals(self, start_ts: datetime, end_ts: datetime, new_idx: int,\n",
    "                              generate_test_query, generate_main_query, process_result) -> bool:\n",
    "        \"\"\"\n",
    "        Processes sub-intervals within a given time range by dividing it into smaller chunks.\n",
    "\n",
    "        It iterates over the time interval using the granularity defined by ALLOWED_INTERVALS\n",
    "        at the index new_idx, executes queries for each sub-interval, and processes the results.\n",
    "        If any sub-interval fails, it returns False.\n",
    "\n",
    "        Args:\n",
    "            start_ts (datetime): Start timestamp of the interval.\n",
    "            end_ts (datetime): End timestamp of the interval.\n",
    "            new_idx (int): Current index into the ALLOWED_INTERVALS list representing granularity.\n",
    "            generate_test_query: Function to generate a test query.\n",
    "            generate_main_query: Function to generate the main query.\n",
    "            process_result: Function to process the query result.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if all sub-interval queries succeed, False otherwise.\n",
    "        \"\"\"\n",
    "        # Get the new granularity (hours) from the allowed intervals list.\n",
    "        new_granularity = QueryLogCollector.ALLOWED_INTERVALS[new_idx]\n",
    "        success = True\n",
    "        current = start_ts\n",
    "        # Loop over the overall interval in steps of the granularity.\n",
    "        while current < end_ts:\n",
    "            # Calculate the end of the current sub-interval.\n",
    "            sub_end = current + pd.Timedelta(hours=new_granularity)\n",
    "            # Ensure the sub-interval does not exceed the overall end time.\n",
    "            if sub_end > end_ts:\n",
    "                sub_end = end_ts\n",
    "            # Calculate the length of the current sub-interval in hours.\n",
    "            sub_length = (sub_end - current).total_seconds() / 3600.0\n",
    "            # Determine the starting index based on the sub-interval length.\n",
    "            sub_idx = find_starting_index(sub_length)\n",
    "            print(f\"ðŸ” Querying sub-interval {current} to {sub_end}...\")\n",
    "            # Attempt to execute the query for the sub-interval.\n",
    "            if not self.attempt_interval_query(current, sub_end, sub_idx,\n",
    "                                               generate_test_query, generate_main_query, process_result):\n",
    "                success = False\n",
    "            # Move to the next sub-interval.\n",
    "            current = sub_end\n",
    "        return success\n",
    "\n",
    "    def attempt_interval_query(self, start_ts: datetime, end_ts: datetime, current_idx: int,\n",
    "                               generate_test_query, generate_main_query, process_result) -> bool:\n",
    "        \"\"\"\n",
    "        Attempts to execute queries for a given time interval and verify that the results match.\n",
    "\n",
    "        It first runs a test query to check if any rows are returned. If rows exist, it then runs\n",
    "        the main query. If the row count between test and main queries does not match, it retries\n",
    "        with a smaller interval (if possible). If the minimum granularity is reached and the query still fails,\n",
    "        it skips the interval.\n",
    "\n",
    "        Args:\n",
    "            start_ts (datetime): Start timestamp of the interval.\n",
    "            end_ts (datetime): End timestamp of the interval.\n",
    "            current_idx (int): Current index in the allowed intervals indicating query granularity.\n",
    "            generate_test_query: Function to generate a test query.\n",
    "            generate_main_query: Function to generate the main query.\n",
    "            process_result: Function to process the query result.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the query for the interval succeeds, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"â„¹ï¸ Sending test query for interval {start_ts} to {end_ts}...\")\n",
    "            # Generate and execute the test query.\n",
    "            test_query = generate_test_query(start_ts, end_ts)\n",
    "            test_result = self.execute_query(self.context, test_query)\n",
    "            # Retrieve the first row of the test result to check the total count.\n",
    "            test_row = test_result.first()\n",
    "            test_count = 0 if test_row is None else test_row[\"totalCount\"]\n",
    "\n",
    "            # If the test query returns zero rows, skip further processing for this interval.\n",
    "            if test_count == 0:\n",
    "                print(f\"âš ï¸ Test query returned 0 rows for interval {start_ts} to {end_ts}. Skipping...\")\n",
    "                return True\n",
    "\n",
    "            print(f\"â„¹ï¸ Sending main query for interval {start_ts} to {end_ts}...\")\n",
    "            # Generate and execute the main query.\n",
    "            main_query = generate_main_query(start_ts, end_ts)\n",
    "            main_result = self.execute_query(self.context, main_query)\n",
    "            main_count = main_result.count()\n",
    "\n",
    "            # Ensure that the number of rows from the main query matches the test query.\n",
    "            if main_count != test_count:\n",
    "                raise Exception(f\"Query result truncated: main_count ({main_count}) != test_count ({test_count})\")\n",
    "            else:\n",
    "                print(f\"âœ… Query results match: {main_count} rows.\")\n",
    "            # Process the successful query result.\n",
    "            process_result(main_result, start_ts)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            # Log the error with details about the interval and granularity.\n",
    "            print(f\"âŒ Interval {start_ts} to {end_ts} at granularity {QueryLogCollector.ALLOWED_INTERVALS[current_idx]}h failed. Error: {e}\")\n",
    "            # If we are at the minimum granularity, skip this interval.\n",
    "            if current_idx == len(QueryLogCollector.ALLOWED_INTERVALS) - 1:\n",
    "                print(f\"âŒ Minimum granularity reached for {start_ts} to {end_ts}; skipping interval.\")\n",
    "                return False\n",
    "            else:\n",
    "                # Otherwise, try with a smaller interval (next index in ALLOWED_INTERVALS).\n",
    "                new_idx = current_idx + 1\n",
    "                print(f\"âš ï¸ Retrying with smaller interval ({QueryLogCollector.ALLOWED_INTERVALS[new_idx]}h) for {start_ts} to {end_ts}...\")\n",
    "                return self.process_sub_intervals(start_ts, end_ts, new_idx,\n",
    "                                                  generate_test_query, generate_main_query, process_result)\n",
    "\n",
    "    def process_query_intervals(self, log_table, days_range, query_handler: BaseQueryHandler, label: str, apply_cutoff=True):\n",
    "        \"\"\"\n",
    "        Processes multiple query intervals across a range of days.\n",
    "\n",
    "        For each day in the days_range, this method determines the missing hours based on the log_table,\n",
    "        groups contiguous missing hours into intervals, and attempts to collect query logs using the provided\n",
    "        query_handler.\n",
    "\n",
    "        Args:\n",
    "            log_table: A Spark DataFrame representing the log table.\n",
    "            days_range: An iterable of integers representing days ago to process.\n",
    "            query_handler (BaseQueryHandler): An instance of a query handler to generate and process queries.\n",
    "            label (str): A label for logging purposes (e.g., \"detailed logs\").\n",
    "            apply_cutoff (bool): Whether to apply a cutoff for the most recent day.\n",
    "        \"\"\"\n",
    "        # Process each day in the provided range.\n",
    "        for days_ago in days_range:\n",
    "            # Calculate the start of the day (midnight) adjusted for days ago.\n",
    "            day_start = self.context[\"as_of_datetime\"].replace(hour=0, minute=0, second=0, microsecond=0) - relativedelta(days=days_ago)\n",
    "            day_date = day_start.date()\n",
    "            # Determine missing hours for the day.\n",
    "            missing_hours = get_missing_hours_for_day(log_table, day_date) if log_table is not None else list(range(24))\n",
    "            # If applying cutoff for the most recent day, adjust missing hours accordingly.\n",
    "            if apply_cutoff and days_ago == 1:\n",
    "                cutoff_hour = int(((self.context[\"as_of_datetime\"] - timedelta(hours=min_hours_before_current) - day_start).total_seconds()) // 3600)\n",
    "                missing_hours = [h for h in missing_hours if h < cutoff_hour]\n",
    "                if not missing_hours:\n",
    "                    print(f\"âœ… All eligible {label} collected for {day_date} after cutoff.\")\n",
    "                    continue\n",
    "            # If there are no missing hours, move to the next day.\n",
    "            if not missing_hours:\n",
    "                print(f\"âœ… All {label} collected for {day_date}.\")\n",
    "                continue\n",
    "            # Group missing hours into continuous intervals.\n",
    "            intervals = group_missing_hours(missing_hours)\n",
    "            # Process each missing interval.\n",
    "            for start_hr, end_hr in intervals:\n",
    "                # Calculate the start and end datetime for the current interval.\n",
    "                interval_start = day_start + pd.Timedelta(hours=start_hr)\n",
    "                interval_end = day_start + pd.Timedelta(hours=end_hr)\n",
    "                # Compute the block length in hours.\n",
    "                block_length = (interval_end - interval_start).total_seconds() / 3600.0\n",
    "                # Determine the appropriate index based on the block length.\n",
    "                idx = find_starting_index(block_length)\n",
    "                print(f\"ðŸ” Collecting {label} for {day_date} from hour {start_hr} to {end_hr} (block length: {block_length}h)...\")\n",
    "                # Attempt to query the current interval.\n",
    "                self.attempt_interval_query(interval_start, interval_end, idx,\n",
    "                                            query_handler.generate_test_query, query_handler.generate_main_query, query_handler.process_result)\n",
    "\n",
    "\n",
    "# Query Handler Implementations\n",
    "\n",
    "class ObjectCountQueryHandler(BaseQueryHandler):\n",
    "    \"\"\"\n",
    "    Query handler for collecting query counts for model objects (columns, measures, or REPORT MEASUREs).\n",
    "\n",
    "    This handler builds a KQL query tailored to the object type, executes the query, saves the results,\n",
    "    and collects any distinct REPORT MEASURE strings.\n",
    "    \"\"\"\n",
    "    def __init__(self, context: dict, object_type: str, model_objects_df):\n",
    "        self.context = context\n",
    "        self.object_type = object_type\n",
    "        self.model_objects_df = model_objects_df\n",
    "        # Set to keep track of distinct REPORT MEASURE values encountered.\n",
    "        self.distinct_report_measures = set()\n",
    "\n",
    "    def _build_query(self, start_ts: datetime, end_ts: datetime) -> str:\n",
    "        \"\"\"\n",
    "        Builds the KQL query string for collecting query counts.\n",
    "\n",
    "        Depending on the object type, the query will include different clauses to extract or expand model objects.\n",
    "        It also integrates user group information into the query.\n",
    "\n",
    "        Args:\n",
    "            start_ts (datetime): Start timestamp for the query.\n",
    "            end_ts (datetime): End timestamp for the query.\n",
    "\n",
    "        Returns:\n",
    "            str: The constructed KQL query.\n",
    "        \"\"\"\n",
    "        # Build user groups let statements and case conditions.\n",
    "        let_statements, case_conditions = build_user_groups()\n",
    "        if self.object_type == \"REPORT MEASURE\":\n",
    "            # For REPORT MEASUREs, extend the query to extract the measure details.\n",
    "            model_extend = \"\"\"\n",
    "            | extend ModelObject = extract_all(@\"MEASURE (.*?\\\\/\\\\* USER DAX END \\\\*\\\\/\\\\))\", EventText)\n",
    "            | mv-expand ModelObject\n",
    "            | where isnotempty(ModelObject)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # For other object types, if model objects data is provided, add them as a dynamic array.\n",
    "            if self.model_objects_df is not None:\n",
    "                objs_list = \", \".join(f'\"{obj}\"' for obj in self.model_objects_df[\"ModelObject\"])\n",
    "                let_statements += f\"\\nlet modelObjects = dynamic([{objs_list}]);\"\n",
    "            # Use mv-apply to match EventText against each model object.\n",
    "            model_extend = \"\"\"\n",
    "            | mv-apply ModelObject = modelObjects to typeof(string) on (\n",
    "                extend Matched = iff(EventText contains_cs ModelObject, true, false)\n",
    "                | where Matched\n",
    "                | project-away Matched\n",
    "            )\n",
    "            \"\"\"\n",
    "        # Format the start and end timestamps for the query.\n",
    "        start_str = format_datetime(start_ts)\n",
    "        end_str = format_datetime(end_ts)\n",
    "        # Build the complete query string.\n",
    "        query = f'''\n",
    "        let model_uuid = \"{self.context[\"source_model_uuid\"]}\";\n",
    "        {let_statements}\n",
    "        SemanticModelLogs\n",
    "            | where Timestamp between ({start_str} .. {end_str})\n",
    "            | where ItemId == model_uuid\n",
    "            | where OperationName == \"QueryBegin\"\n",
    "            | extend ReportId = extract_json(\"$.Sources[0].ReportId\", tostring(parse_xml(XmlaProperties)[\"PropertyList\"][\"ApplicationContext\"]), typeof(string))\n",
    "            | extend AsOfHour = datetime_part(\"hour\", Timestamp)\n",
    "            | project AsOfHour, ExecutingUser, EventText, ReportId\n",
    "            {model_extend}\n",
    "            | extend ExecutingUserGroup =\n",
    "                case(\n",
    "                    {case_conditions},\n",
    "                    \"{default_user_group}\"\n",
    "                )\n",
    "            | summarize QueryCount = count() by tostring(ModelObject), AsOfHour, ExecutingUserGroup, ReportId\n",
    "        '''.strip().replace(\"\\n\", \" \")\n",
    "        return query\n",
    "\n",
    "    def generate_test_query(self, start_ts: datetime, end_ts: datetime) -> str:\n",
    "        \"\"\"\n",
    "        Generates a test query that summarizes the total count.\n",
    "\n",
    "        Args:\n",
    "            start_ts (datetime): Start timestamp for the test query.\n",
    "            end_ts (datetime): End timestamp for the test query.\n",
    "\n",
    "        Returns:\n",
    "            str: The KQL test query string.\n",
    "        \"\"\"\n",
    "        # Append a summarize clause to count total rows.\n",
    "        return self._build_query(start_ts, end_ts) + \" | summarize totalCount = count()\"\n",
    "\n",
    "    def generate_main_query(self, start_ts: datetime, end_ts: datetime) -> str:\n",
    "        \"\"\"\n",
    "        Generates the main query used to fetch detailed query counts.\n",
    "\n",
    "        Args:\n",
    "            start_ts (datetime): Start timestamp for the main query.\n",
    "            end_ts (datetime): End timestamp for the main query.\n",
    "\n",
    "        Returns:\n",
    "            str: The KQL main query string.\n",
    "        \"\"\"\n",
    "        return self._build_query(start_ts, end_ts)\n",
    "\n",
    "    def process_result(self, main_result, start_ts: datetime) -> None:\n",
    "        \"\"\"\n",
    "        Processes the main query result.\n",
    "\n",
    "        It saves the result to a Delta table and updates the distinct_report_measures set for REPORT MEASUREs.\n",
    "\n",
    "        Args:\n",
    "            main_result: The Spark DataFrame returned from executing the main query.\n",
    "            start_ts (datetime): The starting timestamp of the query interval.\n",
    "        \"\"\"\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=main_result,\n",
    "            table_name=historical_table_names[\"object_query_count\"],\n",
    "            context=self.context,\n",
    "            AsOfDate=start_ts.date(),\n",
    "            AsOfDateTime=start_ts,\n",
    "            ObjectType=self.object_type,\n",
    "        )\n",
    "        # If processing REPORT MEASUREs, collect distinct model objects.\n",
    "        if self.object_type == \"REPORT MEASURE\":\n",
    "            for row in main_result.select(\"ModelObject\").distinct().collect():\n",
    "                if row[\"ModelObject\"]:\n",
    "                    self.distinct_report_measures.add(row[\"ModelObject\"])\n",
    "\n",
    "\n",
    "class DetailedLogsQueryHandler(BaseQueryHandler):\n",
    "    \"\"\"\n",
    "    Query handler for capturing detailed logs from semantic model operations.\n",
    "\n",
    "    This handler builds queries to fetch both query begin and query end logs,\n",
    "    joins them together, processes the results (including user name masking), and collects ReportIds.\n",
    "    \"\"\"\n",
    "    def __init__(self, context: dict):\n",
    "        self.context = context\n",
    "        # Set to collect unique ReportIds.\n",
    "        self.report_ids = set()\n",
    "        # Dictionary to store historical mapping for principal names.\n",
    "        self.historical_mapping = {}\n",
    "        # Lock to prevent concurrent modifications of the historical mapping.\n",
    "        self.mapping_lock = threading.Lock()\n",
    "\n",
    "    def _build_query(self, start_ts: datetime, end_ts: datetime) -> str:\n",
    "        \"\"\"\n",
    "        Builds the KQL query string for detailed logs.\n",
    "\n",
    "        The query fetches logs for both \"QueryEnd\" (and \"Error\") and \"QueryBegin\" events,\n",
    "        then joins them together and applies user group mapping.\n",
    "\n",
    "        Args:\n",
    "            start_ts (datetime): Start timestamp for the query.\n",
    "            end_ts (datetime): End timestamp for the query.\n",
    "\n",
    "        Returns:\n",
    "            str: The constructed KQL query string.\n",
    "        \"\"\"\n",
    "        let_statements, case_conditions = build_user_groups()\n",
    "        # Convert timestamps to ISO format strings.\n",
    "        start_iso = start_ts.isoformat(timespec=\"seconds\")\n",
    "        end_iso = end_ts.isoformat(timespec=\"seconds\")\n",
    "        # For QueryBegin, extend the window to one day before start_ts.\n",
    "        query_begin_start_iso = (start_ts - timedelta(days=1)).isoformat(timespec=\"seconds\")\n",
    "        query = f'''\n",
    "        let model_uuid = \"{self.context[\"source_model_uuid\"]}\";\n",
    "        {let_statements}\n",
    "        let base_data =\n",
    "            SemanticModelLogs\n",
    "            | where ItemId == model_uuid;\n",
    "        let query_end = base_data\n",
    "            | where Timestamp between (datetime({start_iso}) .. datetime({end_iso}))\n",
    "            | where OperationName in (\"Error\", \"QueryEnd\")\n",
    "            | project Timestamp, OperationName, OperationDetailName, OperationId, XmlaSessionId, ExecutingUser, DurationMs, CpuTimeMs, EventText, Status, StatusCode;\n",
    "        let query_begin = base_data\n",
    "            | where Timestamp between (datetime({query_begin_start_iso}) .. datetime({end_iso}))\n",
    "            | where OperationName == \"QueryBegin\"\n",
    "            | extend ActivityId = tostring(parse_xml(XmlaProperties)[\"PropertyList\"][\"DbpropMsmdActivityID\"])\n",
    "            | extend RequestId = tostring(parse_xml(XmlaProperties)[\"PropertyList\"][\"DbpropMsmdRequestID\"])\n",
    "            | extend CurrentActivityId = tostring(parse_xml(XmlaProperties)[\"PropertyList\"][\"DbpropMsmdCurrentActivityID\"])\n",
    "            | extend ReportId = extract_json(\"$.Sources[0].ReportId\", tostring(parse_xml(XmlaProperties)[\"PropertyList\"][\"ApplicationContext\"]), typeof(string))\n",
    "            | distinct ActivityId, RequestId, CurrentActivityId, ReportId, OperationId, XmlaSessionId;\n",
    "        query_end\n",
    "        | join kind=leftouter (query_begin) on OperationId, XmlaSessionId\n",
    "        | extend ExecutingUserGroup =\n",
    "            case(\n",
    "                    {case_conditions},\n",
    "                    \"{default_user_group}\"\n",
    "                )\n",
    "        | extend AsOfHour = datetime_part(\"hour\", Timestamp)\n",
    "        | project Timestamp, AsOfHour, OperationName, OperationDetailName, ReportId, ExecutingUser, ExecutingUserGroup, DurationMs, CpuTimeMs, EventText, OperationId, XmlaSessionId, ActivityId, RequestId, CurrentActivityId, Status, StatusCode\n",
    "        '''.strip().replace(\"\\n\", \"\")\n",
    "        return query\n",
    "\n",
    "    def generate_test_query(self, start_ts: datetime, end_ts: datetime) -> str:\n",
    "        \"\"\"\n",
    "        Generates a test query for detailed logs that summarizes the total count.\n",
    "\n",
    "        Args:\n",
    "            start_ts (datetime): Start timestamp for the test query.\n",
    "            end_ts (datetime): End timestamp for the test query.\n",
    "\n",
    "        Returns:\n",
    "            str: The KQL test query string.\n",
    "        \"\"\"\n",
    "        return self._build_query(start_ts, end_ts) + \" | summarize totalCount = count()\"\n",
    "\n",
    "    def generate_main_query(self, start_ts: datetime, end_ts: datetime) -> str:\n",
    "        \"\"\"\n",
    "        Generates the main query for capturing detailed logs.\n",
    "\n",
    "        Args:\n",
    "            start_ts (datetime): Start timestamp for the main query.\n",
    "            end_ts (datetime): End timestamp for the main query.\n",
    "\n",
    "        Returns:\n",
    "            str: The KQL main query string.\n",
    "        \"\"\"\n",
    "        return self._build_query(start_ts, end_ts)\n",
    "\n",
    "    def process_result(self, main_result, start_ts: datetime) -> None:\n",
    "        \"\"\"\n",
    "        Processes the detailed logs query result.\n",
    "\n",
    "        Depending on configuration, it may mask user names, save the results to a Delta table,\n",
    "        and extract distinct ReportIds from the result.\n",
    "\n",
    "        Args:\n",
    "            main_result: The Spark DataFrame containing the query results.\n",
    "            start_ts (datetime): The starting timestamp of the query interval.\n",
    "        \"\"\"\n",
    "        # Optionally mask user names based on the configuration setting.\n",
    "        if collect_principal_names == 2:\n",
    "            # Completely mask executing user names.\n",
    "            main_result = main_result.withColumn(\"ExecutingUser\", lit(\"Masked\"))\n",
    "        elif collect_principal_names == 1:\n",
    "            try:\n",
    "                # Retrieve a list of distinct executing users.\n",
    "                new_users = [row[0].strip().lower() for row in main_result.select(\"ExecutingUser\").distinct().collect() if row[0]]\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error retrieving distinct users: {e}\")\n",
    "                new_users = []\n",
    "            with self.mapping_lock:\n",
    "                # Update the historical mapping for any new users.\n",
    "                for user in new_users:\n",
    "                    if user and user not in self.historical_mapping:\n",
    "                        self.historical_mapping[user] = str(uuid4())\n",
    "            # Broadcast the mapping to all Spark workers.\n",
    "            broadcast_map = spark.sparkContext.broadcast(self.historical_mapping)\n",
    "            # Define a UDF to mask user names using the broadcasted mapping.\n",
    "            def mask_user(actual):\n",
    "                return broadcast_map.value.get(actual.strip().lower() if actual else None, actual)\n",
    "            mask_udf = udf(mask_user, StringType())\n",
    "            # Apply the UDF to mask the ExecutingUser column.\n",
    "            main_result = main_result.withColumn(\"ExecutingUser\", mask_udf(col(\"ExecutingUser\")))\n",
    "        # Save the processed detailed logs to the Delta table.\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=main_result,\n",
    "            table_name=historical_table_names[\"detailed_logs\"],\n",
    "            context=self.context,\n",
    "            AsOfDate=start_ts.date(),\n",
    "            AsOfDateTime=start_ts,\n",
    "        )\n",
    "        try:\n",
    "            # Extract distinct ReportIds from the result.\n",
    "            distinct_ids = set(main_result.select(\"ReportId\").rdd.flatMap(lambda x: x).collect())\n",
    "            self.report_ids.update(distinct_ids)\n",
    "            print(f\"âœ… Collected {len(distinct_ids)} ReportIds for interval starting at {start_ts}.\")\n",
    "        except Exception:\n",
    "            print(f\"âŒ Failed to extract ReportIds for interval starting at {start_ts}.\")\n",
    "\n",
    "\n",
    "# Generic Function for Processing Query Collections\n",
    "def process_query_collection(context: dict, table_key: str, query_handler: BaseQueryHandler, label: str, apply_cutoff=True, filter_expr=None, override_context: dict = None):\n",
    "    \"\"\"\n",
    "    Retrieves the appropriate log table, instantiates a QueryLogCollector,\n",
    "    and processes query intervals using the specified query handler.\n",
    "\n",
    "    Args:\n",
    "        context (dict): The context containing configuration and connection details.\n",
    "        table_key (str): Key to identify the table in historical_table_names.\n",
    "        query_handler (BaseQueryHandler): An instance of a query handler for building and processing queries.\n",
    "        label (str): A label used for logging and messaging.\n",
    "        apply_cutoff (bool): Flag to indicate if cutoff logic should be applied.\n",
    "        filter_expr: A Spark SQL expression to filter the log table.\n",
    "        override_context (dict): Optional dictionary to override parts of the context.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create a copy of the context to avoid modifying the original.\n",
    "    ctx = context.copy()\n",
    "    if override_context:\n",
    "        # Update the context with any override values.\n",
    "        ctx.update(override_context)\n",
    "    if filter_expr is None:\n",
    "        # Use a default filter expression based on the source model UUID.\n",
    "        filter_expr = col(\"ModelUuid\") == ctx[\"source_model_uuid\"]\n",
    "    # Retrieve the log table using the provided key and filter.\n",
    "    log_table = get_log_table(historical_table_names[table_key], filter_expr)\n",
    "    # Instantiate the query log collector.\n",
    "    qcollector = QueryLogCollector(ctx)\n",
    "    # Process query intervals over a range of days.\n",
    "    qcollector.process_query_intervals(log_table, range(1, max_days_ago_to_collect + 1), query_handler, label, apply_cutoff)\n",
    "\n",
    "\n",
    "# Capture Functions Using the Refactored Logic\n",
    "\n",
    "@log_function_calls\n",
    "def capture_query_counts_by_object(context: dict, object_type: str, model_objects_df) -> None:\n",
    "    \"\"\"\n",
    "    Captures query counts for a specific object type by processing the query collection.\n",
    "\n",
    "    It sets up a filter expression for the model UUID and object type,\n",
    "    instantiates the ObjectCountQueryHandler, and processes the queries.\n",
    "    For REPORT MEASURE objects, it also saves any new mappings discovered.\n",
    "\n",
    "    Args:\n",
    "        context (dict): The execution context.\n",
    "        object_type (str): The type of object (\"COLUMN\", \"MEASURE\", \"REPORT MEASURE\").\n",
    "        model_objects_df: DataFrame containing model objects; can be None for REPORT MEASURE.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    filter_expr = (col(\"ModelUuid\") == context[\"source_model_uuid\"]) & (col(\"ObjectType\") == object_type)\n",
    "    query_handler = ObjectCountQueryHandler(context, object_type, model_objects_df)\n",
    "    process_query_collection(context, \"object_query_count\", query_handler, f\"{object_type} query counts\", True, filter_expr)\n",
    "    # For REPORT MEASURE, save new mappings if any were discovered.\n",
    "    if object_type == \"REPORT MEASURE\":\n",
    "        if query_handler.distinct_report_measures:\n",
    "            print(f\"ðŸ“ Saving {len(query_handler.distinct_report_measures)} new REPORT MEASURE mappings...\")\n",
    "            try:\n",
    "                save_report_measure_mappings(query_handler.distinct_report_measures, context)\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to save REPORT MEASURE mappings: {e}\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No new REPORT MEASUREs discovered.\")\n",
    "\n",
    "\n",
    "@log_function_calls\n",
    "def capture_detailed_logs(context: dict) -> set:\n",
    "    \"\"\"\n",
    "    Captures detailed logs from the semantic model and returns a set of ReportIds.\n",
    "\n",
    "    This function optionally builds a historical mapping for principal names,\n",
    "    processes detailed logs, and applies masking to older records if configured.\n",
    "\n",
    "    Args:\n",
    "        context (dict): The execution context.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of ReportIds captured from the detailed logs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If configuration indicates that principal names should be collected,\n",
    "        # build a historical mapping from logs over the past 30 days.\n",
    "        if collect_principal_names == 1:\n",
    "            start_hist = (context[\"as_of_datetime\"] - relativedelta(days=30)).replace(minute=0, second=0, microsecond=0)\n",
    "            end_hist = (context[\"as_of_datetime\"] - relativedelta(days=1)).replace(minute=0, second=0, microsecond=0)\n",
    "            def gen_hist_main(s, e):\n",
    "                return f\"\"\"\n",
    "                    let startTime = {s.strftime(\"datetime(%Y-%m-%dT%H:%M:%S)\")};\n",
    "                    let endTime = {e.strftime(\"datetime(%Y-%m-%dT%H:%M:%S)\")};\n",
    "                    SemanticModelLogs\n",
    "                    | where Timestamp between (startTime .. endTime)\n",
    "                    | where OperationName in (\"Error\", \"QueryEnd\")\n",
    "                    | distinct XmlaSessionId, ExecutingUser\n",
    "                \"\"\"\n",
    "            def make_test_query(query_func):\n",
    "                return lambda s, e: query_func(s, e) + \" | summarize totalCount = count()\"\n",
    "            hist_results = []\n",
    "            def process_hist(main_result, start_ts):\n",
    "                hist_results.append(main_result.toPandas())\n",
    "            total_hours = (end_hist - start_hist).total_seconds() / 3600.0\n",
    "            start_idx = find_starting_index(total_hours)\n",
    "            qcollector = QueryLogCollector(context)\n",
    "            qcollector.attempt_interval_query(start_hist, end_hist, start_idx, make_test_query(gen_hist_main), gen_hist_main, process_hist)\n",
    "            hist_kql_pd = pd.concat(hist_results, ignore_index=True) if hist_results else pd.DataFrame(columns=[\"XmlaSessionId\", \"ExecutingUser\"])\n",
    "            try:\n",
    "                hist_logs_df = spark.table(historical_table_names[\"detailed_logs\"]).select(\"XmlaSessionId\", \"ExecutingUser\").distinct()\n",
    "                hist_logs_pd = hist_logs_df.toPandas()\n",
    "            except Exception:\n",
    "                print(\"âš ï¸ detailed_logs table missing; using empty historical data.\")\n",
    "                hist_logs_pd = pd.DataFrame(columns=[\"XmlaSessionId\", \"ExecutingUser\"])\n",
    "            if not hist_kql_pd.empty and not hist_logs_pd.empty:\n",
    "                hist_kql = hist_kql_pd.rename(columns={\"ExecutingUser\": \"ActualUser\"})\n",
    "                hist_kql[\"ActualUser\"] = hist_kql[\"ActualUser\"].apply(lambda x: x.strip().lower() if x else None)\n",
    "                hist_logs = hist_logs_pd.rename(columns={\"ExecutingUser\": \"MaskedUser\"})\n",
    "                merged = pd.merge(hist_kql, hist_logs, on=\"XmlaSessionId\", how=\"inner\")\n",
    "                merged[\"NormalizedActualUser\"] = merged[\"ActualUser\"].apply(lambda x: x.strip().lower() if x else None)\n",
    "                merged = merged.drop_duplicates(subset=[\"NormalizedActualUser\"])\n",
    "                historical_mapping = { row[\"NormalizedActualUser\"]: row[\"MaskedUser\"] for _, row in merged.iterrows() if pd.notnull(row[\"NormalizedActualUser\"]) }\n",
    "            else:\n",
    "                historical_mapping = {}\n",
    "        else:\n",
    "            historical_mapping = {}\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Historical mapping build failed: {e}\")\n",
    "        historical_mapping = {}\n",
    "\n",
    "    query_handler = DetailedLogsQueryHandler(context)\n",
    "    query_handler.historical_mapping = historical_mapping\n",
    "    process_query_collection(context, \"detailed_logs\", query_handler, \"detailed logs\")\n",
    "    # After collecting detailed logs, optionally mask principal names for records older than a configured number of days.\n",
    "    if mask_principal_names_after_days > 0:\n",
    "        cutoff_date = context[\"as_of_date\"] - timedelta(days=mask_principal_names_after_days)\n",
    "        update_query = f\"\"\"\n",
    "            UPDATE {historical_table_names[\"detailed_logs\"]}\n",
    "            SET ExecutingUser = 'Masked'\n",
    "            WHERE AsOfDate < '{cutoff_date}'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            spark.sql(update_query)\n",
    "            print(f\"âœ… Masked user names for records older than {mask_principal_names_after_days} days (before {cutoff_date}).\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to mask user names for historical detailed logs: {e}\")\n",
    "    print(f\"âœ… capture_detailed_logs complete. Total ReportIds: {len(query_handler.report_ids)}\")\n",
    "    return query_handler.report_ids\n",
    "\n",
    "\n",
    "@log_function_calls\n",
    "def capture_logs_and_mappings(context: dict, model_columns: pd.DataFrame, model_measures: pd.DataFrame) -> set:\n",
    "    \"\"\"\n",
    "    Orchestrates the capture of various logs and object mappings.\n",
    "\n",
    "    It processes model columns, measures, REPORT MEASUREs, detailed logs.\n",
    "    If required context keys are missing, it raises an error.\n",
    "    Returns a consolidated set of ReportIds collected from the detailed logs.\n",
    "\n",
    "    Args:\n",
    "        context (dict): The execution context containing necessary configurations.\n",
    "        model_columns (pd.DataFrame): DataFrame containing model column definitions.\n",
    "        model_measures (pd.DataFrame): DataFrame containing model measure definitions.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of ReportIds collected from the logs.\n",
    "    \"\"\"\n",
    "    required = {\"source_model_uuid\", \"source_model_workspace_uuid\", \"source_model_name\", \"log_analytics_kusto_uri\", \"log_analytics_kusto_database\"}\n",
    "    missing = required - context.keys()\n",
    "    if missing:\n",
    "        raise KeyError(f\"âŒ Missing context keys: {', '.join(missing)}\")\n",
    "    \n",
    "    report_ids = set()\n",
    "    try:\n",
    "        print(\"ðŸ“Š Processing columns...\")\n",
    "        processed_cols = process_semantic_model_objects(model_columns, \"COLUMN\")\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=processed_cols,\n",
    "            table_name=historical_table_names[\"object_mapping\"],\n",
    "            context=context,\n",
    "        )\n",
    "        capture_query_counts_by_object(context, \"COLUMN\", processed_cols)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Columns processing failed. Error: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"ðŸ“Š Processing measures...\")\n",
    "        processed_measures = process_semantic_model_objects(model_measures, \"MEASURE\")\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=processed_measures,\n",
    "            table_name=historical_table_names[\"object_mapping\"],\n",
    "            context=context,\n",
    "        )\n",
    "        capture_query_counts_by_object(context, \"MEASURE\", processed_measures)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Measures processing failed. Error: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"ðŸ“Š Processing REPORT MEASUREs...\")\n",
    "        capture_query_counts_by_object(context, \"REPORT MEASURE\", None)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ REPORT MEASURE processing failed. Error: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"ðŸ“Š Processing detailed logs...\")\n",
    "        detailed_ids = capture_detailed_logs(context)\n",
    "        if detailed_ids:\n",
    "            report_ids.update(detailed_ids)\n",
    "            print(f\"âœ… Updated ReportIds with {len(detailed_ids)} detailed ReportIds.\")\n",
    "        else:\n",
    "            print(\"âš ï¸ No detailed ReportIds captured.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Detailed log capture failed. Error: {e}\")\n",
    "\n",
    "    return report_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc01c2c-e912-4fe9-890d-bc9298d599d7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Capturing Unused Delta Table Columns & Source Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f924f87-9550-47b5-8168-ce3308f531f1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def capture_unused_delta_columns(context: dict) -> None:\n",
    "    \"\"\"\n",
    "    Captures unused columns from the Delta tables by comparing datastore metadata with model usage.\n",
    "\n",
    "    If datastore information is missing (i.e., no datastore name provided), the function inserts placeholder rows.\n",
    "    Otherwise, it connects to the datastore to retrieve column metadata from Delta tables, compares it to the\n",
    "    columns actually used in the model (obtained via the Table Object Model (TOM)), and saves the differences\n",
    "    (unused columns) to designated Delta tables.\n",
    "\n",
    "    If no unused columns are found, a placeholder N/A record is written to the unused columns table.\n",
    "    \"\"\"\n",
    "    # Define the required context keys for retrieving datastore and model metadata.\n",
    "    required_keys = [\n",
    "        \"source_datastore_name\",\n",
    "        \"source_datastore_workspace_uuid\",\n",
    "        \"source_datastore_uuid\",\n",
    "        \"source_model_uuid\",\n",
    "        \"source_model_workspace_uuid\",\n",
    "    ]\n",
    "    # Ensure all required keys are present; if not, raise a KeyError.\n",
    "    for key in required_keys:\n",
    "        if key not in context:\n",
    "            raise KeyError(f\"âŒ Missing required context key: '{key}'\")\n",
    "            \n",
    "    # If datastore details are missing, insert placeholder rows into the target Delta tables and exit.\n",
    "    if not context[\"source_datastore_name\"]:\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=spark.createDataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"TableName\": \"N/A\",\n",
    "                        \"SourceTableName\": \"N/A\",\n",
    "                        \"SourceColumnName\": \"N/A\",\n",
    "                    }\n",
    "                ]\n",
    "            ),\n",
    "            table_name=historical_table_names[\"unused_columns\"],\n",
    "            context=context,\n",
    "        )\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=spark.createDataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"TableName\": \"N/A\",\n",
    "                        \"ColumnName\": \"N/A\",\n",
    "                        \"SourceTableName\": \"N/A\",\n",
    "                        \"SourceColumnName\": \"N/A\",\n",
    "                    }\n",
    "                ]\n",
    "            ),\n",
    "            table_name=historical_table_names[\"source_mapping\"],\n",
    "            context=context,\n",
    "        )\n",
    "        return\n",
    "\n",
    "    tom_tables_info = []\n",
    "    # Connect to the semantic model using the TOM API to retrieve table metadata.\n",
    "    with labs.tom.connect_semantic_model(\n",
    "        dataset=context[\"source_model_uuid\"],\n",
    "        readonly=True,\n",
    "        workspace=context[\"source_model_workspace_uuid\"],\n",
    "    ) as tom:\n",
    "        # Iterate over tables in the semantic model.\n",
    "        for tbl in tom.model.Tables:\n",
    "            # Get the first partition (if any) to extract source information.\n",
    "            partition = next(iter(tbl.Partitions), None)\n",
    "            if partition and partition.Source:\n",
    "                tom_tables_info.append(\n",
    "                    {\n",
    "                        \"tom_table\": tbl,\n",
    "                        \"schema_name\": getattr(partition.Source, \"SchemaName\", None),\n",
    "                        \"source_table_name\": getattr(partition.Source, \"EntityName\", None),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def get_datastore_columns():\n",
    "        \"\"\"\n",
    "        Retrieves columns from datastore Delta tables by manually reading data from known storage paths.\n",
    "        \n",
    "        For each table from the TOM metadata, constructs a list of candidate paths (with and without schema)\n",
    "        and attempts to load the Delta table from each in order, extracting the column names.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        base_path = (\n",
    "            f\"abfss://{context['source_datastore_workspace_uuid']}\"\n",
    "            f\"@{abfss_base_path}/\"\n",
    "            f\"{context['source_datastore_uuid']}/Tables/\"\n",
    "        )\n",
    "        \n",
    "        for item in tom_tables_info:\n",
    "            schema = item[\"schema_name\"]\n",
    "            entity = item[\"source_table_name\"]\n",
    "            if not entity:\n",
    "                continue\n",
    "\n",
    "            # Build candidate paths: try with schema (if available) first, then without.\n",
    "            candidate_paths = []\n",
    "            if schema:\n",
    "                candidate_paths.append(f\"{base_path}{schema}/{entity}/\")\n",
    "            candidate_paths.append(f\"{base_path}{entity}/\")\n",
    "\n",
    "            loaded = False\n",
    "            last_exception = None\n",
    "\n",
    "            # Try each candidate path until one succeeds.\n",
    "            for path in candidate_paths:\n",
    "                try:\n",
    "                    df = spark.read.format(\"delta\").load(path)\n",
    "                    # On success, record the column names and break out of the loop.\n",
    "                    for col_name in df.schema.fieldNames():\n",
    "                        data.append({\"Table Name\": entity, \"Column Name\": col_name})\n",
    "                    loaded = True\n",
    "                    break\n",
    "                except Exception as ex:\n",
    "                    print(f\"âš ï¸ Unable to read from {path}: {ex}\")\n",
    "                    last_exception = ex\n",
    "\n",
    "            # If no candidate path was successful, raise the last encountered exception.\n",
    "            if not loaded:\n",
    "                raise last_exception\n",
    "\n",
    "        return spark.createDataFrame(data)\n",
    "\n",
    "    # Retrieve all columns from the datastore using the defined helper function.\n",
    "    all_cols_df = get_datastore_columns()\n",
    "\n",
    "    # Ensure the result is a Spark DataFrame (convert if necessary).\n",
    "    if not isinstance(all_cols_df, pyspark.sql.DataFrame):\n",
    "        all_cols_df = spark.createDataFrame(all_cols_df)\n",
    "\n",
    "    # Group the retrieved columns by table name and collect them into sets.\n",
    "    grouped_df = (\n",
    "        all_cols_df.groupBy(\"Table Name\")\n",
    "        .agg(collect_set(\"Column Name\").alias(\"columns\"))\n",
    "        .collect()\n",
    "    )\n",
    "    # Create a dictionary mapping each table to its set of columns.\n",
    "    table_columns = {row[\"Table Name\"]: set(row[\"columns\"]) for row in grouped_df}\n",
    "\n",
    "    remaining_columns = []  # To store columns present in the datastore but unused in the model.\n",
    "    source_mapping = []     # To store mapping between model columns and source columns as defined in TOM.\n",
    "\n",
    "    for info in tom_tables_info:\n",
    "        tbl = info[\"tom_table\"]\n",
    "        src_table = info[\"source_table_name\"]\n",
    "        if not src_table:\n",
    "            continue\n",
    "        # Get the set of columns available in the datastore for this source table.\n",
    "        delta_cols = table_columns.get(src_table, set())\n",
    "        # Get the set of columns used in the model from TOM metadata.\n",
    "        used_cols = {col.SourceColumn for col in tbl.Columns if hasattr(col, \"SourceColumn\")}\n",
    "        # Build the source mapping for each column that has a SourceColumn attribute.\n",
    "        for col in tbl.Columns:\n",
    "            if hasattr(col, \"SourceColumn\"):\n",
    "                source_mapping.append(\n",
    "                    {\n",
    "                        \"TableName\": tbl.Name,\n",
    "                        \"ColumnName\": col.Name,\n",
    "                        \"SourceTableName\": src_table,\n",
    "                        \"SourceColumnName\": col.SourceColumn,\n",
    "                    }\n",
    "                )\n",
    "        # Identify columns in the datastore that are not used in the model.\n",
    "        for unused in delta_cols - used_cols:\n",
    "            remaining_columns.append(\n",
    "                {\n",
    "                    \"TableName\": tbl.Name,\n",
    "                    \"SourceTableName\": src_table,\n",
    "                    \"SourceColumnName\": unused,\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    # If no unused columns were found, insert a placeholder record.\n",
    "    if not remaining_columns:\n",
    "        remaining_columns.append(\n",
    "            {\"TableName\": \"N/A\", \"SourceTableName\": \"N/A\", \"SourceColumnName\": \"N/A\"}\n",
    "        )\n",
    "\n",
    "    # Convert the lists of unused columns and source mappings to Spark DataFrames.\n",
    "    unused_df = spark.createDataFrame(remaining_columns)\n",
    "    mapping_df = spark.createDataFrame(source_mapping)\n",
    "    # Save the unused columns data to the Delta table for unused columns.\n",
    "    save_dataframe_to_delta_table(\n",
    "        data=unused_df,\n",
    "        table_name=historical_table_names[\"unused_columns\"],\n",
    "        context=context,\n",
    "    )\n",
    "    # Save the source mapping data to the Delta table for source mappings.\n",
    "    save_dataframe_to_delta_table(\n",
    "        data=mapping_df,\n",
    "        table_name=historical_table_names[\"source_mapping\"],\n",
    "        context=context,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8064f9-1cfd-46fc-9713-d116141ce85e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Capturing and Processing Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d224f01-da6b-4af9-9cbf-0e0a573d97fe",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def get_reports(context: dict, all_workspaces: pd.DataFrame, report_ids_to_keep: list) -> None:\n",
    "    \"\"\"\n",
    "    Retrieves report information from each workspace and filters to retain only the specified ReportIds.\n",
    "    \n",
    "    For each workspace:\n",
    "      - Calls the Fabric API to list reports.\n",
    "      - Selects and renames columns for consistency.\n",
    "      - Adds workspace identifiers.\n",
    "      - Filters out reports not in the report_ids_to_keep list.\n",
    "      - For any missing ReportIds, creates placeholder rows with default \"Unknown\" values.\n",
    "    The final combined DataFrame is saved to the Delta table for reports.\n",
    "    \"\"\"\n",
    "    reports_list = []  # Initialize a list to store DataFrames of reports from each workspace.\n",
    "    try:\n",
    "        # Iterate over each workspace provided in the all_workspaces DataFrame.\n",
    "        for _, workspace in all_workspaces.iterrows():\n",
    "            ws_id = workspace[\"Id\"]\n",
    "            ws_name = workspace[\"Name\"]\n",
    "            try:\n",
    "                # Retrieve the list of reports for the current workspace using the Fabric API.\n",
    "                reports_df = fabric.list_reports(workspace=ws_id)\n",
    "            except Exception as e:\n",
    "                # If report listing fails for a workspace, log the error and skip to the next workspace.\n",
    "                print(f'âŒ Failed to list reports for workspace \"{ws_name}\" (ID: {ws_id}). Error: {e}')\n",
    "                continue\n",
    "            \n",
    "            # Select only the relevant columns and rename them for consistency.\n",
    "            reports_df = reports_df[[\"Id\", \"Name\", \"Web Url\"]].rename(\n",
    "                columns={\"Id\": \"ReportId\", \"Name\": \"ReportName\", \"Web Url\": \"WebUrl\"}\n",
    "            )\n",
    "            # Add workspace-specific identifiers to the report DataFrame.\n",
    "            reports_df[\"WorkspaceId\"] = ws_id\n",
    "            reports_df[\"WorkspaceName\"] = ws_name\n",
    "            # Filter the reports to keep only those whose ReportId is in the report_ids_to_keep list.\n",
    "            reports_df = reports_df[reports_df[\"ReportId\"].isin(report_ids_to_keep)]\n",
    "            # Append the filtered DataFrame to the list.\n",
    "            reports_list.append(reports_df)\n",
    "        \n",
    "        # Combine all report DataFrames into one; if none were found, create an empty DataFrame with the required columns.\n",
    "        combined_reports = (\n",
    "            pd.concat(reports_list, ignore_index=True)\n",
    "            if reports_list\n",
    "            else pd.DataFrame(\n",
    "                columns=[\n",
    "                    \"ReportId\",\n",
    "                    \"ReportName\",\n",
    "                    \"WebUrl\",\n",
    "                    \"WorkspaceId\",\n",
    "                    \"WorkspaceName\",\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        # Determine which ReportIds from the desired list are missing in the combined reports.\n",
    "        missing_ids = set(report_ids_to_keep) - set(combined_reports[\"ReportId\"])\n",
    "        if missing_ids:\n",
    "            # For any missing ReportIds, create placeholder rows with default \"Unknown\" values.\n",
    "            missing_df = pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"ReportId\": rid,\n",
    "                        \"ReportName\": \"Unknown\",\n",
    "                        \"WebUrl\": \"Unknown\",\n",
    "                        \"WorkspaceId\": \"Unknown\",\n",
    "                        \"WorkspaceName\": \"Unknown\",\n",
    "                    }\n",
    "                    for rid in missing_ids\n",
    "                ]\n",
    "            )\n",
    "            # Append the placeholder rows to the combined DataFrame.\n",
    "            combined_reports = pd.concat(\n",
    "                [combined_reports, missing_df], ignore_index=True\n",
    "            )\n",
    "        # Save the final combined reports DataFrame to the Delta table for source reports.\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=combined_reports,\n",
    "            table_name=historical_table_names[\"source_reports\"],\n",
    "            context=context,\n",
    "        )\n",
    "        print(f\"âœ… Retrieved and saved {len(combined_reports)} reports.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ get_reports encountered an error: {e}\")\n",
    "        raise\n",
    "\n",
    "@log_function_calls\n",
    "def get_app_reports(context: dict) -> None:\n",
    "    \"\"\"\n",
    "    Retrieves all Power BI apps accessible and\n",
    "    lists the reports contained in each app, then saves the combined\n",
    "    DataFrame to the Delta table for app reports.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List all apps\n",
    "        client = fabric.PowerBIRestClient()\n",
    "        apps_response = client.get(\"https://api.powerbi.com/v1.0/myorg/apps\")\n",
    "        apps_df = pd.DataFrame(apps_response.json().get(\"value\", []))\n",
    "        \n",
    "        reports_list = []\n",
    "        # For each app, list its reports\n",
    "        for _, app in apps_df.iterrows():\n",
    "            app_id = app[\"id\"]\n",
    "            app_name = app[\"name\"]\n",
    "            try:\n",
    "                resp = client.get(\n",
    "                    f\"https://api.powerbi.com/v1.0/myorg/apps/{app_id}/reports\"\n",
    "                )\n",
    "                reports_df = pd.DataFrame(resp.json().get(\"value\", []))\n",
    "                \n",
    "                if reports_df.empty:\n",
    "                    # No reports to keep in this app â†’ skip\n",
    "                    continue\n",
    "\n",
    "                # Tag each row with its source app\n",
    "                reports_df[\"appName\"] = app_name\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'âŒ Failed to list reports for app \"{app_name}\" (ID: {app_id}): {e}')\n",
    "                continue\n",
    "\n",
    "            reports_list.append(reports_df)\n",
    "\n",
    "        # Combine or build an empty schema if nothing matched\n",
    "        if reports_list:\n",
    "            combined = pd.concat(reports_list, ignore_index=True)\n",
    "            combined = combined.drop(columns=[\"users\", \"subscriptions\"])\n",
    "        else:\n",
    "            # Ensure consistent schema even if empty\n",
    "            combined = pd.DataFrame(columns=[\"id\", \"reportType\", \"name\", \"webUrl\", \"embedUrl\", \"isOwnedByMe\",\n",
    "                \"datasetId\", \"appId\", \"originalReportObjectId\", \"reportFlags\", \"appName\", \"description\"])\n",
    "\n",
    "        # Persist to Delta\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=combined,\n",
    "            table_name=historical_table_names[\"source_app_reports\"],\n",
    "            context=context,\n",
    "        )\n",
    "        print(f\"âœ… Retrieved and saved {len(combined)} appâ€report records.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ get_app_reports encountered a fatal error: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23b213-57a3-4654-997e-b1aa8ba88e38",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Cold Cache Helpers: Log Table, Model Refresh, Cache Clear, and Timing Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a826e-017e-4e28-97c4-67549f954e20",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def fetch_log_table(context: dict, table_name: str) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Attempts to fetch a log table for today's QueryEnd events for the current model.\n",
    "\n",
    "    Returns:\n",
    "      A Spark DataFrame filtered by ModelUuid, AsOfDate, and EventClass,\n",
    "      or None if the table does not exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the table from Spark using its name.\n",
    "        raw_tbl = spark.read.table(table_name)\n",
    "        # Apply filters: match the current model, today's date, and ensure the event class is \"QueryEnd\".\n",
    "        filters = (\n",
    "            (col(\"ModelUuid\") == context[\"source_model_uuid\"]) &\n",
    "            (col(\"AsOfDate\") == context[\"as_of_date\"]) &\n",
    "            (col(\"EventClass\") == \"QueryEnd\")\n",
    "        )\n",
    "        return raw_tbl.filter(filters)\n",
    "    except Exception:\n",
    "        # Log informational message if the table is not found; it may be created later.\n",
    "        print(f\"â„¹ï¸ Log table `{table_name}` does not exist. It will be created if needed.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "@log_function_calls\n",
    "def wait_for_model_creation() -> None:\n",
    "    \"\"\"\n",
    "    Polls the target workspace until the cloned model is created.\n",
    "\n",
    "    Continues checking every 5 seconds until the cloned model appears in the dataset list.\n",
    "    \"\"\"\n",
    "    # Continuously check if the cloned model is present in the dataset list from the target workspace.\n",
    "    while (\n",
    "        cloned_model_name\n",
    "        not in fabric.list_datasets(workspace=cold_cache_target_workspace_name, mode=\"rest\")[\"Dataset Name\"].to_list()\n",
    "    ):\n",
    "        print(\"âŒ› Waiting for cloned model creation...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "\n",
    "@log_function_calls\n",
    "def refresh_dataset(model_name: str, refresh_type: str) -> None:\n",
    "    \"\"\"\n",
    "    Initiates a dataset refresh using the specified refresh type (e.g., \"clearValues\" or \"full\").\n",
    "\n",
    "    Waits until the refresh operation completes with a status in valid_refresh_statuses.\n",
    "    \"\"\"\n",
    "    attempts = 0\n",
    "    # Start the refresh process and obtain a refresh status identifier.\n",
    "    refresh_status = fabric.refresh_dataset(model_name, refresh_type=refresh_type)\n",
    "    # Poll until the refresh status indicates completion or failure.\n",
    "    while (\n",
    "        fabric.get_refresh_execution_details(model_name, refresh_status).status not in valid_refresh_statuses\n",
    "    ):\n",
    "        attempts += 1\n",
    "        if attempts >= max_attempts:\n",
    "            raise Exception(f\"âŒ Refresh failed after {attempts} attempts.\")\n",
    "        # Wait briefly before the next check.\n",
    "        time.sleep(3)\n",
    "\n",
    "\n",
    "@log_function_calls\n",
    "def clear_cache(model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Clears the VertiPaq cache for the specified model.\n",
    "\n",
    "    It calls a helper function to clear the cache and then verifies the operation\n",
    "    by executing a trivial DAX query. The process is retried until successful or\n",
    "    until the maximum number of attempts is reached.\n",
    "    \"\"\"\n",
    "    attempts = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # Attempt to clear the cache via a helper function.\n",
    "            labs.clear_cache(model_name)\n",
    "            # Verify the cache clear by executing a simple DAX query.\n",
    "            fabric.evaluate_dax(model_name, \"EVALUATE {1}\")\n",
    "            print(f\"âœ… Cache cleared for model `{model_name}`.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            print(f\"âš ï¸ Cache clear attempt failed: {e}\")\n",
    "            if attempts >= max_attempts:\n",
    "                raise Exception(\"âŒ Failed to clear VertiPaq cache.\")\n",
    "            # Refresh the TOM cache for the target workspace to ensure consistency.\n",
    "            fabric.refresh_tom_cache(cold_cache_target_workspace_name)\n",
    "            time.sleep(5)\n",
    "\n",
    "\n",
    "def capture_cold_cache_timings(column_name: str, trace) -> None:\n",
    "    \"\"\"\n",
    "    Executes a DAX query for a specific column to measure cold cache performance.\n",
    "\n",
    "    After running the query, it checks the trace logs for a QueryEnd event that\n",
    "    references the given column name. Raises an exception if the expected event\n",
    "    is not found after a specified number of attempts.\n",
    "    \"\"\"\n",
    "    # Construct a DAX expression to query a sample of the column's values.\n",
    "    dax_expr = f\"EVALUATE TOPN(1, VALUES({column_name}))\"\n",
    "    try:\n",
    "        # Execute the DAX query on the cloned model.\n",
    "        fabric.evaluate_dax(cloned_model_name, dax_expr)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ DAX evaluation error for {column_name}: {e}\")\n",
    "        raise Exception(f\"Failed to evaluate DAX for {column_name}: {e}\")\n",
    "    attempts = 0\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            # Retrieve trace logs from the trace object.\n",
    "            trace_logs = trace.get_trace_logs()\n",
    "            if trace_logs is None:\n",
    "                raise Exception(f\"Trace logs are None for column {column_name}\")\n",
    "            # Look for a QueryEnd event that includes the column name in its Text Data.\n",
    "            matching_logs = trace_logs[\n",
    "                (trace_logs[\"Event Class\"] == \"QueryEnd\") &\n",
    "                (trace_logs[\"Text Data\"].str.contains(re.escape(column_name), na=False))\n",
    "            ]\n",
    "            if not matching_logs.empty:\n",
    "                break  # Expected trace log found; exit loop.\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading trace logs for {column_name}: {e}\")\n",
    "            raise Exception(f\"Failed to access trace logs for {column_name}: {e}\")\n",
    "        attempts += 1\n",
    "        if attempts >= max_attempts:\n",
    "            raise Exception(f\"âŒ Failed after {attempts} attempts for column {column_name}\")\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbe3c0-3113-4639-a8c5-66f1741f0626",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Capturing Cold Cache Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbefda-b9cb-4a89-9efe-2e6aa7846335",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def capture_cold_cache_performance(context: dict, model_columns: pd.DataFrame) -> set:\n",
    "    \"\"\"\n",
    "    Measures cold cache performance for eligible columns by deploying a cloned model and executing parallel DAX queries.\n",
    "\n",
    "    Steps:\n",
    "      1. Set up and deploy a cloned version of the model.\n",
    "      2. Refresh and clear the cache of the cloned model.\n",
    "      3. Create a trace to capture QueryEnd events and measure performance.\n",
    "      4. Execute DAX queries in parallel for each eligible column.\n",
    "      5. Save the trace logs with performance metrics.\n",
    "\n",
    "    Returns:\n",
    "      A set of column identifiers (formatted strings) that were successfully processed.\n",
    "    \"\"\"\n",
    "    global cloned_model_name, valid_refresh_statuses, max_attempts\n",
    "\n",
    "    # Define the cloned model's name based on the source model.\n",
    "    cloned_model_name = f\"{context['source_model_name']} - Semantic Model Audit\"\n",
    "    # Valid statuses indicating the refresh operation has completed.\n",
    "    valid_refresh_statuses = [\"Completed\", \"Failed\"]\n",
    "    # Maximum number of attempts for refresh and cache clearing.\n",
    "    max_attempts = 120\n",
    "\n",
    "    # Try to get existing cold cache log data, if available.\n",
    "    log_tbl = fetch_log_table(context, historical_table_names[\"cold_cache_measurements\"])\n",
    "\n",
    "    # Filter out columns not eligible for cold cache measurement (e.g., those starting with \"RowNumber-\").\n",
    "    eligible_df = model_columns[~model_columns[\"ColumnName\"].str.startswith(\"RowNumber-\")]\n",
    "    # Format each eligible column as a string representation: 'TableName'[ColumnName]\n",
    "    eligible_columns = [\n",
    "        f\"'{row['TableName']}'[{row['ColumnName']}]\"\n",
    "        for _, row in eligible_df.iterrows()\n",
    "    ]\n",
    "\n",
    "    if log_tbl is not None:\n",
    "        # Group the existing log data by column name and count entries.\n",
    "        counts_df = log_tbl.groupBy(\"ColumnName\").count()\n",
    "        # Identify columns that have already reached the maximum queries per day.\n",
    "        columns_to_skip = {\n",
    "            row[\"ColumnName\"]\n",
    "            for row in counts_df.filter(col(\"count\") >= max_queries_daily)\n",
    "            .select(\"ColumnName\")\n",
    "            .collect()\n",
    "        }\n",
    "        # Exclude columns that should be skipped.\n",
    "        filtered_columns = [col for col in eligible_columns if col not in columns_to_skip]\n",
    "        num_skipped = len(columns_to_skip)\n",
    "    else:\n",
    "        filtered_columns = eligible_columns\n",
    "        num_skipped = 0\n",
    "\n",
    "    num_query = len(filtered_columns)\n",
    "    print(f\"ðŸ“Š {num_query} columns to query; {num_skipped} columns skipped.\")\n",
    "\n",
    "    # If cold cache measurements should be collected and there are columns to query.\n",
    "    if collect_cold_cache_measurements and num_query > 0:\n",
    "        try:\n",
    "            # Deploy a cloned version of the model for cold cache testing.\n",
    "            labs.deploy_semantic_model(\n",
    "                source_dataset=context[\"source_model_name\"],\n",
    "                source_workspace=context[\"source_model_workspace_name\"],\n",
    "                target_dataset=cloned_model_name,\n",
    "                target_workspace=cold_cache_target_workspace_name,\n",
    "                refresh_target_dataset=False,\n",
    "                overwrite=True,\n",
    "            )\n",
    "            # Refresh the TOM cache for the target workspace.\n",
    "            fabric.refresh_tom_cache(cold_cache_target_workspace_name)\n",
    "            time.sleep(30)  # Wait for the cache refresh to settle.\n",
    "            wait_for_model_creation()  # Poll until the cloned model is created.\n",
    "            # Refresh the cloned model's dataset with a \"clearValues\" and then \"full\" refresh.\n",
    "            refresh_dataset(cloned_model_name, \"clearValues\")\n",
    "            refresh_dataset(cloned_model_name, \"full\")\n",
    "            # Clear the VertiPaq cache for the cloned model.\n",
    "            clear_cache(cloned_model_name)\n",
    "            time.sleep(5)  # Short delay after clearing the cache.\n",
    "\n",
    "            # Set up a trace connection to capture QueryEnd events for performance metrics.\n",
    "            trace_conn = fabric.create_trace_connection(\n",
    "                dataset=cloned_model_name, workspace=cold_cache_target_workspace_name\n",
    "            )\n",
    "            trace_conn.drop_traces()  # Clear any existing traces.\n",
    "            trace_name = f\"Simple DAX Trace {uuid4()}\"\n",
    "            event_schema = {\n",
    "                \"QueryEnd\": [\"EventClass\", \"TextData\", \"Duration\", \"CpuTime\", \"Success\"]\n",
    "            }\n",
    "            # Create a trace within a context manager to ensure proper resource management.\n",
    "            with fabric.create_trace_connection(\n",
    "                dataset=cloned_model_name, workspace=cold_cache_target_workspace_name\n",
    "            ) as trace_conn:\n",
    "                with trace_conn.create_trace(event_schema=event_schema, name=trace_name) as trace:\n",
    "                    trace.start()\n",
    "                    # Wait until the trace has started.\n",
    "                    while not trace.is_started:\n",
    "                        time.sleep(2)\n",
    "                    print(\"ðŸ”„ Querying columns in parallel...\")\n",
    "                    total_cols = len(filtered_columns)\n",
    "                    if total_cols == 0:\n",
    "                        print(\"â„¹ï¸ No columns to query after filtering.\")\n",
    "                        return set()\n",
    "                    # Set up progress tracking.\n",
    "                    progress_interval = math.ceil(total_cols / 10)\n",
    "                    next_progress = progress_interval\n",
    "                    completed = 0\n",
    "                    successful = set()\n",
    "                    failed = set()\n",
    "                    # Execute queries in parallel using a thread pool.\n",
    "                    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                        future_to_col = {\n",
    "                            executor.submit(capture_cold_cache_timings, col, trace): col\n",
    "                            for col in filtered_columns\n",
    "                        }\n",
    "                        for future in as_completed(future_to_col):\n",
    "                            col_name = future_to_col[future]\n",
    "                            try:\n",
    "                                # This will raise an exception if the query for this column fails.\n",
    "                                future.result()\n",
    "                                successful.add(col_name)\n",
    "                            except Exception as e:\n",
    "                                print(f\"âŒ Error processing {col_name}: {e}\")\n",
    "                                failed.add(col_name)\n",
    "                                continue\n",
    "                            completed += 1\n",
    "                            # Print progress updates at regular intervals.\n",
    "                            if completed >= next_progress:\n",
    "                                print(f\"âœ… {completed / total_cols * 100:.0f}% of columns completed.\")\n",
    "                                next_progress += progress_interval\n",
    "                    try:\n",
    "                        # Stop the trace and retrieve the captured trace logs.\n",
    "                        trace_logs = trace.stop()\n",
    "                        if trace_logs is not None and not trace_logs.empty:\n",
    "                            # Extract the column name from the \"Text Data\" field using a regex.\n",
    "                            trace_logs[\"ColumnName\"] = trace_logs[\"Text Data\"].str.extract(r\"VALUES\\s*\\(\\s*(.+?)\\s*\\)\\s*\\)\")\n",
    "                            # Save the trace logs to the Delta table for cold cache measurements.\n",
    "                            save_dataframe_to_delta_table(\n",
    "                                data=trace_logs,\n",
    "                                table_name=historical_table_names[\"cold_cache_measurements\"],\n",
    "                                context=context,\n",
    "                                QueryUuid=str(uuid4()),\n",
    "                            )\n",
    "                    except Exception as e:\n",
    "                        print(f\"âŒ Failed to process trace logs: {e}\")\n",
    "                    if failed:\n",
    "                        print(f\"âš ï¸ {len(failed)} columns failed: {', '.join(failed)}\")\n",
    "                    else:\n",
    "                        print(\"âœ… All columns processed successfully.\")\n",
    "                    print(f\"âœ… Cold cache performance capture complete. {len(successful)} columns queried successfully.\")\n",
    "                    return successful\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during cold cache performance capture: {e}\")\n",
    "            return set()\n",
    "    else:\n",
    "        print(\"â„¹ï¸ No columns to query after filtering; inserting placeholder.\")\n",
    "        try:\n",
    "            # If no columns are eligible, insert a placeholder record to maintain table structure.\n",
    "            save_dataframe_to_delta_table(\n",
    "                data=pd.DataFrame({\n",
    "                    \"EventClass\": [\"N/A\"],\n",
    "                    \"TextData\": [\"N/A\"],\n",
    "                    \"Duration\": [0],\n",
    "                    \"CpuTime\": [0],\n",
    "                    \"Success\": [\"N/A\"],\n",
    "                    \"ColumnName\": [\"N/A\"],\n",
    "                }),\n",
    "                table_name=historical_table_names[\"cold_cache_measurements\"],\n",
    "                context=context,\n",
    "                QueryUuid=str(uuid4()),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to insert placeholder for cold cache measurements: {e}\")\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d5cfd-613c-4b4d-b3da-4c8717cacfd8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Capturing Resident Column Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dde9ed4-001a-4c14-b85a-52bf3693c866",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def capture_resident_statistics(context: dict, queried_columns: set) -> None:\n",
    "    \"\"\"\n",
    "    Captures resident statistics (e.g., whether columns are loaded in memory, sizes) for model columns.\n",
    "    \n",
    "    It compares current model columns (using the Fabric API) with historical resident statistics,\n",
    "    and saves only new records for columns that have not been recorded yet.\n",
    "    \"\"\"\n",
    "    \n",
    "    def format_column(row: dict) -> str:\n",
    "        # Standardize the column identifier by combining the table and column names.\n",
    "        return f\"'{row['TableName']}'[{row['ColumnName']}]\"\n",
    "    \n",
    "    try:\n",
    "        # Read previously captured resident statistics for the current date from the Delta table.\n",
    "        existing_stats = (\n",
    "            spark.read.table(historical_table_names[\"resident_statistics\"])\n",
    "            .filter(\n",
    "                (col(\"ModelUuid\") == context[\"source_model_uuid\"]) &\n",
    "                (col(\"AsOfDate\") == context[\"as_of_date\"])\n",
    "            )\n",
    "            .select(\"TableName\", \"ColumnName\")\n",
    "            .collect()\n",
    "        )\n",
    "        # Create a set of standardized identifiers for the existing resident statistics.\n",
    "        existing = {format_column(row.asDict()) for row in existing_stats}\n",
    "    except Exception:\n",
    "        print(\"âš ï¸ Could not read existing resident statistics; proceeding without.\")\n",
    "        existing = set()\n",
    "    \n",
    "    # Determine which model to query: if cold cache was measured, use the cloned model; otherwise, use the source model.\n",
    "    resident_model = (\n",
    "        cloned_model_name\n",
    "        if collect_cold_cache_measurements\n",
    "        else context[\"source_model_name\"]\n",
    "    )\n",
    "    resident_workspace = (\n",
    "        cold_cache_target_workspace_name\n",
    "        if collect_cold_cache_measurements\n",
    "        else context[\"source_model_workspace_name\"]\n",
    "    )\n",
    "    \n",
    "    # Retrieve the current list of model columns using the Fabric API.\n",
    "    model_columns_resident = fabric.list_columns(\n",
    "        dataset=resident_model,\n",
    "        extended=True,\n",
    "        workspace=resident_workspace,\n",
    "    )\n",
    "    # Remove spaces from column names to ensure consistency in identifiers.\n",
    "    model_columns_resident.columns = model_columns_resident.columns.str.replace(\" \", \"\", regex=True)\n",
    "    # Build a set of standardized identifiers for the current model columns.\n",
    "    model_set = {format_column(row) for _, row in model_columns_resident.iterrows()}\n",
    "    \n",
    "    # Identify columns that are new (i.e., not present in the historical resident statistics).\n",
    "    to_capture = model_set - existing\n",
    "    if collect_cold_cache_measurements:\n",
    "        # Optionally restrict to only columns that were previously queried for cold cache metrics.\n",
    "        to_capture = to_capture.intersection(queried_columns)\n",
    "    to_capture = list(to_capture)\n",
    "    \n",
    "    if to_capture:\n",
    "        # Filter the current model columns DataFrame to include only those columns that are new.\n",
    "        filtered = [\n",
    "            row\n",
    "            for _, row in model_columns_resident.iterrows()\n",
    "            if format_column(row) in to_capture\n",
    "        ]\n",
    "        filtered_df = pd.DataFrame(filtered)\n",
    "        print(f\"ðŸ“ˆ Capturing resident statistics for {len(filtered_df)} new columns.\")\n",
    "        # Save the new resident statistics to the designated Delta table.\n",
    "        save_dataframe_to_delta_table(\n",
    "            data=filtered_df,\n",
    "            table_name=historical_table_names[\"resident_statistics\"],\n",
    "            context=context,\n",
    "        )\n",
    "    else:\n",
    "        print(\"â„¹ï¸ No new resident statistics to capture for this run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0250cc-64ff-4b3e-9d66-71b9f4b3377b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Workspace Monitoring Information and Datastore Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b01fec-eaf4-4b25-be1e-07dc19179834",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def get_workspace_monitoring_info(workspace: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Retrieves the Query Service URI and KQL Database Id for the Monitoring KQL database in the given workspace.\n",
    "    \n",
    "    This function queries the list of KQL databases in the workspace using the Fabric API (via labs.list_kql_databases).\n",
    "    It then filters the result to find the database named \"Monitoring KQL database\". If such a database is not found,\n",
    "    it raises a ValueError. Otherwise, it extracts and returns the Query Service URI and the KQL Database Id as a tuple.\n",
    "    \n",
    "    Returns:\n",
    "      A tuple (kusto_uri, kusto_db_guid) where:\n",
    "        - kusto_uri: The URI for querying the KQL database.\n",
    "        - kusto_db_guid: The unique identifier (GUID) for the KQL database.\n",
    "    \n",
    "    Raises:\n",
    "      ValueError: If no KQL databases or the specific \"Monitoring KQL database\" is found in the workspace.\n",
    "    \"\"\"\n",
    "    # Retrieve a DataFrame containing all KQL databases for the given workspace.\n",
    "    df = labs.list_kql_databases(workspace=workspace)\n",
    "    # Check if the DataFrame is empty; if so, no KQL databases exist in the workspace.\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"âŒ No KQL databases found in workspace `{workspace}`.\")\n",
    "    # Filter the DataFrame to find the row where the KQL Database Name matches \"Monitoring KQL database\".\n",
    "    df_monitor = df[df[\"KQL Database Name\"] == \"Monitoring KQL database\"]\n",
    "    # If no matching database is found, raise an error.\n",
    "    if df_monitor.empty:\n",
    "        raise ValueError(\n",
    "            f\"âŒ Monitoring KQL database not found in workspace `{workspace}`.\"\n",
    "        )\n",
    "    # Extract the Query Service URI from the first (and expected only) matching row.\n",
    "    kusto_uri = df_monitor.iloc[0][\"Query Service URI\"]\n",
    "    # Extract the KQL Database Id (GUID) from the same row.\n",
    "    kusto_db_uuid = df_monitor.iloc[0][\"KQL Database Id\"]\n",
    "    # Return the extracted URI and Database Id as a tuple.\n",
    "    return kusto_uri, kusto_db_uuid\n",
    "\n",
    "\n",
    "def resolve_datastore_id(data_store_name: str, workspace_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Resolves the ID for a given datastore (lakehouse or warehouse) based on its name and associated workspace.\n",
    "    \n",
    "    The function first attempts to resolve the ID as a lakehouse by calling:\n",
    "        labs.resolve_lakehouse_id(data_store_name, workspace_name)\n",
    "    If that call fails (either by raising an exception or returning a falsy value), it then attempts\n",
    "    to resolve the ID as a warehouse using:\n",
    "        abs.resolve_warehouse_id(data_store_name, workspace_name)\n",
    "    \n",
    "    If both attempts fail, a RuntimeError is raised. No error or message is shown if the ID is resolved successfully.\n",
    "    \n",
    "    Parameters:\n",
    "        data_store_name (str): The name of the datastore (lakehouse or warehouse).\n",
    "        workspace_name (str): The name of the workspace associated with the datastore.\n",
    "    \n",
    "    Returns:\n",
    "        str: The resolved datastore ID.\n",
    "    \n",
    "    Raises:\n",
    "        RuntimeError: If neither a lakehouse nor a warehouse ID can be resolved.\n",
    "    \"\"\"\n",
    "    datastore_id = None\n",
    "\n",
    "    # Attempt to resolve as a lakehouse\n",
    "    try:\n",
    "        datastore_id = labs.resolve_lakehouse_id(data_store_name, workspace_name)\n",
    "        if datastore_id:\n",
    "            return datastore_id\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Attempt to resolve as a warehouse if lakehouse resolution fails\n",
    "    try:\n",
    "        datastore_id = labs.resolve_warehouse_id(data_store_name, workspace_name)\n",
    "        if datastore_id:\n",
    "            return datastore_id\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"Failed to resolve a valid ID for datastore '{data_store_name}' in workspace '{workspace_name}'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4dddd-c67e-4de4-bdb7-b7ebcdefbc16",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Main Orchestration: Collecting Statistics for Each Semantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e158cd5-41b8-46e2-9819-f77d44c87f38",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "@log_function_calls\n",
    "def collect_model_statistics(models: list) -> None:\n",
    "    \"\"\"\n",
    "    Main orchestration function that processes each semantic model to capture various statistics.\n",
    "    \n",
    "    For each model in the provided list, the function performs the following steps:\n",
    "      1. Clean up incomplete historical data if configured (by dropping tables or removing incomplete runs).\n",
    "      2. Build a context dictionary containing all necessary model and workspace details.\n",
    "      3. Record the start of the run in the run_history table.\n",
    "      4. Retrieve workspace monitoring information (using datastore details if available).\n",
    "      5. Capture model objects (columns and measures) and save them for historical tracking.\n",
    "      6. Capture measure dependencies via a DAX query.\n",
    "      7. Capture query counts for various model objects and update mappings accordingly.\n",
    "      8. Retrieve detailed query logs and extract ReportIds.\n",
    "      9. Capture unused Delta table columns and cold cache performance.\n",
    "      10. Capture resident column statistics (e.g., column residency in memory).\n",
    "      11. Record the run completion status (marking the run as completed or failed).\n",
    "    \n",
    "    If any critical step fails for a model, the function marks the run as failed and proceeds with the next model.\n",
    "    \"\"\"\n",
    "    # Step 1: Clean up historical data if configured.\n",
    "    if force_delete_historical_tables:\n",
    "        print(\"âš ï¸ Force-deleting historical tables. All data will be lost.\")\n",
    "        drop_historical_tables()\n",
    "    elif force_delete_incomplete_runs:\n",
    "        print(\"â„¹ï¸ Removing records for incomplete runs.\")\n",
    "        cleanup_incomplete_runs()\n",
    "\n",
    "    # Step 2: Retrieve all workspaces available in the system.\n",
    "    all_workspaces = fabric.list_workspaces()\n",
    "\n",
    "    # Process each model from the provided models list.\n",
    "    for model in models:\n",
    "        now = datetime.now()  # Capture current timestamp for this run.\n",
    "        # Step 2: Build the context dictionary with required metadata.\n",
    "        context = {\n",
    "            \"run_uuid\": str(uuid4()),\n",
    "            \"as_of_datetime\": now,\n",
    "            \"as_of_date\": now.date(),\n",
    "            \"source_model_workspace_name\": model[\"model_workspace_name\"],\n",
    "            \"source_model_workspace_uuid\": fabric.resolve_workspace_id(model[\"model_workspace_name\"]),\n",
    "            \"source_model_name\": model[\"model_name\"],\n",
    "            \"source_model_uuid\": fabric.resolve_dataset_id(model[\"model_name\"], model[\"model_workspace_name\"]),\n",
    "        }\n",
    "        # Include datastore details in the context if they are provided.\n",
    "        if model[\"datastore_name\"] and model[\"datastore_workspace_name\"]:\n",
    "            context.update({\n",
    "                \"source_datastore_name\": model[\"datastore_name\"],\n",
    "                \"source_datastore_workspace_name\": model[\"datastore_workspace_name\"],\n",
    "                \"source_datastore_uuid\": resolve_datastore_id(model[\"datastore_name\"], model[\"datastore_workspace_name\"]),\n",
    "                \"source_datastore_workspace_uuid\": fabric.resolve_workspace_id(model[\"datastore_workspace_name\"]),\n",
    "            })\n",
    "        else:\n",
    "            # Use empty strings if datastore details are not provided.\n",
    "            context.update({\n",
    "                \"source_datastore_name\": \"\",\n",
    "                \"source_datastore_workspace_name\": \"\",\n",
    "                \"source_datastore_uuid\": \"\",\n",
    "                \"source_datastore_workspace_uuid\": \"\",\n",
    "            })\n",
    "        try:\n",
    "            print(f\"ðŸ“ Processing model `{model['model_name']}`\")\n",
    "            # Step 3: Record the start of the run.\n",
    "            record_run_start(context)\n",
    "\n",
    "            # Step 4: Determine workspace monitoring info.\n",
    "            if model[\"log_analytics_kusto_uri\"] or model[\"log_analytics_kusto_database_uuid\"]:\n",
    "                context[\"log_analytics_kusto_uri\"] = model[\"log_analytics_kusto_uri\"]\n",
    "                context[\"log_analytics_kusto_database\"] = model[\"log_analytics_kusto_database_uuid\"]\n",
    "            elif context[\"source_model_workspace_uuid\"]:\n",
    "                (context[\"log_analytics_kusto_uri\"],\n",
    "                 context[\"log_analytics_kusto_database\"]) = get_workspace_monitoring_info(context[\"source_model_workspace_uuid\"])\n",
    "            else:\n",
    "                context[\"log_analytics_kusto_uri\"] = \"\"\n",
    "                context[\"log_analytics_kusto_database\"] = \"\"\n",
    "\n",
    "            report_ids = set()  # To accumulate ReportIds from detailed logs.\n",
    "\n",
    "            try:\n",
    "                # Step 5: Capture model objects (columns and measures).\n",
    "                model_columns, model_measures = capture_semantic_model_objects(context)\n",
    "                # Step 6: Capture measure dependencies.\n",
    "                capture_semantic_model_dependencies(context, model_measures)\n",
    "                # Step 7: Capture query counts and update object mappings.\n",
    "                report_ids = capture_logs_and_mappings(context, model_columns, model_measures)\n",
    "                if report_ids:\n",
    "                    # Step 8a: Retrieve and save reports using the collected ReportIds.\n",
    "                    get_reports(context, all_workspaces, list(report_ids))\n",
    "                else:\n",
    "                    print(\"â„¹ï¸ No ReportIds found to process.\")\n",
    "                # Step 8b: Retrieve and save report app mapping.\n",
    "                get_app_reports(context)\n",
    "            except Exception as critical_err:\n",
    "                print(f\"âŒ Critical step failed for model `{model['model_name']}`: {critical_err}\")\n",
    "                try:\n",
    "                    record_run_completion(context, \"failed\")\n",
    "                    print(f\"ðŸ”´ Run UUID: {context['run_uuid']} marked as failed.\")\n",
    "                except Exception as update_err:\n",
    "                    print(f\"âŒ Failed to mark run UUID: {context['run_uuid']} as failed. Error: {update_err}\")\n",
    "                continue  # Skip to next model if a critical step fails.\n",
    "\n",
    "            try:\n",
    "                # Step 9: Capture unused Delta table columns.\n",
    "                capture_unused_delta_columns(context)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Failed to capture unused Delta columns for model `{model['model_name']}`: {e}\")\n",
    "\n",
    "            try:\n",
    "                # Step 9 (continued): Capture cold cache performance metrics.\n",
    "                queried_cols = capture_cold_cache_performance(context, model_columns)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Cold cache performance capture failed for model `{model['model_name']}`: {e}\")\n",
    "                queried_cols = set()\n",
    "\n",
    "            try:\n",
    "                # Step 10: Capture resident statistics for columns.\n",
    "                capture_resident_statistics(context, queried_cols)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Resident statistics capture failed for model `{model['model_name']}`: {e}\")\n",
    "\n",
    "            try:\n",
    "                # Step 11: Mark the run as completed.\n",
    "                record_run_completion(context, \"completed\")\n",
    "                print(f\"âœ… Run UUID: {context['run_uuid']} completed successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to mark run UUID: {context['run_uuid']} as completed. Error: {e}\")\n",
    "        except Exception as e:\n",
    "            # Log any unexpected error during processing and move to the next model.\n",
    "            print(f\"âŒ Unexpected error processing model `{model['model_name']}`: {e}\")\n",
    "            continue  # Continue with next model on error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a0322-7bc8-4dd9-8b23-60e7929c9b1d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Execute the Statistics Collection for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ae15f-3f81-42ef-b051-78b75a2a0795",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"f4bb0e6a-b53e-44d5-bb6f-69bde5b2e449\",\"activityId\":\"70bb5c8c-7698-4eeb-8837-503f08c96015\",\"applicationId\":\"application_1741620896689_0001\",\"jobGroupId\":\"24\",\"advices\":{\"warn\":20}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the main function to process all models defined in the models list.\n",
    "collect_model_statistics(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7213b-d4aa-4e49-8ddb-f99be5009347",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Generate Star Schema\n",
    "### Helper Function for Star Schema: Generate Table Key\n",
    "Used throughout the star schema creation SQL to produce unique keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f49953-1a3e-4256-baba-dbbd0551c57b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def generate_table_key(*columns) -> str:\n",
    "    \"\"\"\n",
    "    Generates a SQL expression to produce a unique key from the concatenated values of the given columns.\n",
    "    \n",
    "    It uses IFNULL to replace NULLs, CONCAT to join values, MD5 for hashing,\n",
    "    and CONV to convert the hash to a BIGINT.\n",
    "    \n",
    "    Returns:\n",
    "      A string containing the SQL expression for the unique key.\n",
    "    \"\"\"\n",
    "    # Create IFNULL expressions for each column to avoid NULL issues.\n",
    "    ifnull_parts = [f\"IFNULL({col}, '')\" for col in columns]\n",
    "    # Build the SQL expression.\n",
    "    sql_expr = f\"\"\"\n",
    "        CAST(CONV(\n",
    "            RIGHT(MD5(CONCAT({\", \".join(ifnull_parts)})), 16),\n",
    "            16,\n",
    "            -10\n",
    "        ) AS BIGINT)\n",
    "    \"\"\"\n",
    "    return sql_expr.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cbf4b0-00dd-4e33-8386-1827daaa4433",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create ```DIM_ModelObject```\n",
    "Collects the most recent column, measure, and unused column definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47061cf3-2f39-4f9d-9e1c-3195c3a56abd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "query_result = spark.sql(f\"\"\"\n",
    "    -- Get the latest report measures\n",
    "    WITH latest_report_measures AS (\n",
    "        SELECT\n",
    "            mapping.TableName,\n",
    "            mapping.ObjectName,\n",
    "            mapping.ObjectType,\n",
    "            mapping.ModelObject AS Expression,\n",
    "            '' AS Description,\n",
    "            NULL AS ModifiedDate,\n",
    "            'True' AS DeletedFromModelFlag,\n",
    "            query_count.RunUuid,\n",
    "            query_count.ModelUuid,\n",
    "            query_count.AsOfDate,\n",
    "            query_count.AsOfDateTime\n",
    "        FROM {historical_table_names[\"object_query_count\"]} AS query_count\n",
    "        JOIN (\n",
    "            SELECT\n",
    "                ModelUuid,\n",
    "                ModelObject,\n",
    "                MAX(AsOfDateTime) AS MaxAsOfDateTime\n",
    "            FROM {historical_table_names[\"object_query_count\"]}\n",
    "            GROUP BY ALL\n",
    "        ) AS latest ON\n",
    "            latest.ModelObject = query_count.ModelObject\n",
    "            AND latest.ModelUuid = query_count.ModelUuid\n",
    "            AND latest.MaxAsOfDateTime = query_count.AsOfDateTime\n",
    "        LEFT JOIN {historical_table_names[\"object_mapping\"]} AS mapping ON\n",
    "            mapping.RunUuid = query_count.RunUuid\n",
    "            AND mapping.ModelUuid = query_count.ModelUuid\n",
    "            AND mapping.ObjectType = query_count.ObjectType\n",
    "            AND mapping.ModelObject = query_count.ModelObject\n",
    "        WHERE query_count.ObjectType = 'REPORT MEASURE'\n",
    "    ),\n",
    "\n",
    "    -- Get the latest model columns\n",
    "    latest_model_columns AS (\n",
    "        SELECT\n",
    "            mapping.TableName,\n",
    "            mapping.ObjectName,\n",
    "            mapping.ObjectType,\n",
    "            '' AS Expression,\n",
    "            model_column.Description,\n",
    "            CAST(ModifiedTime AS DATE) AS ModifiedDate,\n",
    "            CASE \n",
    "                WHEN MAX(model_column.AsOfDate) OVER() = model_column.AsOfDate THEN 'False'\n",
    "                ELSE 'True'\n",
    "            END AS DeletedFromModelFlag,\n",
    "            model_column.RunUuid,\n",
    "            model_column.ModelUuid,\n",
    "            model_column.AsOfDate,\n",
    "            model_column.AsOfDateTime\n",
    "        FROM {historical_table_names[\"model_columns\"]} AS model_column\n",
    "        JOIN (\n",
    "            SELECT\n",
    "                ModelUuid,\n",
    "                TableName,\n",
    "                ColumnName,\n",
    "                MAX(AsOfDateTime) AS MaxAsOfDateTime\n",
    "            FROM {historical_table_names[\"model_columns\"]}\n",
    "            GROUP BY ALL\n",
    "        ) AS latest ON\n",
    "            latest.ModelUuid = model_column.ModelUuid\n",
    "            AND latest.TableName = model_column.TableName\n",
    "            AND latest.ColumnName = model_column.ColumnName\n",
    "            AND latest.MaxAsOfDateTime = model_column.AsOfDateTime\n",
    "        LEFT JOIN {historical_table_names[\"object_mapping\"]} AS mapping ON\n",
    "            mapping.RunUuid = model_column.RunUuid\n",
    "            AND mapping.ModelUuid = model_column.ModelUuid\n",
    "            AND mapping.ObjectType = 'COLUMN'\n",
    "            AND mapping.TableName = model_column.TableName\n",
    "            AND mapping.ObjectName = model_column.ColumnName\n",
    "    ),\n",
    "\n",
    "    -- Get the latest model measures\n",
    "    latest_model_measures AS (\n",
    "        SELECT\n",
    "            mapping.TableName,\n",
    "            mapping.ObjectName,\n",
    "            mapping.ObjectType,\n",
    "            model_measure.MeasureExpression AS Expression,\n",
    "            model_measure.MeasureDescription AS Description,\n",
    "            NULL AS ModifiedDate,\n",
    "            CASE \n",
    "                WHEN MAX(model_measure.AsOfDate) OVER() = model_measure.AsOfDate THEN 'False'\n",
    "                ELSE 'True'\n",
    "            END AS DeletedFromModelFlag,\n",
    "            model_measure.RunUuid,\n",
    "            model_measure.ModelUuid,\n",
    "            model_measure.AsOfDate,\n",
    "            model_measure.AsOfDateTime\n",
    "        FROM {historical_table_names[\"model_measures\"]} AS model_measure\n",
    "        JOIN (\n",
    "            SELECT\n",
    "                ModelUuid,\n",
    "                TableName,\n",
    "                MeasureName,\n",
    "                MAX(AsOfDateTime) AS MaxAsOfDateTime\n",
    "            FROM {historical_table_names[\"model_measures\"]}\n",
    "            GROUP BY ALL\n",
    "        ) AS latest ON\n",
    "            latest.ModelUuid = model_measure.ModelUuid\n",
    "            AND latest.TableName = model_measure.TableName\n",
    "            AND latest.MeasureName = model_measure.MeasureName\n",
    "            AND latest.MaxAsOfDateTime = model_measure.AsOfDateTime\n",
    "        LEFT JOIN {historical_table_names[\"object_mapping\"]} AS mapping ON\n",
    "            mapping.ModelUuid = model_measure.ModelUuid\n",
    "            AND mapping.RunUuid = model_measure.RunUuid\n",
    "            AND mapping.ObjectType = 'MEASURE'\n",
    "            AND mapping.TableName = model_measure.TableName\n",
    "            AND mapping.ObjectName = model_measure.MeasureName\n",
    "    ),\n",
    "\n",
    "    -- Get unused columns\n",
    "    latest_unused_columns AS (\n",
    "        SELECT\n",
    "            unused_column.TableName,\n",
    "            IFNULL(model_column_with_mapping.ObjectName, unused_column.SourceColumnName) AS ObjectName,\n",
    "            'COLUMN' AS ObjectType,\n",
    "            '' AS Expression,\n",
    "            model_column_with_mapping.Description,\n",
    "            model_column_with_mapping.ModifiedDate,\n",
    "            'True' AS DeletedFromModelFlag,\n",
    "            unused_column.RunUuid,\n",
    "            unused_column.ModelUuid,\n",
    "            unused_column.AsOfDate,\n",
    "            unused_column.AsOfDateTime\n",
    "        FROM {historical_table_names[\"unused_columns\"]} AS unused_column\n",
    "        LEFT JOIN (\n",
    "            SELECT\n",
    "                ModelUuid,\n",
    "                SourceTableName,\n",
    "                SourceColumnName,\n",
    "                MAX(AsOfDateTime) AS MaxAsOfDateTime\n",
    "            FROM {historical_table_names[\"unused_columns\"]}\n",
    "            GROUP BY ALL\n",
    "        ) AS latest ON\n",
    "            latest.ModelUuid = unused_column.ModelUuid\n",
    "            AND latest.SourceTableName = unused_column.SourceTableName\n",
    "            AND latest.SourceColumnName = unused_column.SourceColumnName\n",
    "            AND latest.MaxAsOfDateTime = unused_column.AsOfDateTime\n",
    "        LEFT JOIN (\n",
    "            SELECT\n",
    "                latest_model_columns.ModelUuid,\n",
    "                latest_model_columns.TableName,\n",
    "                latest_model_columns.ObjectName,\n",
    "                latest_model_columns.Description,\n",
    "                latest_model_columns.ModifiedDate,\n",
    "                source_mapping.SourceTableName,\n",
    "                source_mapping.SourceColumnName,\n",
    "                source_mapping.RunUuid\n",
    "            FROM latest_model_columns\n",
    "            LEFT JOIN {historical_table_names[\"source_mapping\"]} AS source_mapping ON\n",
    "                source_mapping.ModelUuid = latest_model_columns.ModelUuid\n",
    "                AND source_mapping.RunUuid = latest_model_columns.RunUuid\n",
    "                AND source_mapping.TableName = latest_model_columns.TableName\n",
    "                AND source_mapping.ColumnName = latest_model_columns.ObjectName\n",
    "        ) AS model_column_with_mapping ON\n",
    "            model_column_with_mapping.ModelUuid = unused_column.ModelUuid\n",
    "            AND model_column_with_mapping.RunUuid = unused_column.RunUuid\n",
    "            AND model_column_with_mapping.SourceTableName = unused_column.SourceTableName\n",
    "            AND model_column_with_mapping.SourceColumnName = unused_column.SourceColumnName\n",
    "    ),\n",
    "\n",
    "    -- Union all objects\n",
    "    union_all_objects AS (\n",
    "        SELECT * FROM latest_report_measures\n",
    "        UNION\n",
    "        SELECT * FROM latest_model_columns\n",
    "        UNION\n",
    "        SELECT * FROM latest_model_measures\n",
    "        UNION\n",
    "        SELECT * FROM latest_unused_columns\n",
    "    ),\n",
    "\n",
    "    -- Enrich and keep the latest records\n",
    "    keep_latest_record_and_enrich AS (\n",
    "        SELECT\n",
    "            union_all_objects.TableName,\n",
    "            union_all_objects.ObjectName,\n",
    "            union_all_objects.ObjectType,\n",
    "            union_all_objects.Expression,\n",
    "            union_all_objects.Description,\n",
    "            union_all_objects.ModifiedDate,\n",
    "            union_all_objects.DeletedFromModelFlag,\n",
    "            union_all_objects.ModelUuid,\n",
    "            IFNULL(source_mapping.SourceTableName, 'N/A') AS SourceTableName,\n",
    "            IFNULL(source_mapping.SourceColumnName, 'N/A') AS SourceColumnName,\n",
    "            CASE \n",
    "                WHEN MAX(union_all_objects.AsOfDate) OVER() = union_all_objects.AsOfDate THEN 'False'\n",
    "                ELSE 'True'\n",
    "            END AS DeletedFromDatastoreFlag,\n",
    "            {\n",
    "                generate_table_key(\n",
    "                    \"union_all_objects.ModelUuid\",\n",
    "                    \"union_all_objects.TableName\",\n",
    "                    '''\n",
    "                        CASE \n",
    "                            WHEN union_all_objects.ObjectType = 'REPORT MEASURE'\n",
    "                            THEN union_all_objects.Expression\n",
    "                            ELSE union_all_objects.ObjectName\n",
    "                        END\n",
    "                    ''',\n",
    "                )\n",
    "            } AS ModelObjectId\n",
    "        FROM union_all_objects\n",
    "        JOIN (\n",
    "            SELECT\n",
    "                ModelUuid,\n",
    "                TableName,\n",
    "                ObjectName,\n",
    "                Expression,\n",
    "                MAX(AsOfDateTime) AS MaxAsOfDateTime\n",
    "            FROM union_all_objects\n",
    "            GROUP BY ALL\n",
    "        ) AS latest ON\n",
    "            latest.ModelUuid = union_all_objects.ModelUuid\n",
    "            AND latest.TableName = union_all_objects.TableName\n",
    "            AND latest.ObjectName = union_all_objects.ObjectName\n",
    "            AND latest.Expression = union_all_objects.Expression\n",
    "            AND latest.MaxAsOfDateTime = union_all_objects.AsOfDateTime\n",
    "        LEFT JOIN {historical_table_names[\"source_mapping\"]} AS source_mapping ON\n",
    "            source_mapping.ModelUuid = union_all_objects.ModelUuid\n",
    "            AND source_mapping.RunUuid = union_all_objects.RunUuid\n",
    "            AND source_mapping.TableName = union_all_objects.TableName\n",
    "            AND source_mapping.ColumnName = union_all_objects.ObjectName\n",
    "    )\n",
    "    SELECT * FROM keep_latest_record_and_enrich\n",
    "\"\"\")\n",
    "\n",
    "query_result.write.mode(\"overwrite\").format(\"delta\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(star_schema_table_names[\"dim_model_object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f59597-9975-4ba1-83ac-e9e5a541c3b1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create ```DIM_Model```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c06fa-6030-4ffd-885e-998ffc6dd0c3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "query_result = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        models.source_model_workspace_name AS WorkspaceName,\n",
    "        models.source_model_name AS ModelName,\n",
    "        models.ModelUuid AS ModelUuid,\n",
    "        models.source_datastore_name AS DatastoreName,\n",
    "        models.source_datastore_uuid AS DatastoreUuid,\n",
    "        models.source_datastore_workspace_name AS DatastoreWorkspaceName,\n",
    "        models.source_datastore_workspace_uuid AS DatastoreWorkspaceUuid\n",
    "    FROM {historical_table_names[\"run_history\"]} AS models\n",
    "    JOIN (\n",
    "        SELECT\n",
    "            ModelUuid,\n",
    "            MAX(AsOfDateTime) AS AsOfDateTime\n",
    "        FROM {historical_table_names[\"run_history\"]}\n",
    "        WHERE Status = 'completed'\n",
    "        GROUP BY ModelUuid\n",
    "    ) AS latest ON\n",
    "        latest.ModelUuid = models.ModelUuid\n",
    "        AND latest.AsOfDateTime = models.AsOfDateTime\n",
    "\"\"\")\n",
    "\n",
    "query_result.write.mode(\"overwrite\").format(\"delta\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(star_schema_table_names[\"dim_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d514d5-2a03-42f7-b489-bf683f642e64",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create ```DIM_Report```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c91c17-5dca-4a7c-ac12-521c05518854",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "query_result = spark.sql(f\"\"\"\n",
    "    WITH src_reports AS (\n",
    "        SELECT\n",
    "            sr.ReportId,\n",
    "            sr.ReportName,\n",
    "            sr.WebUrl,\n",
    "            sr.WorkspaceId AS WorkspaceUuid,\n",
    "            sr.WorkspaceName,\n",
    "            sr.AsOfDateTime\n",
    "        FROM {historical_table_names[\"source_reports\"]} AS sr\n",
    "        JOIN (\n",
    "            SELECT\n",
    "                ReportId,\n",
    "                MAX(AsOfDateTime) AS MaxAsOfDateTime\n",
    "            FROM {historical_table_names[\"source_reports\"]}\n",
    "            GROUP BY ReportId\n",
    "        ) AS latest_reports\n",
    "            ON sr.ReportId = latest_reports.ReportId\n",
    "            AND sr.AsOfDateTime = latest_reports.MaxAsOfDateTime\n",
    "    ),\n",
    "    final_reports AS (\n",
    "        SELECT DISTINCT\n",
    "            src.ReportId AS ReportUuid,\n",
    "            COALESCE(mapping_data.MappedReportName, src.ReportName) AS ReportName,\n",
    "            COALESCE(mapping_data.MappedWebUrl, src.WebUrl) AS WebUrl,\n",
    "            COALESCE(mapping_data.MappedWorkspaceUuid, src.WorkspaceUuid) AS WorkspaceUuid,\n",
    "            COALESCE(mapping_data.MappedWorkspaceName, src.WorkspaceName) AS WorkspaceName,\n",
    "            CASE WHEN mapping_data.AppId IS NOT NULL THEN TRUE ELSE FALSE END AS IsAppReport,\n",
    "            COALESCE(mapping_data.OriginalReportObjectId, src.ReportId) AS OriginalReportUuid\n",
    "        FROM src_reports AS src\n",
    "        LEFT JOIN (\n",
    "            SELECT DISTINCT\n",
    "                app.id AS AppId,\n",
    "                app.originalReportObjectId AS OriginalReportObjectId,\n",
    "                mapped.ReportName AS MappedReportName,\n",
    "                mapped.WebUrl AS MappedWebUrl,\n",
    "                mapped.WorkspaceUuid AS MappedWorkspaceUuid,\n",
    "                mapped.WorkspaceName AS MappedWorkspaceName\n",
    "            FROM {historical_table_names[\"source_app_reports\"]} AS app\n",
    "            LEFT JOIN src_reports AS mapped\n",
    "                ON app.originalReportObjectId = mapped.ReportId\n",
    "        ) AS mapping_data\n",
    "            ON src.ReportId = mapping_data.AppId\n",
    "    )\n",
    "    SELECT\n",
    "        ReportUuid,\n",
    "        CASE WHEN ReportUuid = '' THEN 'Non-Report' ELSE ReportName END AS ReportName,\n",
    "        WebUrl,\n",
    "        WorkspaceUuid,\n",
    "        WorkspaceName,\n",
    "        IsAppReport,\n",
    "        OriginalReportUuid\n",
    "    FROM final_reports\n",
    "\"\"\")\n",
    "\n",
    "query_result.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(star_schema_table_names[\"dim_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66fb43e-12e0-41f7-b654-7fd432c2b640",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create ```DIM_User```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5414e1c-1d6c-4c0e-b96e-ca2a039d01d0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "query_result = spark.sql(f\"\"\"\n",
    "    WITH union_data AS (\n",
    "        SELECT\n",
    "            'Masked' AS ExecutingUser,\n",
    "            ExecutingUserGroup\n",
    "        FROM {historical_table_names[\"object_query_count\"]}\n",
    "        UNION ALL\n",
    "        SELECT\n",
    "            ExecutingUser,\n",
    "            ExecutingUserGroup\n",
    "        FROM {historical_table_names[\"detailed_logs\"]}\n",
    "    ),\n",
    "    remove_duplicates_and_add_key AS (\n",
    "        SELECT DISTINCT\n",
    "            ExecutingUser,\n",
    "            ExecutingUserGroup,\n",
    "            {generate_table_key(\"ExecutingUser\", \"ExecutingUserGroup\")} AS UserId\n",
    "        FROM union_data\n",
    "    )\n",
    "    SELECT * FROM remove_duplicates_and_add_key\n",
    "\"\"\")\n",
    "\n",
    "query_result.write.mode(\"overwrite\").format(\"delta\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(star_schema_table_names[\"dim_user\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a708f15-2563-4e16-8e3d-78d0f70b3d05",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create ```FACT_ModelObjectQueryCount```\n",
    "Maps queries back to model objects, including dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a8846-e2ca-41ca-b2a5-151034563f01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "query_result = spark.sql(f\"\"\"\n",
    "    -- Map query counts with model objects\n",
    "    WITH query_counts_with_mapping AS (\n",
    "        SELECT\n",
    "            query_count.AsOfDate,\n",
    "            query_count.AsOfHour,\n",
    "            query_count.ExecutingUserGroup,\n",
    "            IFNULL(query_count.ReportId, 'N/A') AS ReportUuid,\n",
    "            query_count.QueryCount,\n",
    "            query_count.RunUuid,\n",
    "            query_count.ObjectType,\n",
    "            query_count.ModelUuid,\n",
    "            query_count.ModelObject,\n",
    "            mapping.TableName,\n",
    "            mapping.ObjectName,\n",
    "            {\n",
    "                generate_table_key(\n",
    "                    \"query_count.ModelUuid\",\n",
    "                    \"mapping.TableName\",\n",
    "                    '''\n",
    "                        CASE \n",
    "                            WHEN query_count.ObjectType = 'REPORT MEASURE'\n",
    "                            THEN query_count.ModelObject\n",
    "                            ELSE mapping.ObjectName\n",
    "                        END\n",
    "                    ''',\n",
    "                )\n",
    "            } AS ModelObjectId\n",
    "        FROM\n",
    "            {historical_table_names[\"object_query_count\"]} AS query_count\n",
    "        LEFT JOIN\n",
    "            {historical_table_names[\"object_mapping\"]} AS mapping ON\n",
    "                mapping.ModelUuid = query_count.ModelUuid\n",
    "                AND mapping.RunUuid = query_count.RunUuid\n",
    "                AND mapping.ModelObject = query_count.ModelObject\n",
    "    ),\n",
    "\n",
    "    -- Identify dependencies\n",
    "    dependencies AS (\n",
    "        SELECT DISTINCT\n",
    "            ModelUuid,\n",
    "            TableName,\n",
    "            ObjectName,\n",
    "            ReferencedTableName,\n",
    "            ReferencedObjectName,\n",
    "            RunUuid,\n",
    "            ObjectType\n",
    "        FROM {historical_table_names[\"dependencies\"]}\n",
    "    ),\n",
    "\n",
    "    -- Join dependencies with query counts\n",
    "    dependencies_join_query_count AS (\n",
    "        SELECT\n",
    "            dependencies.ModelUuid,\n",
    "            query_count.AsOfDate,\n",
    "            query_count.AsOfHour,\n",
    "            {\n",
    "                generate_table_key(\n",
    "                    \"dependencies.ModelUuid\",\n",
    "                    \"dependencies.ReferencedTableName\",\n",
    "                    \"dependencies.ReferencedObjectName\",\n",
    "                )\n",
    "            } AS ModelObjectId,\n",
    "            IFNULL(query_count.ReportUuid, 'N/A') AS ReportUuid,\n",
    "            query_count.ExecutingUserGroup,\n",
    "            query_count.QueryCount\n",
    "        FROM \n",
    "            dependencies\n",
    "        LEFT JOIN\n",
    "            query_counts_with_mapping AS query_count ON\n",
    "                query_count.ModelUuid = dependencies.ModelUuid\n",
    "                AND query_count.RunUuid = dependencies.RunUuid\n",
    "                AND query_count.TableName = dependencies.TableName\n",
    "                AND query_count.ObjectName = dependencies.ObjectName\n",
    "        WHERE\n",
    "            query_count.ObjectName IS NOT NULL\n",
    "    ),\n",
    "\n",
    "    -- Union data to create the final fact table\n",
    "    union_model_and_report_objects AS (\n",
    "        SELECT\n",
    "            ModelUuid,\n",
    "            AsOfDate,\n",
    "            AsOfHour,\n",
    "            ModelObjectId,\n",
    "            ReportUuid,\n",
    "            {generate_table_key(\"'Masked'\", \"ExecutingUserGroup\")} AS UserId,\n",
    "            'True' AS DirectReferenceFlag,\n",
    "            QueryCount\n",
    "        FROM\n",
    "            query_counts_with_mapping\n",
    "        UNION ALL\n",
    "        SELECT\n",
    "            ModelUuid,\n",
    "            AsOfDate,\n",
    "            AsOfHour,\n",
    "            ModelObjectId,\n",
    "            ReportUuid,\n",
    "            {generate_table_key(\"'Masked'\", \"ExecutingUserGroup\")} AS UserId,\n",
    "            'False' AS DirectReferenceFlag,\n",
    "            QueryCount\n",
    "        FROM\n",
    "            dependencies_join_query_count\n",
    "    )\n",
    "\n",
    "    -- Select all records from the final union\n",
    "    SELECT * FROM union_model_and_report_objects\n",
    "\"\"\")\n",
    "\n",
    "query_result.write.mode(\"overwrite\").format(\"delta\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(star_schema_table_names[\"fact_model_object_query_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ebb8fd-f1fc-4d09-aff0-ac310bcc4680",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create ```FACT_ModelLogs```\n",
    "Stores detailed DAX query logs for performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195be479-27a9-4a5a-b710-67933751c9b2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "query_result = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        AsOfDate,\n",
    "        OperationName,\n",
    "        OperationDetailName,\n",
    "        ReportId AS ReportUuid,\n",
    "        ModelUuid,\n",
    "        Timestamp AS DateTime,\n",
    "        {generate_table_key(\"ExecutingUser\", \"ExecutingUserGroup\")} AS UserId,\n",
    "        DurationMs,\n",
    "        CpuTimeMs,\n",
    "        EventText,\n",
    "        OperationId,\n",
    "        XmlaSessionId,\n",
    "        ActivityId,\n",
    "        RequestId,\n",
    "        CurrentActivityId,\n",
    "        StatusCode,\n",
    "        Status\n",
    "    FROM\n",
    "        {historical_table_names[\"detailed_logs\"]} AS query_count\n",
    "\"\"\")\n",
    "\n",
    "query_result.write.mode(\"overwrite\").format(\"delta\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(star_schema_table_names[\"fact_detailed_logs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3948f-66d0-4e88-94d1-edf76d27cbfd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create ```FACT_ModelObjectStatistics```\n",
    "Blends cold-cache data, table residency, and table sizes for columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd23920-dc4c-4a5b-8d47-7f914d5bce6d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "query_result = spark.sql(f\"\"\"\n",
    "    -- Create distinct combinations of object and date\n",
    "    WITH distinct_object_date_combo AS (\n",
    "        SELECT DISTINCT\n",
    "            AsOfDate,\n",
    "            ModelUuid,\n",
    "            TableName,\n",
    "            ColumnName AS ObjectName,\n",
    "            {generate_table_key(\"ModelUuid\", \"TableName\", \"ColumnName\")} AS ModelObjectId\n",
    "        FROM\n",
    "            {historical_table_names[\"model_columns\"]}\n",
    "    ),\n",
    "\n",
    "    -- Map cold cache with object mapping\n",
    "    cold_cache_with_mapping AS (\n",
    "        SELECT\n",
    "            mapping.TableName,\n",
    "            mapping.ObjectName,\n",
    "            cold_cache.ModelUuid,\n",
    "            cold_cache.AsOfDate,\n",
    "            cold_cache.Duration,\n",
    "            cold_cache.CpuTime\n",
    "        FROM\n",
    "            {historical_table_names[\"cold_cache_measurements\"]} AS cold_cache\n",
    "        LEFT JOIN\n",
    "            {historical_table_names[\"object_mapping\"]} AS mapping ON\n",
    "                mapping.RunUuid = cold_cache.RunUuid\n",
    "                AND mapping.ModelObject = cold_cache.ColumnName\n",
    "        WHERE \n",
    "            cold_cache.Success = 'Success'\n",
    "            AND cold_cache.ColumnName IS NOT NULL\n",
    "    ),\n",
    "\n",
    "    -- Join facts and aggregate metrics\n",
    "    join_facts AS (\n",
    "        SELECT\n",
    "            combos.ModelUuid,\n",
    "            combos.AsOfDate,\n",
    "            combos.ModelObjectId,\n",
    "            COUNT(residency.IsResident) AS ColumnResidencyMeasuredCount,\n",
    "            SUM(CASE WHEN residency.IsResident = True THEN 1 ELSE 0 END) AS ColumnResidencyTrueCount,\n",
    "            AVG(data_size.TotalSize) AS TotalSize,\n",
    "            AVG(data_size.DataSize) AS DataSize,\n",
    "            AVG(data_size.DictionarySize) AS DictionarySize,\n",
    "            AVG(data_size.HierarchySize) AS HierarchySize,\n",
    "            AVG(cold_cache.Duration) AS DurationTime,\n",
    "            AVG(cold_cache.CpuTime) AS CpuTime\n",
    "        FROM\n",
    "            distinct_object_date_combo AS combos\n",
    "        LEFT JOIN\n",
    "            {historical_table_names[\"model_columns\"]} AS residency ON\n",
    "                residency.ModelUuid = combos.ModelUuid\n",
    "                AND residency.TableName = combos.TableName\n",
    "                AND residency.ColumnName = combos.ObjectName\n",
    "                AND residency.AsOfDate = combos.AsOfDate\n",
    "        LEFT JOIN\n",
    "            {historical_table_names[\"resident_statistics\"]} AS data_size ON\n",
    "                data_size.ModelUuid = combos.ModelUuid\n",
    "                AND data_size.TableName = combos.TableName\n",
    "                AND data_size.ColumnName = combos.ObjectName\n",
    "                AND data_size.AsOfDate = combos.AsOfDate\n",
    "        LEFT JOIN\n",
    "            cold_cache_with_mapping AS cold_cache ON\n",
    "                cold_cache.ModelUuid = combos.ModelUuid\n",
    "                AND cold_cache.TableName = combos.TableName\n",
    "                AND cold_cache.ObjectName = combos.ObjectName\n",
    "                AND cold_cache.AsOfDate = combos.AsOfDate\n",
    "        GROUP BY ALL\n",
    "    )\n",
    "\n",
    "    -- Select all results from the final join\n",
    "    SELECT * FROM join_facts\n",
    "\"\"\")\n",
    "\n",
    "query_result.write.mode(\"overwrite\").format(\"delta\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(star_schema_table_names[\"fact_model_statistics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442ba88-d460-4344-a380-d8380a8a628d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "mssparkutils.session.stop()"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "environment": {},
   "lakehouse": {
    "default_lakehouse": "",
    "default_lakehouse_name": "",
    "default_lakehouse_workspace_id": ""
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
