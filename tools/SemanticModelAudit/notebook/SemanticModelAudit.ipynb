{"cells":[{"cell_type":"markdown","source":["## Fabric Semantic Model Audit\n","\n","### Overview\n","\n","This notebook is designed to perform a comprehensive audit of Fabric semantic models by collecting and tracking logs and metadata over time. It supports ongoing evaluation of model performance, usage patterns, and metadata changes, which can help you:\n","\n","- **Identify Unused or Obsolete Columns:** Determine which columns may be removed from the model or underlying Delta tables.\n","- **Monitor Performance:** Evaluate DAX query performance over time.\n","- **Track Model Usage:** Collect historical query logs and usage statistics.\n","\n","### Key Components and Functionality\n","\n","1. **Initial Setup and Requirements:**\n","   - **Workspace Monitoring:** The notebook requires that Workspace Monitoring is enabled. See [this blog post](https://blog.fabric.microsoft.com/blog/announcing-public-preview-of-workspace-monitoring) for guidance.\n","   - **Scheduled Execution:** To capture detailed usage statistics, it is recommended to schedule this notebook to run multiple times per day (e.g., 6 times per day).\n","   - **Configure Run Parameters:** Configure the run parameters at the top of the notebook based on your models and other requirements.\n","   - **Logging Lakehouse:** Attach a lakehouse so log tables can be saved. \n","\n","1. **Core Functionality:**\n","   - **Metadata Capture:** Functions to retrieve and save semantic model objects (columns and measures) along with dependencies using Fabric API calls.\n","   - **Query Log Collection:** Modules to capture query counts and detailed logs, which help track model usage and performance over specified time intervals.\n","   - **Unused Columns and Source Mapping:** Compares lakehouse metadata with model usage to detect columns that are no longer utilized.\n","   - **Cold Cache Performance:** Deploys a cloned version of the model to measure cold-cache performance via parallel DAX queries and trace log analysis.\n","   - **Resident Statistics:** Captures statistics about column residency (e.g., memory load, sizes) to further evaluate model performance.\n","\n","1. **Star Schema Generation:**\n","   - The notebook constructs several star schema tables for in-depth analysis:\n","      - **DIM_ModelObject:** Latest definitions for columns, measures, and unused columns.\n","      - **DIM_Model:** Basic model details.\n","      - **DIM_Report:** Report information with optional known ID mapping.\n","      - **DIM_User:** Standardized user info from logs.\n","      - **FACT_ModelObjectQueryCount:** Ties query counts to model objects and their dependencies.\n","      - **FACT_ModelLogs:** Detailed logs for performance tracking.\n","      - **FACT_ModelObjectStatistics:** Combines daily statistics such as cold cache performance and memory size for columns.\n","\n","1. **Orchestration and Execution:**\n","   - The main orchestration function (`collect_model_statistics`) processes each model sequentially, performing all capture steps (metadata, logs, unused columns, cold cache, resident statistics) and finally marking each run as completed or failed.\n","   - The notebook concludes by writing the star schema tables to Delta format, ready for import into a Fabric semantic model for further analysis.\n","\n","### Usage Notes\n","\n","- **Scheduling and Monitoring:** To capture granular historical data, consider scheduling this notebook to run at regular intervals throughout the day.\n","- **Configuration:** Adjust the parameters (e.g., `max_queries_daily`, `max_workers`) to suit your environment and workload.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"527f53b1-6a78-4d95-9e2c-a27a85aea446"},{"cell_type":"markdown","source":["### Install the Semantic Link Labs package\n","Check [here](https://pypi.org/project/semantic-link-labs/) to see the latest version."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"2c74d5d8-2ab6-4114-af39-6549df850d59"},{"cell_type":"code","source":["%pip install semantic-link-labs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4d6c6da9-1527-414a-9130-ee52a2f805ed"},{"cell_type":"markdown","source":["### Import Required Packages"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"9aad99cd-1e49-401b-970c-bf0c00edf2a5"},{"cell_type":"code","source":["# Standard Library Imports\n","import builtins\n","import functools\n","import math\n","import re\n","import threading\n","import time\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from contextlib import contextmanager\n","from datetime import datetime, timedelta\n","from uuid import uuid4\n","\n","import pandas as pd\n","import pyspark\n","\n","# Local Project-Specific Modules\n","import sempy.fabric as fabric\n","import sempy_labs as labs\n","\n","# Third-Party Libraries\n","from dateutil.relativedelta import relativedelta\n","from pyspark.sql.functions import col, lit, collect_set, udf\n","from pyspark.sql.types import StringType"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"cd5bfeed-3de0-4c21-931d-2ed258587773"},{"cell_type":"markdown","source":["### Set Initial Parameters"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"868df2b0-07c5-4bca-84a5-4452fc6d5781"},{"cell_type":"code","source":["# Models to collect statistics for\n","models = [\n","    {\n","        \"model_name\": \"Semantic Model Name\", # The name of the target model\n","        \"model_workspace_name\": \"Semantic Model Workspace Name\", # The name of the target model workspace\n","        \"lakehouse_name\": \"Semantic Model Lakehouse Name\", # Only needed for Direct Lake models\n","        \"lakehouse_workspace_name\": \"Semantic Model Lakehouse Workspace Name\", # Only needed for DirectLake models\n","        \"log_analytics_kusto_uri\": \"\",     # Optional: provide your own Kusto URI or leave as empty string (leaving blank will result in using the get_workspace_monitoring_info function)\n","        \"log_analytics_kusto_database_uuid\": \"\", # Optional: provide your own Kusto DB Uuid or leave as empty string (leaving blank will result in using the get_workspace_monitoring_info function)\n","    },\n","]\n","\n","# Settings for collecting cold cache performance measurements.\n","collect_cold_cache_measurements = True # Only recommended for Direct Lake or small Import models\n","max_queries_daily = 1                  # Maximum cold cache performance queries per column per day\n","max_workers = 50                       # Number of concurrent column queries\n","\n","# Determine target workspace for the cloned model (used in cold cache measurements)\n","cold_cache_target_workspace_name = fabric.resolve_workspace_name()\n","\n","# Settings for model query collection (how many days back to collect data)\n","max_days_ago_to_collect = 30  # Collect data from 1 to 30 days ago (only days with no data are collected)\n","\n","# Adjustment for recent logs: exclude intervals within this many hours of the as-of datetime for the most recent day\n","min_hours_before_current = 3\n","\n","# Principal name collection mode:\n","#   0 = Keep original ExecutingUser,\n","#   1 = Anonymize using historical mapping,\n","#   2 = Always set to \"Masked\"\n","collect_principal_names = 0\n","mask_principal_names_after_days = 30  # Set to 0 if masking is not required\n","\n","# Define user groups to bucket executing users found in the object count and detailed logs.\n","user_groups = {  \n","    \"Engineers (Example)\": [\n","        \"engineer1@microsoft.com\",\n","        \"engineer2@microsoft.com\",\n","    ],\n","    \"Project Managers (Example)\": [\n","        \"pm1@microsoft.com\",\n","        \"pm2@microsoft.com\",\n","    ],\n","}\n","default_user_group = \"Other Users\"\n","\n","# Define report mappings for cases where monitoring workspace assigns unexplained UUIDs\n","report_uuid_mapping = [\n","    {\"ReportUuid\": \"Unmappable uuid in workspace monitoring\", \"MapToReportUuid\": \"Main uuid for report to map to\"},\n","]\n","\n","# Replace with your target schema name or leave as an empty string for default behavior.\n","lakehouse_target_schema = \"dbo\"\n","\n","# Delta table names for historical data (new records are appended each run)\n","historical_table_names = {\n","    \"run_history\": \"run_history\",\n","    \"model_columns\": \"model_columns\",\n","    \"model_measures\": \"model_measures\",\n","    \"object_query_count\": \"model_object_query_count\",\n","    \"detailed_logs\": \"model_detailed_logs\",\n","    \"object_mapping\": \"model_object_mapping\",\n","    \"dependencies\": \"model_dependencies\",\n","    \"unused_columns\": \"unused_delta_table_columns\",\n","    \"source_mapping\": \"model_column_source_mapping\",\n","    \"cold_cache_measurements\": \"model_column_cold_cache_measurement\",\n","    \"resident_statistics\": \"model_column_resident_statistics\",\n","    \"source_reports\": \"source_reports\",\n","}\n","\n","# Delta table names for star schema (these tables are overwritten each run)\n","star_schema_table_names = {\n","    \"dim_model_object\": \"DIM_ModelObject\",\n","    \"dim_model\": \"DIM_Model\",\n","    \"dim_report\": \"DIM_Report\",\n","    \"dim_user\": \"DIM_User\",\n","    \"fact_model_object_query_count\": \"FACT_ModelObjectQueryCount\",\n","    \"fact_detailed_logs\": \"FACT_ModelLogs\",\n","    \"fact_model_statistics\": \"FACT_ModelObjectStatistics\",\n","}\n","\n","# Flags for table management\n","force_delete_historical_tables = False\n","force_delete_incomplete_runs = True\n","\n","# Abfss base path\n","abfss_base_path = \"onelake.dfs.fabric.microsoft.com\"\n","\n","# Ensure Spark uses case-sensitive SQL\n","spark.conf.set(\"spark.sql.caseSensitive\", True)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6d52b16f-b4c3-4bb3-a50c-d6153fb3d882"},{"cell_type":"markdown","source":["### Helper Functions: Logging, Retry, and Saving DataFrames"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"749dae3e-8429-435c-b927-b741cfe30592"},{"cell_type":"code","source":["# Thread-local storage to track call depth (used for logging indentation)\n","# This allows each thread to maintain its own \"call depth\" counter independently.\n","_thread_local = threading.local()\n","\n","\n","@contextmanager\n","def indented_print(indent_level: int):\n","    \"\"\"\n","    A context manager that temporarily replaces the built-in print function.\n","    It prepends a specific indent (based on indent_level) to every print output,\n","    which makes nested function calls easier to trace visually.\n","    \"\"\"\n","    # Save the original print function so it can be restored later.\n","    original_print = builtins.print\n","\n","    def custom_print(*args, **kwargs):\n","        # Create an indent by repeating four spaces per indent level.\n","        indent = \"    \" * indent_level\n","        # Call the original print with the indented message.\n","        original_print(indent + \" \".join(map(str, args)), **kwargs)\n","\n","    # Replace the built-in print with our custom_print.\n","    builtins.print = custom_print\n","    try:\n","        # Yield control back to the caller.\n","        yield\n","    finally:\n","        # Restore the original print function after exiting the block.\n","        builtins.print = original_print\n","\n","\n","def log_function_calls(func):\n","    \"\"\"\n","    Decorator that logs the start and end of a function call using indented printing.\n","    It uses a thread-local counter to indent log messages, so nested calls are visually offset.\n","    \n","    Example:\n","        @log_function_calls\n","        def my_func():\n","            ...\n","    \"\"\"\n","    @functools.wraps(func)\n","    def wrapper(*args, **kwargs):\n","        # Initialize call_depth for the thread if not already set.\n","        if not hasattr(_thread_local, \"call_depth\"):\n","            _thread_local.call_depth = 0\n","\n","        # Capture the current call depth to determine the indent.\n","        indent = _thread_local.call_depth\n","\n","        # Log the start message using the indented_print context manager.\n","        with indented_print(indent):\n","            print(f\"✅ {func.__name__} - Starting\")\n","\n","        # Increase the call depth as we enter the function.\n","        _thread_local.call_depth += 1\n","        try:\n","            # Log any output inside the function with increased indentation.\n","            with indented_print(_thread_local.call_depth):\n","                result = func(*args, **kwargs)\n","        finally:\n","            # Decrease the call depth on function exit.\n","            _thread_local.call_depth -= 1\n","            with indented_print(_thread_local.call_depth):\n","                print(f\"✅ {func.__name__} - Ending\")\n","        return result\n","\n","    return wrapper\n","\n","\n","def retry(exceptions, num_retries=3, initial_delay=5, backoff_factor=2, logger=None):\n","    \"\"\"\n","    Decorator factory that returns a decorator to automatically retry a function call if it raises\n","    one of the specified exceptions. It uses exponential backoff between retries.\n","    \n","    Parameters:\n","        exceptions (tuple or Exception): Exception(s) that trigger a retry.\n","        num_retries (int): Number of retry attempts before giving up.\n","        initial_delay (int): Initial delay in seconds before the first retry.\n","        backoff_factor (int): Factor by which the delay is multiplied after each retry.\n","        logger (callable, optional): Logger function for reporting retries (defaults to print).\n","    \n","    Usage:\n","        @retry((ValueError,), num_retries=3, initial_delay=2, backoff_factor=2)\n","        def my_func():\n","            ...\n","    \"\"\"\n","    def decorator_retry(func):\n","        @functools.wraps(func)\n","        def wrapper_retry(*args, **kwargs):\n","            attempts, delay = num_retries, initial_delay\n","            # Retry loop: try the function until attempts are exhausted.\n","            while attempts > 1:\n","                try:\n","                    return func(*args, **kwargs)\n","                except exceptions as e:\n","                    msg = f\"⚠️ {func.__name__} failed with {e}, retrying in {delay} seconds...\"\n","                    if logger:\n","                        logger(msg)\n","                    else:\n","                        print(msg)\n","                    # Pause execution for 'delay' seconds before retrying.\n","                    time.sleep(delay)\n","                    attempts -= 1\n","                    # Increase the delay for the next attempt.\n","                    delay *= backoff_factor\n","            # Final attempt: if previous retries failed, let any exception propagate.\n","            return func(*args, **kwargs)\n","        return wrapper_retry\n","    return decorator_retry\n","\n","\n","@log_function_calls\n","def save_dataframe_to_delta_table(data, table_name: str, context: dict, **extra_columns) -> None:\n","    \"\"\"\n","    Appends a pandas or Spark DataFrame to a Delta table with additional contextual columns.\n","    Extra columns provided as keyword arguments are added to the DataFrame before writing.\n","    \n","    Parameters:\n","        data (pandas.DataFrame or pyspark.sql.DataFrame): The input data.\n","        table_name (str): The target Delta table name.\n","        context (dict): A context dictionary that must contain keys:\n","            - 'as_of_datetime'\n","            - 'as_of_date'\n","            - 'run_uuid'\n","            - 'source_model_uuid'\n","        **extra_columns: Any additional columns to add to the DataFrame.\n","    \n","    The function converts a pandas DataFrame to a Spark DataFrame if necessary and ensures\n","    that column names have no spaces. It then writes the DataFrame to the specified Delta table.\n","    \"\"\"\n","    # Default columns added to every record from the context.\n","    default_cols = {\n","        \"AsOfDateTime\": context[\"as_of_datetime\"],\n","        \"AsOfDate\": context[\"as_of_date\"],\n","        \"RunUuid\": context[\"run_uuid\"],\n","        \"ModelUuid\": context[\"source_model_uuid\"],\n","    }\n","    # Merge default columns with any extra columns provided.\n","    all_extra_cols = {**default_cols, **extra_columns}\n","\n","    def add_columns(df, cols: dict):\n","        \"\"\"\n","        Helper function to add extra columns to a DataFrame.\n","        Works for both pandas and Spark DataFrames.\n","        \"\"\"\n","        for col_name, value in cols.items():\n","            if isinstance(df, pd.DataFrame):\n","                # Direct assignment for pandas DataFrame.\n","                df[col_name] = value\n","            else:\n","                # For Spark DataFrame, use the withColumn method and lit() to add constant columns.\n","                df = df.withColumn(col_name, lit(value))\n","        return df\n","\n","    if isinstance(data, pd.DataFrame):\n","        # For pandas DataFrame, remove spaces in column names for consistency.\n","        data.columns = data.columns.str.replace(\" \", \"\", regex=True)\n","        data = add_columns(data, all_extra_cols)\n","        # Convert the cleaned pandas DataFrame into a Spark DataFrame.\n","        spark_df = spark.createDataFrame(data)\n","    elif isinstance(data, pyspark.sql.DataFrame):\n","        # For Spark DataFrame, rename columns by removing any spaces.\n","        for c in data.columns:\n","            data = data.withColumnRenamed(c, c.replace(\" \", \"\"))\n","        spark_df = add_columns(data, all_extra_cols)\n","    else:\n","        # Raise error if data is not a recognized DataFrame type.\n","        raise TypeError(\"❌ Unsupported data type. Expected pandas or Spark DataFrame.\")\n","\n","    try:\n","        # Write the DataFrame to the specified Delta table.\n","        # The \"mergeSchema\" option allows the schema to evolve if needed.\n","        spark_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n","        print(f\"✅ Table `{table_name}` updated successfully.\")\n","    except Exception as e:\n","        print(f\"❌ Failed to save table `{table_name}`. Error: {e}\")\n","        raise\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ab6fee32-f67f-4c5c-9377-02beeb956858"},{"cell_type":"markdown","source":["### Run History & Cleanup Functions"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"f23bef74-dd30-42ef-a177-16c1963355bb"},{"cell_type":"code","source":["@log_function_calls\n","def record_run_start(context: dict) -> None:\n","    \"\"\"\n","    Records the start of a run by inserting a new record into the run_history table.\n","    \n","    It creates a DataFrame that includes the current start time, a placeholder for the end time,\n","    and a status of 'started'. Additional context data (except for keys that are handled elsewhere)\n","    is included to help identify the run.\n","    \"\"\"\n","    # Filter out keys that are specific to run identification and handled separately,\n","    # ensuring that the DataFrame only includes the general context information.\n","    context_filtered = {\n","        k: v for k, v in context.items() if k not in [\"run_uuid\", \"source_model_uuid\"]\n","    }\n","    # Construct a pandas DataFrame with a single row representing the run start.\n","    # Both StartTime and EndTime are set to the current timestamp; EndTime will be updated upon completion.\n","    run_start_df = pd.DataFrame(\n","        [\n","            {\n","                **context_filtered,\n","                \"StartTime\": datetime.now(),  # Capture the current time as the start time.\n","                \"EndTime\": datetime.now(),    # Placeholder for end time; to be updated later.\n","                \"Status\": \"started\",          # Set initial status as 'started'.\n","            }\n","        ]\n","    )\n","    # Write the run start DataFrame to the Delta table designated for run history.\n","    save_dataframe_to_delta_table(\n","        data=run_start_df,\n","        table_name=historical_table_names[\"run_history\"],\n","        context=context,\n","    )\n","    # Log a confirmation message including the run's unique identifier.\n","    print(f\"✅ Recorded run start for UUID: {context['run_uuid']}\")\n","\n","\n","@log_function_calls\n","def record_run_completion(context: dict, status: str) -> None:\n","    \"\"\"\n","    Updates the run_history table to mark the run as completed or failed.\n","    \n","    It sets the EndTime to the current timestamp and updates the run's Status.\n","    The function ensures that the run_uuid is present and safely escapes it for SQL usage.\n","    \"\"\"\n","    # Retrieve the unique run identifier from the context.\n","    run_uuid = context.get(\"run_uuid\")\n","    if not run_uuid:\n","        raise ValueError(\"❌ 'run_uuid' missing from context.\")\n","    # Escape single quotes in the run_uuid to prevent SQL injection or syntax issues.\n","    escaped_uuid = run_uuid.replace(\"'\", \"''\")\n","    # Format the current datetime as a string suitable for SQL TIMESTAMP.\n","    end_time_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    # Construct the SQL update query to set EndTime and Status for the given run.\n","    update_query = f\"\"\"\n","        UPDATE {historical_table_names[\"run_history\"]}\n","        SET EndTime = CAST('{end_time_str}' AS TIMESTAMP),\n","            Status = '{status}'\n","        WHERE RunUuid = '{escaped_uuid}'\n","    \"\"\"\n","    try:\n","        # Execute the SQL update query using Spark's SQL interface.\n","        spark.sql(update_query)\n","        # Log a success message indicating the run has been updated.\n","        print(f\"✅ Run UUID: {run_uuid} updated with status '{status}'.\")\n","    except Exception as e:\n","        # Log the error if the update fails and re-raise the exception.\n","        print(f\"❌ Failed to update run UUID: {run_uuid}. Error: {e}\")\n","        raise\n","\n","\n","@log_function_calls\n","def cleanup_incomplete_runs() -> None:\n","    \"\"\"\n","    Removes records associated with runs that did not complete successfully.\n","    \n","    The function performs two main operations:\n","      1. Deletes records from all historical tables (except run_history) corresponding to incomplete runs.\n","      2. Updates the run_history table to mark those incomplete runs as 'removed'.\n","      \n","    This cleanup helps maintain data consistency by removing or flagging partially recorded runs.\n","    \"\"\"\n","    # Check if the run_history table exists; if not, there is nothing to clean up.\n","    if not spark.catalog.tableExists(historical_table_names[\"run_history\"]):\n","        print(\"✅ run_history table does not exist yet. No cleanup necessary.\")\n","        return\n","\n","    try:\n","        # Retrieve all run_uuids from run_history where the Status is not 'completed' or 'removed'.\n","        incomplete_df = (\n","            spark.table(historical_table_names[\"run_history\"])\n","            .filter(~col(\"Status\").isin(\"completed\", \"removed\"))\n","            .select(\"RunUuid\")\n","        )\n","        # Collect the run uuids from the DataFrame to a list.\n","        incomplete_uuids = [row[\"RunUuid\"] for row in incomplete_df.collect()]\n","\n","        # If there are no incomplete runs, log the info and exit.\n","        if not incomplete_uuids:\n","            print(\"✅ No incomplete runs to clean.\")\n","            return\n","\n","        print(f\"✅ Found {len(incomplete_uuids)} incomplete run(s); proceeding with cleanup.\")\n","        # Escape each run_uuid for safe SQL query usage.\n","        escaped_uuids = [uuid.replace(\"'\", \"''\") for uuid in incomplete_uuids]\n","        # Create a comma-separated string of escaped run_uuids for use in SQL IN clause.\n","        uuid_list_str = \", \".join(f\"'{uuid}'\" for uuid in escaped_uuids)\n","\n","        # Iterate over each historical table (except run_history) to remove incomplete run records.\n","        for logical_name, table in historical_table_names.items():\n","            if logical_name == \"run_history\":\n","                continue  # Skip the run_history table in this deletion loop.\n","            try:\n","                # Only attempt deletion if the table exists.\n","                if not spark.catalog.tableExists(table):\n","                    print(f\"✅ Table {table} not found. Skipping deletion.\")\n","                    continue\n","                # Construct and execute the deletion query for the current table.\n","                delete_query = f\"DELETE FROM {table} WHERE RunUuid IN ({uuid_list_str})\"\n","                spark.sql(delete_query)\n","                print(f\"✅ Deleted records in table {table} for incomplete runs.\")\n","            except Exception as e:\n","                # Log the error for the current table and continue with the next one.\n","                print(f\"❌ Failed to clean table {table}. Error: {e}\")\n","                continue\n","\n","        # After cleaning other tables, update the run_history table to mark incomplete runs as 'removed'.\n","        update_query = f\"\"\"\n","            UPDATE {historical_table_names[\"run_history\"]}\n","            SET Status = 'removed'\n","            WHERE RunUuid IN ({uuid_list_str})\n","        \"\"\"\n","        spark.sql(update_query)\n","        print(f\"✅ Marked {len(incomplete_uuids)} incomplete run(s) as removed.\")\n","    except Exception as e:\n","        # Log and re-raise any exception encountered during the cleanup process.\n","        print(f\"❌ Cleanup failed. Error: {e}\")\n","        raise\n","\n","\n","def drop_historical_tables() -> None:\n","    \"\"\"\n","    Drops all historical Delta tables.\n","    \n","    Use this function with caution as it permanently deletes all historical audit data\n","    stored in the tables defined in the 'historical_table_names' mapping.\n","    \"\"\"\n","    # Loop through each historical table and attempt to drop it.\n","    for logical_name, table in historical_table_names.items():\n","        try:\n","            print(f\"🗑️ Dropping table: {table}\")\n","            # Execute the DROP TABLE command; IF EXISTS ensures no error is thrown if the table doesn't exist.\n","            spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n","            print(f\"✅ Dropped table: {table}\")\n","        except Exception as e:\n","            # Log any failure to drop a table.\n","            print(f\"❌ Failed to drop table `{table}`. Error: {e}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"30690f3b-be06-4ec8-8674-8664501c0b71"},{"cell_type":"markdown","source":["### Capturing Semantic Model Objects & Dependencies"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"a4866699-d2e4-4452-a14c-4c5a915dfeb0"},{"cell_type":"code","source":["@log_function_calls\n","def capture_semantic_model_objects(context: dict) -> tuple[pd.DataFrame, pd.DataFrame]:\n","    \"\"\"\n","    Retrieves columns and measures for the specified semantic model.\n","\n","    It uses Fabric API calls to capture:\n","      - Columns (with extended metadata) from the model.\n","      - Measures from the model.\n","    The captured data is saved to Delta tables for historical tracking.\n","\n","    Returns:\n","      A tuple (model_columns, model_measures) as pandas DataFrames.\n","    \"\"\"\n","    # Check that the context contains the required keys for API calls.\n","    for key in [\"source_model_uuid\", \"source_model_workspace_uuid\"]:\n","        if key not in context:\n","            raise KeyError(f\"❌ Missing context key: '{key}'\")\n","    try:\n","        # Refresh the Table Object Model (TOM) cache to ensure up-to-date metadata.\n","        fabric.refresh_tom_cache(context[\"source_model_workspace_uuid\"])\n","\n","        # Retrieve model columns with extended metadata via Sempy.\n","        model_columns = fabric.list_columns(\n","            dataset=context[\"source_model_uuid\"],\n","            extended=True,\n","            workspace=context[\"source_model_workspace_uuid\"],\n","        )\n","        # Clean column names by removing spaces for consistency.\n","        model_columns.columns = model_columns.columns.str.replace(\" \", \"\", regex=True)\n","        # Save the captured columns to the Delta table for historical tracking.\n","        save_dataframe_to_delta_table(\n","            data=model_columns,\n","            table_name=historical_table_names[\"model_columns\"],\n","            context=context,\n","        )\n","\n","        # Similarly, capture model measures from Sempy.\n","        model_measures = fabric.list_measures(\n","            dataset=context[\"source_model_uuid\"],\n","            workspace=context[\"source_model_workspace_uuid\"],\n","        )\n","        # Remove spaces from measure column names.\n","        model_measures.columns = model_measures.columns.str.replace(\" \", \"\", regex=True)\n","        # Save the captured measures to the corresponding Delta table.\n","        save_dataframe_to_delta_table(\n","            data=model_measures,\n","            table_name=historical_table_names[\"model_measures\"],\n","            context=context,\n","        )\n","    except Exception as e:\n","        print(\n","            f\"❌ Failed to capture model objects for model `{context['source_model_uuid']}`. Error: {e}\"\n","        )\n","        raise\n","    return model_columns, model_measures\n","\n","\n","def find_dependencies_recursive(\n","    dependencies: pd.DataFrame,\n","    root_table: str,\n","    root_object: str,\n","    ref_table: str,\n","    ref_object: str,\n","    level: int,\n","    accum: list,\n",") -> None:\n","    \"\"\"\n","    Recursively finds dependencies for a measure.\n","\n","    It traverses the dependency DataFrame (obtained from a DAX query) to:\n","      - Append each dependency with the current recursion level.\n","      - Recursively follow further dependencies if the referenced object is a measure.\n","\n","    Parameters:\n","      dependencies (pd.DataFrame): DataFrame containing dependency data.\n","      root_table (str): The table name of the original measure.\n","      root_object (str): The measure name for which dependencies are being traced.\n","      ref_table (str): The table name of the current referenced object.\n","      ref_object (str): The name of the current referenced object.\n","      level (int): The current recursion depth (starting at 1).\n","      accum (list): List used to accumulate dependency entries.\n","    \"\"\"\n","    # Filter the dependency DataFrame to only include rows matching the current reference.\n","    refs = dependencies[\n","        (dependencies[\"[TABLE]\"] == ref_table) &\n","        (dependencies[\"[OBJECT]\"] == ref_object)\n","    ]\n","    for _, row in refs.iterrows():\n","        # Append dependency details to the accumulator.\n","        accum.append(\n","            {\n","                \"ObjectType\": \"MEASURE\",  # All entries here are measures.\n","                \"TableName\": root_table,  # Original measure's table.\n","                \"ObjectName\": root_object,  # Original measure's name.\n","                \"ReferencedObjectType\": row[\"[REFERENCED_OBJECT_TYPE]\"],\n","                \"ReferencedTableName\": row[\"[REFERENCED_TABLE]\"],\n","                \"ReferencedObjectName\": row[\"[REFERENCED_OBJECT]\"],\n","                \"Level\": level,  # Record the current recursion level.\n","            }\n","        )\n","        # If the referenced object is itself a measure, continue recursion.\n","        if row[\"[REFERENCED_OBJECT_TYPE]\"] == \"MEASURE\":\n","            find_dependencies_recursive(\n","                dependencies,\n","                root_table,\n","                root_object,\n","                row[\"[REFERENCED_TABLE]\"],\n","                row[\"[REFERENCED_OBJECT]\"],\n","                level + 1,  # Increase recursion level.\n","                accum,\n","            )\n","\n","\n","@log_function_calls\n","def capture_semantic_model_dependencies(\n","    context: dict, model_measures: pd.DataFrame\n",") -> None:\n","    \"\"\"\n","    Captures dependencies for model measures.\n","\n","    It runs a DAX query to retrieve dependency data for measures,\n","    then recursively traces dependencies for each measure using the provided model_measures DataFrame.\n","    The complete dependency mapping is saved to the Delta table for dependencies.\n","    \"\"\"\n","    # Execute a DAX query to fetch dependency data for measures.\n","    model_measure_deps = fabric.evaluate_dax(\n","        dataset=context[\"source_model_uuid\"],\n","        workspace=context[\"source_model_workspace_uuid\"],\n","        dax_string=\"\"\"\n","            EVALUATE\n","            FILTER(\n","                INFO.CALCDEPENDENCY(\"OBJECT_TYPE\", \"MEASURE\"),\n","                [REFERENCED_OBJECT_TYPE] IN { \"COLUMN\", \"MEASURE\" }\n","            )\n","        \"\"\",\n","    )\n","    accum = []  # Initialize an empty list to accumulate dependency entries.\n","    # Iterate over each measure to start the recursive dependency search.\n","    for _, row in model_measures.iterrows():\n","        # Begin recursion for each measure using its own table and name as the root.\n","        find_dependencies_recursive(\n","            dependencies=model_measure_deps,\n","            root_table=row[\"TableName\"],\n","            root_object=row[\"MeasureName\"],\n","            ref_table=row[\"TableName\"],\n","            ref_object=row[\"MeasureName\"],\n","            level=1,  # Start at level 1.\n","            accum=accum,\n","        )\n","    # Convert the accumulated dependency records to a Spark DataFrame and save to Delta table.\n","    save_dataframe_to_delta_table(\n","        data=spark.createDataFrame(accum),\n","        table_name=historical_table_names[\"dependencies\"],\n","        context=context,\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3dc3ef49-801a-4761-ac06-a7015eee2f8f"},{"cell_type":"markdown","source":["### Processing Semantic Model Objects & Saving Mappings"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"b20a47ef-833f-4b9c-814c-2d2c14e099b4"},{"cell_type":"code","source":["@log_function_calls\n","def process_semantic_model_objects(\n","    model_objects: pd.DataFrame, object_type: str\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Standardizes the metadata for model objects (columns or measures) into a mapping DataFrame.\n","\n","    For columns, it produces multiple formatting variants (e.g., quoted, unquoted, bracketed)\n","    to facilitate later matching. For measures, only a single representation is generated.\n","\n","    Returns:\n","      A DataFrame with standardized object mapping information, including:\n","         - TableName: Original table name.\n","         - ObjectName: The base name (column or measure name).\n","         - ObjectType: 'COLUMN' or 'MEASURE'.\n","         - ModelObject: The variant string representation for matching.\n","    \"\"\"\n","\n","    def map_row(row: pd.Series) -> list:\n","        # Process based on whether the object is a column or a measure.\n","        if object_type == \"COLUMN\":\n","            tbl = row[\"TableName\"]\n","            col_name = row[\"ColumnName\"]\n","            # Generate different string variants for the column.\n","            quoted_variant = f\"'{tbl}'[{col_name}]\"   # e.g., 'TableName'[ColumnName]\n","            unquoted_variant = quoted_variant.replace(\"'\", \"\")  # Remove quotes, e.g., TableName[ColumnName]\n","            bracket_variant = f\"[{tbl}].[{col_name}]\"   # e.g., [TableName].[ColumnName]\n","            # If the table name includes spaces, avoid the unquoted variant.\n","            variants = (\n","                [quoted_variant, bracket_variant]\n","                if \" \" in tbl\n","                else [quoted_variant, unquoted_variant, bracket_variant]\n","            )\n","            obj_name = col_name\n","        else:\n","            # For measures, generate only one variant.\n","            measure = row[\"MeasureName\"]\n","            variants = [f\"[{measure}]\"]  # Format measure as [MeasureName]\n","            obj_name = measure\n","\n","        # Base dictionary holds common mapping fields.\n","        base = {\n","            \"TableName\": row[\"TableName\"],\n","            \"ObjectName\": obj_name,\n","            \"ObjectType\": object_type,\n","        }\n","        # Return a list of dictionaries, one for each variant.\n","        return [{**base, \"ModelObject\": variant} for variant in variants]\n","\n","    # For each row in the input DataFrame, apply map_row to produce a list of mapping dictionaries.\n","    mapped = [item for _, row in model_objects.iterrows() for item in map_row(row)]\n","    # Convert the list of mapping dictionaries into a pandas DataFrame.\n","    return pd.DataFrame(mapped)\n","\n","\n","@log_function_calls\n","def save_report_measure_mappings(distinct_objects: set, context: dict) -> None:\n","    \"\"\"\n","    Saves new mappings for REPORT MEASURE objects.\n","\n","    It parses each report measure string using regular expressions to extract:\n","      - The table name (from content within square brackets or single quotes).\n","      - The measure name (from a specific DAX pattern).\n","    These mappings are then saved to the object_mapping Delta table.\n","    \"\"\"\n","    rows = []\n","    # Regular expression to capture table names enclosed in either square brackets or single quotes.\n","    table_pattern = re.compile(r\"(?:\\[(?P<name_bracket>[^\\]]+)\\]|'(?P<name_quote>[^']+)')\")\n","    \n","    # Regular expression to capture the measure name from the expression pattern.\n","    measure_pattern = re.compile(r\"\\[([^\\]]+)\\]\\s*=\\s*\\(\\/\\* USER DAX BEGIN \\*\\/\")\n","\n","    # Iterate over each report measure string in the distinct_objects set.\n","    for model_object in distinct_objects:\n","        tbl_match = table_pattern.search(model_object)\n","        measure_match = measure_pattern.search(model_object)\n","        # Extract the table name from the regex groups if a match is found; otherwise, set as None.\n","        if tbl_match:\n","            table_name = tbl_match.group(\"name_bracket\") or tbl_match.group(\"name_quote\")\n","        else:\n","            table_name = None\n","\n","        # Append a mapping dictionary with the extracted values.\n","        rows.append(\n","            {\n","                \"TableName\": table_name,\n","                \"ObjectName\": measure_match.group(1) if measure_match else None,\n","                \"ModelObject\": model_object,\n","                \"ObjectType\": \"REPORT MEASURE\",\n","            }\n","        )\n","    # If mappings were found, convert them to a DataFrame and save to the Delta table.\n","    if rows:\n","        df_report = pd.DataFrame(rows)\n","        save_dataframe_to_delta_table(\n","            data=df_report,\n","            table_name=historical_table_names[\"object_mapping\"],\n","            context=context,\n","        )\n","        print(f\"✅ Saved {len(rows)} REPORT MEASURE mappings.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b0dffae9-b257-4d94-bbac-dad9d7913302"},{"cell_type":"markdown","source":["### Define Query Log Collector Class"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"b992608d-fb74-4acd-a519-75c02f04a275"},{"cell_type":"code","source":["class QueryLogCollector:\n","    def __init__(self, context: dict):\n","        # Save the context dictionary, which contains configuration parameters needed for queries.\n","        self.context = context\n","\n","    @staticmethod\n","    def find_starting_index(block_hours: float) -> int:\n","        \"\"\"\n","        Determines the starting index for allowed interval granularity based on the block's length in hours.\n","        \n","        It compares the given block_hours against a list of allowed intervals (24 down to 1 hour).\n","        Returns the index in ALLOWED_INTERVALS for which block_hours is greater than or equal.\n","        \"\"\"\n","        ALLOWED_INTERVALS = list(range(24, 0, -1))  # Allowed granularity from 24h down to 1h.\n","        for i, hrs in enumerate(ALLOWED_INTERVALS):\n","            if block_hours >= hrs:\n","                return i\n","        # If no allowed interval is less than or equal, return the index corresponding to the smallest interval.\n","        return len(ALLOWED_INTERVALS) - 1\n","\n","    @staticmethod\n","    def group_missing_hours(missing_hours: list) -> list:\n","        \"\"\"\n","        Groups a list of missing hour integers into continuous intervals.\n","        \n","        For example, if missing_hours = [2,3,4,6,7], it returns [(2,5), (6,8)],\n","        where each tuple represents a half-open interval [start, end).\n","        \"\"\"\n","        if not missing_hours:\n","            return []\n","        intervals = []\n","        start = missing_hours[0]\n","        end = missing_hours[0]\n","        # Iterate over the missing hours and group consecutive hours together.\n","        for h in missing_hours[1:]:\n","            if h == end + 1:\n","                end = h  # Extend the current interval.\n","            else:\n","                # Append the current interval as (start, end+1) and start a new interval.\n","                intervals.append((start, end + 1))\n","                start = h\n","                end = h\n","        # Append the last interval.\n","        intervals.append((start, end + 1))\n","        return intervals\n","\n","    @staticmethod\n","    def get_missing_hours_for_day(log_table, day_date) -> list:\n","        \"\"\"\n","        Determines which hours (0-23) have missing data for a given day by comparing against the log table.\n","        \n","        Parameters:\n","          log_table: A Spark DataFrame containing log records.\n","          day_date: A date object representing the day to check.\n","        \n","        Returns:\n","          A list of hour integers for which no log data is found.\n","        \"\"\"\n","        existing = set()\n","        if log_table is not None:\n","            try:\n","                # Filter the log table for the given day, select distinct 'AsOfHour', and collect them into a set.\n","                existing = {row[\"AsOfHour\"] for row in log_table.filter(col(\"AsOfDate\") == day_date)\n","                            .select(\"AsOfHour\").distinct().collect()}\n","            except Exception as e:\n","                print(f\"⚠️ Error retrieving existing hours for {day_date}: {e}\")\n","        # Return hours from 0 to 23 that are not present in the existing set.\n","        return [hour for hour in range(24) if hour not in existing]\n","\n","    def build_user_groups(self) -> tuple:\n","        \"\"\"\n","        Constructs dynamic arrays and case conditions for user groups for use in KQL queries.\n","        \n","        For each user group, it creates:\n","          - A 'let' statement that defines a dynamic array of user emails.\n","          - A case condition to map an executing user to the group name.\n","        \n","        Returns:\n","          A tuple with two strings: one for the combined let statements and one for the combined case conditions.\n","        \"\"\"\n","        let_statements = []\n","        case_conditions = []\n","        for group, emails in user_groups.items():\n","            # Normalize the group name to be used as a variable (remove spaces and dashes, and lower-case).\n","            grp_var = group.replace(\" \", \"\").replace(\"-\", \"\").lower()\n","            # Construct a comma-separated string of email addresses, each enclosed in single quotes.\n","            emails_str = \", \".join(\"'{0}'\".format(email) for email in emails)\n","            let_statements.append(\"let {0} = dynamic([{1}]);\".format(grp_var, emails_str))\n","            # Build the case condition that assigns the group name if the ExecutingUser is in the group.\n","            case_conditions.append('ExecutingUser in ({0}), \"{1}\"'.format(grp_var, group))\n","        # Join all let statements and case conditions into single strings.\n","        return \"\\n\".join(let_statements), \",\\n\".join(case_conditions)\n","\n","    def generate_object_count_query(self, object_type: str, model_objects_df: pd.DataFrame,\n","                                    start_ts: datetime, end_ts: datetime,\n","                                    include_summary: bool = False) -> str:\n","        \"\"\"\n","        Generates a KQL query to count queries for the given object type.\n","        \n","        For \"REPORT MEASURE\" objects, a specific extraction is applied to retrieve the measure.\n","        For other object types, it builds a dynamic array of model objects to check for matches in log text.\n","        \n","        Parameters:\n","          object_type (str): The type of object (\"REPORT MEASURE\", \"COLUMN\", \"MEASURE\", etc.).\n","          model_objects_df (pd.DataFrame): DataFrame containing standardized model objects.\n","          start_ts (datetime): Start timestamp for filtering logs.\n","          end_ts (datetime): End timestamp for filtering logs.\n","          include_summary (bool): If True, adds a summarization clause to the query.\n","        \n","        Returns:\n","          A string representing the constructed KQL query.\n","        \"\"\"\n","        let_statements, case_conditions = self.build_user_groups()\n","        if object_type == \"REPORT MEASURE\":\n","            # For report measures, extract the measure from EventText using a regex pattern.\n","            model_extend = \"\"\"\n","            | extend ModelObject = extract_all(@\"MEASURE (.*?\\\\/\\\\* USER DAX END \\\\*\\\\/\\\\))\", EventText)\n","            | mv-expand ModelObject\n","            | where isnotempty(ModelObject)\n","            \"\"\"\n","        else:\n","            if model_objects_df is not None:\n","                # Build a dynamic array of model objects to match against the EventText.\n","                objs_list = \",\\n\".join('\"{}\"'.format(obj) for obj in model_objects_df[\"ModelObject\"])\n","                let_statements += \"\\nlet modelObjects = dynamic([{0}]);\".format(objs_list)\n","            # Use mv-apply to check for presence of any model object in the EventText.\n","            model_extend = \"\"\"\n","            | mv-apply ModelObject = modelObjects to typeof(string) on (\n","                extend Matched = iff(EventText contains_cs ModelObject, true, false)\n","                | where Matched\n","                | project-away Matched\n","            )\n","            \"\"\"\n","        # Format the start and end timestamps in the required datetime format for KQL.\n","        start_str = start_ts.strftime(\"datetime(%Y-%m-%dT%H:%M:%S)\")\n","        end_str = end_ts.strftime(\"datetime(%Y-%m-%dT%H:%M:%S)\")\n","        # Construct the full query string using f-string formatting.\n","        query = f'''\n","        let model_name = \"{self.context[\"source_model_name\"]}\";\n","        {let_statements}\n","        SemanticModelLogs\n","            | where Timestamp between ({start_str} .. {end_str})\n","            | where ItemName == model_name\n","            | where OperationName == \"QueryBegin\"\n","            | extend ReportId = extract_json(\"$.Sources[0].ReportId\", tostring(parse_xml(XmlaProperties)[\"PropertyList\"][\"ApplicationContext\"]), typeof(string))\n","            | extend AsOfHour = datetime_part(\"hour\", Timestamp)\n","            | project AsOfHour, ExecutingUser, EventText, ReportId\n","            {model_extend}\n","            | extend ExecutingUserGroup =\n","                case(\n","                    {case_conditions},\n","                    \"{default_user_group}\"\n","                )\n","            | summarize QueryCount = count() by tostring(ModelObject), AsOfHour, ExecutingUserGroup, ReportId\n","        '''.replace(\"\\n\", \" \").strip()\n","        if include_summary:\n","            query += \" | summarize totalCount = count()\"\n","        return query\n","\n","    def generate_detailed_query(self, start_ts: datetime, end_ts: datetime,\n","                                include_summary: bool = False) -> str:\n","        \"\"\"\n","        Generates a KQL query to capture detailed DAX query metrics.\n","\n","        It builds separate subqueries to retrieve query begin and query end events,\n","        then joins them to combine detailed performance metrics and user information.\n","\n","        Parameters:\n","          start_ts (datetime): Start timestamp for the query window.\n","          end_ts (datetime): End timestamp for the query window.\n","          include_summary (bool): If True, includes a final summarization clause.\n","        \n","        Returns:\n","          A string representing the complete KQL query for detailed logs.\n","        \"\"\"\n","        let_statements, case_conditions = self.build_user_groups()\n","        start_iso = start_ts.isoformat(timespec=\"seconds\")\n","        end_iso = end_ts.isoformat(timespec=\"seconds\")\n","        query_begin_start_iso = (start_ts - timedelta(days=1)).isoformat(timespec=\"seconds\")\n","        query = f'''\n","        let model_name = \"{self.context[\"source_model_name\"]}\";\n","        {let_statements}\n","        let base_data =\n","            SemanticModelLogs\n","            | where ItemName == model_name;\n","        let query_end = base_data\n","            | where Timestamp between (datetime({start_iso}) .. datetime({end_iso}))\n","            | where OperationName in (\"Error\", \"QueryEnd\")\n","            | project Timestamp, OperationName, OperationDetailName, OperationId, XmlaSessionId, ExecutingUser, DurationMs, CpuTimeMs, EventText, Status, StatusCode;\n","        let query_begin = base_data\n","            | where Timestamp between (datetime({query_begin_start_iso}) .. datetime({end_iso}))\n","            | where OperationName == \"QueryBegin\"\n","            | extend ActivityId = tostring(parse_xml(XmlaProperties)[\"PropertyList\"][\"DbpropMsmdActivityID\"])\n","            | extend RequestId = tostring(parse_xml(XmlaProperties)[\"PropertyList\"][\"DbpropMsmdRequestID\"])\n","            | extend CurrentActivityId = tostring(parse_xml(XmlaProperties)[\"PropertyList\"][\"DbpropMsmdCurrentActivityID\"])\n","            | extend ReportId = extract_json(\"$.Sources[0].ReportId\", tostring(parse_xml(XmlaProperties)[\"PropertyList\"][\"ApplicationContext\"]), typeof(string))\n","            | distinct ActivityId, RequestId, CurrentActivityId, ReportId, OperationId, XmlaSessionId;\n","        query_end\n","        | join kind=leftouter (query_begin) on OperationId, XmlaSessionId\n","        | extend ExecutingUserGroup =\n","            case(\n","                    {case_conditions},\n","                    \"{default_user_group}\"\n","                )\n","        | extend AsOfHour = datetime_part(\"hour\", Timestamp)\n","        | project Timestamp, AsOfHour, OperationName, OperationDetailName, ReportId, ExecutingUser, ExecutingUserGroup, DurationMs, CpuTimeMs, EventText, OperationId, XmlaSessionId, ActivityId, RequestId, CurrentActivityId, Status, StatusCode\n","        '''.replace(\"\\n\", \"\").strip()\n","        if include_summary:\n","            query += \" | summarize totalCount = count()\"\n","        return query\n","\n","    @staticmethod\n","    @log_function_calls\n","    @retry(exceptions=(Exception,), num_retries=2, initial_delay=30, backoff_factor=2, logger=print)\n","    def execute_query(context: dict, kql_query: str) -> pyspark.sql.DataFrame:\n","        \"\"\"\n","        Executes a KQL query against the Log Analytics Kusto database.\n","\n","        It obtains an access token via mssparkutils, sets up the Kusto Spark connector with the proper options,\n","        and returns the result as a Spark DataFrame. The retry decorator ensures that transient failures are retried.\n","        \"\"\"\n","        try:\n","            # Retrieve an access token required for Kusto authentication.\n","            access_token = mssparkutils.credentials.getToken(context[\"log_analytics_kusto_uri\"])\n","            # Configure and run the Kusto query via the Spark data source.\n","            result = (\n","                spark.read.format(\"com.microsoft.kusto.spark.synapse.datasource\")\n","                .option(\"accessToken\", access_token)\n","                .option(\"kustoCluster\", context[\"log_analytics_kusto_uri\"])\n","                .option(\"kustoDatabase\", context[\"log_analytics_kusto_database\"])\n","                .option(\"kustoQuery\", kql_query)\n","                .load()\n","            )\n","            print(\"✅ KQL query executed successfully.\")\n","            return result\n","        except Exception:\n","            print(\"❌ Failed to execute KQL query.\")\n","            raise\n","\n","    def attempt_interval_query(self, start_ts: datetime, end_ts: datetime, current_idx: int,\n","                               generate_test_query, generate_main_query, process_result) -> bool:\n","        \"\"\"\n","        Attempts to run a query for the specified interval and, if necessary,\n","        subdivides the interval until complete data is returned.\n","\n","        This method works by:\n","          1. Running a test query to verify the expected row count.\n","          2. If no data is found, it skips the interval.\n","          3. If data is found, it runs the main query and checks if the results match.\n","          4. On discrepancies, it subdivides the interval into smaller chunks recursively.\n","\n","        Parameters:\n","          start_ts (datetime): Start timestamp for the query interval.\n","          end_ts (datetime): End timestamp for the query interval.\n","          current_idx (int): Current index into ALLOWED_INTERVALS, representing the current granularity.\n","          generate_test_query (callable): Function that generates a test query for the given interval.\n","          generate_main_query (callable): Function that generates the main query for the interval.\n","          process_result (callable): Callback function to process the main query result.\n","        \n","        Returns:\n","          bool: True if the interval or its sub-intervals were processed successfully, False otherwise.\n","        \"\"\"\n","        try:\n","            print(f\"ℹ️ Sending test query for interval {start_ts} to {end_ts}...\")\n","            # Generate and execute the test query.\n","            test_query = generate_test_query(start_ts, end_ts)\n","            test_result = self.execute_query(self.context, test_query)\n","            test_row = test_result.first()\n","            test_count = 0 if test_row is None else test_row[\"totalCount\"]\n","\n","            # If the test query returns 0 rows, skip processing this interval.\n","            if test_count == 0:\n","                print(f\"⚠️ Test query returned 0 rows for interval {start_ts} to {end_ts}. Skipping...\")\n","                print(f\"✅ Completed empty interval {start_ts} to {end_ts}.\")\n","                return True\n","\n","            print(f\"ℹ️ Sending main query for interval {start_ts} to {end_ts}...\")\n","            # Generate and execute the main query.\n","            main_query = generate_main_query(start_ts, end_ts)\n","            main_result = self.execute_query(self.context, main_query)\n","            main_count = main_result.count()\n","\n","            # Validate that the main query returns the same row count as the test query.\n","            if main_count != test_count:\n","                raise Exception(f\"Query result truncated: main_count ({main_count}) != test_count ({test_count})\")\n","            else:\n","                print(f\"✅ Query results match: {main_count} rows.\")\n","            # Process the query result via the provided callback.\n","            process_result(main_result, start_ts)\n","            return True\n","        except Exception as e:\n","            # If an exception occurs, attempt to subdivide the interval.\n","            ALLOWED_INTERVALS = list(range(24, 0, -1))\n","            print(f\"❌ Interval {start_ts} to {end_ts} at granularity {ALLOWED_INTERVALS[current_idx]}h failed. Error: {e}\")\n","            if current_idx == len(ALLOWED_INTERVALS) - 1:\n","                # If the smallest granularity is reached, log and skip the interval.\n","                print(f\"❌ Minimum granularity reached for {start_ts} to {end_ts}; skipping interval.\")\n","                return False\n","            else:\n","                new_idx = current_idx + 1\n","                new_granularity = ALLOWED_INTERVALS[new_idx]\n","                print(f\"⚠️ Retrying with smaller interval ({new_granularity}h) for {start_ts} to {end_ts}...\")\n","                success = True\n","                current = start_ts\n","                # Divide the interval into sub-intervals based on the new granularity.\n","                while current < end_ts:\n","                    sub_end = current + pd.Timedelta(hours=new_granularity)\n","                    if sub_end > end_ts:\n","                        sub_end = end_ts\n","                    sub_length = (sub_end - current).total_seconds() / 3600.0\n","                    sub_idx = self.find_starting_index(sub_length)\n","                    print(f\"🔍 Querying sub-interval {current} to {sub_end}...\")\n","                    # Recursively attempt to process each sub-interval.\n","                    if not self.attempt_interval_query(current, sub_end, sub_idx,\n","                                                       generate_test_query, generate_main_query, process_result):\n","                        success = False\n","                    current = sub_end\n","                return success"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"99024f82-68b9-426d-9192-ff1c27f67a2f"},{"cell_type":"markdown","source":["### Capture Query Counts & Detailed Logs"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"a0195830-ebc8-499c-bd64-c35549ede2ea"},{"cell_type":"code","source":["def process_intervals_for_day_generic(context: dict, log_tbl, qcollector: QueryLogCollector, days_ago: int, apply_cutoff: bool, generate_test_query, generate_main_query, process_result, label: str) -> None:\n","    \"\"\"\n","    Generic helper to process missing intervals for a given day.\n","    \n","    Parameters:\n","      context - the run context containing current datetime and other metadata.\n","      log_tbl - the Spark table containing previously collected log data.\n","      qcollector - an instance of QueryLogCollector used to generate and execute queries.\n","      days_ago - number of days ago for which data is being processed.\n","      apply_cutoff - if True, applies a cutoff to ignore recent hours.\n","      generate_test_query - lambda function to generate a test query for a given interval.\n","      generate_main_query - lambda function to generate the main query for the interval.\n","      process_result - callback function to process query results.\n","      label - a string label for logging (e.g., \"query counts\" or \"detailed logs\").\n","    \"\"\"\n","    # Set the start of the day for the target day by resetting time and subtracting days_ago.\n","    day_start = context[\"as_of_datetime\"].replace(hour=0, minute=0, second=0, microsecond=0) - relativedelta(days=days_ago)\n","    # Get the date portion to use for filtering.\n","    day_date = day_start.date()\n","    # Identify which hours in the day are missing from the log table.\n","    missing_hours = QueryLogCollector.get_missing_hours_for_day(log_tbl, day_date)\n","    if apply_cutoff:\n","        # Calculate the cutoff hour by subtracting a minimum number of hours (min_hours_before_current)\n","        # from the current context time relative to the start of the day.\n","        cutoff_hour = int(((context[\"as_of_datetime\"] - timedelta(hours=min_hours_before_current) - day_start).total_seconds()) // 3600)\n","        # Only consider missing hours that are before the cutoff.\n","        missing_hours = [h for h in missing_hours if h < cutoff_hour]\n","        if not missing_hours:\n","            print(f\"✅ All eligible {label} collected for {day_date} after applying min_hours_before_current filter.\")\n","            return\n","    # If there are no missing hours at all, log that the day is fully collected.\n","    if not missing_hours:\n","        print(f\"✅ All {label} collected for {day_date}.\")\n","        return\n","    # Group the missing hours into continuous intervals.\n","    intervals = QueryLogCollector.group_missing_hours(missing_hours)\n","    for start_hr, end_hr in intervals:\n","        # Calculate the actual start and end datetime for the current interval.\n","        interval_start = day_start + pd.Timedelta(hours=start_hr)\n","        interval_end = day_start + pd.Timedelta(hours=end_hr)\n","        # Compute the length of this interval in hours.\n","        block_length = (interval_end - interval_start).total_seconds() / 3600.0\n","        # Determine the starting index for allowed intervals based on the block length.\n","        idx = qcollector.find_starting_index(block_length)\n","        print(f\"🔍 Collecting {label} for {day_date} from hour {start_hr} to {end_hr} (block length: {block_length}h)...\")\n","        # Attempt to query for this interval; this may subdivide further if needed.\n","        qcollector.attempt_interval_query(interval_start, interval_end, idx, generate_test_query, generate_main_query, process_result)\n","        \n","\n","@log_function_calls\n","def capture_query_counts_by_object(context: dict, object_type: str, model_objects_df) -> None:\n","    \"\"\"\n","    Captures query count data for a given object type.\n","    \n","    It retrieves prior log data (if available) from the object_query_count table,\n","    then creates a QueryLogCollector to build KQL queries for the given object type.\n","    The results of the queries are processed and saved to the historical Delta table.\n","    For REPORT MEASURE objects, it collects distinct model object strings for mapping.\n","    \"\"\"\n","    distinct_report_measures = set()\n","    try:\n","        # Attempt to access prior log data for the given model and object type.\n","        log_tbl = spark.table(historical_table_names[\"object_query_count\"]).filter(\n","            (col(\"ModelUuid\") == context[\"source_model_uuid\"]) & (col(\"ObjectType\") == object_type)\n","        )\n","    except Exception:\n","        log_tbl = None\n","        print(\"⚠️ Could not access object_query_count table; proceeding without prior data.\")\n","    \n","    # Instantiate the QueryLogCollector with the current context.\n","    qcollector = QueryLogCollector(context)\n","    \n","    # Build lambda functions to generate the test and main queries.\n","    generate_test_query = lambda s, e: qcollector.generate_object_count_query(object_type, model_objects_df, s, e, include_summary=True)\n","    generate_main_query = lambda s, e: qcollector.generate_object_count_query(object_type, model_objects_df, s, e, include_summary=False)\n","    \n","    def process_interval_result(main_result, start_ts):\n","        # Save the result of the query to the object_query_count Delta table.\n","        save_dataframe_to_delta_table(\n","            data=main_result,\n","            table_name=historical_table_names[\"object_query_count\"],\n","            context=context,\n","            AsOfDate=start_ts.date(),\n","            AsOfDateTime=start_ts,\n","            ObjectType=object_type,\n","        )\n","        # For report measures, collect distinct model object strings for mapping.\n","        if object_type == \"REPORT MEASURE\":\n","            objs = main_result.select(\"ModelObject\").distinct().collect()\n","            for row in objs:\n","                if row[\"ModelObject\"]:\n","                    distinct_report_measures.add(row[\"ModelObject\"])\n","    \n","    # Process intervals for each day from 1 to max_days_ago_to_collect.\n","    for days_ago in range(1, max_days_ago_to_collect + 1):\n","        process_intervals_for_day_generic(context, log_tbl, qcollector, days_ago,\n","                                          apply_cutoff=(days_ago == 1),\n","                                          generate_test_query=generate_test_query,\n","                                          generate_main_query=generate_main_query,\n","                                          process_result=process_interval_result,\n","                                          label=f\"{object_type} query counts\")\n","    \n","    # If processing REPORT MEASURE objects, save new mappings if discovered.\n","    if object_type == \"REPORT MEASURE\":\n","        if distinct_report_measures:\n","            print(f\"📝 Saving {len(distinct_report_measures)} new REPORT MEASURE mappings...\")\n","            try:\n","                save_report_measure_mappings(distinct_report_measures, context)\n","            except Exception as e:\n","                print(f\"❌ Failed to save REPORT MEASURE mappings: {e}\")\n","        else:\n","            print(\"ℹ️ No new REPORT MEASUREs discovered.\")\n","\n","\n","@log_function_calls\n","def capture_detailed_logs(context: dict) -> set:\n","    \"\"\"\n","    Captures detailed DAX query logs, including performance metrics.\n","    \n","    It retrieves prior detailed logs (if available), builds dynamic queries via QueryLogCollector,\n","    and processes the results. The function also handles historical user mapping for principal names.\n","    Finally, it updates the Delta table for detailed logs and masks user names for old records if needed.\n","    \n","    Returns:\n","      A set of ReportIds collected from the detailed logs.\n","    \"\"\"\n","    report_ids = set()\n","    try:\n","        # Attempt to access the detailed_logs table for the current model.\n","        log_tbl = spark.table(historical_table_names[\"detailed_logs\"]).filter(\n","            (col(\"ModelUuid\") == context[\"source_model_uuid\"])\n","        )\n","    except Exception:\n","        log_tbl = None\n","        print(\"⚠️ detailed_logs table not accessible; proceeding without prior data.\")\n","    \n","    # Create a QueryLogCollector instance for detailed queries.\n","    qcollector = QueryLogCollector(context)\n","    generate_test_query = lambda s, e: qcollector.generate_detailed_query(s, e, include_summary=True)\n","    generate_main_query = lambda s, e: qcollector.generate_detailed_query(s, e, include_summary=False)\n","    \n","    def normalize_user(user):\n","        # Normalize user strings for consistency in mapping (trim and lower-case).\n","        return user.strip().lower() if user else None\n","    \n","    historical_mapping = {}\n","    if collect_principal_names == 1:\n","        try:\n","            # Define historical time window for building user mapping (e.g., 30 days ago to yesterday).\n","            start_hist = (context[\"as_of_datetime\"] - relativedelta(days=30)).replace(minute=0, second=0, microsecond=0)\n","            end_hist = (context[\"as_of_datetime\"] - relativedelta(days=1)).replace(minute=0, second=0, microsecond=0)\n","            # Lambda functions to generate test and main queries for historical data.\n","            def gen_hist_test(s, e):\n","                return f\"\"\"\n","                    let startTime = {s.strftime(\"datetime(%Y-%m-%dT%H:%M:%S)\")};\n","                    let endTime = {e.strftime(\"datetime(%Y-%m-%dT%H:%M:%S)\")};\n","                    let data = SemanticModelLogs\n","                        | where Timestamp between (startTime .. endTime)\n","                        | where OperationName in (\"Error\", \"QueryEnd\")\n","                        | distinct XmlaSessionId, ExecutingUser;\n","                    data | summarize totalCount = count()\n","                \"\"\"\n","            def gen_hist_main(s, e):\n","                return f\"\"\"\n","                    let startTime = {s.strftime(\"datetime(%Y-%m-%dT%H:%M:%S)\")};\n","                    let endTime = {e.strftime(\"datetime(%Y-%m-%dT%H:%M:%S)\")};\n","                    SemanticModelLogs\n","                    | where Timestamp between (startTime .. endTime)\n","                    | where OperationName in (\"Error\", \"QueryEnd\")\n","                    | distinct XmlaSessionId, ExecutingUser\n","                \"\"\"\n","            hist_results = []\n","            # Callback to process historical data results.\n","            def process_hist(main_result, start_ts):\n","                hist_results.append(main_result.toPandas())\n","            total_hours = (end_hist - start_hist).total_seconds() / 3600.0\n","            start_idx = qcollector.find_starting_index(total_hours)\n","            # Attempt to collect historical mapping data over the defined period.\n","            success = qcollector.attempt_interval_query(start_hist, end_hist, start_idx, gen_hist_test, gen_hist_main, process_hist)\n","            if not success:\n","                print(\"⚠️ Failed to fully collect historical mapping data.\")\n","            # Concatenate all historical query results into a single DataFrame.\n","            hist_kql_pd = pd.concat(hist_results, ignore_index=True) if hist_results else pd.DataFrame(columns=[\"XmlaSessionId\", \"ExecutingUser\"])\n","            try:\n","                hist_logs_df = spark.table(historical_table_names[\"detailed_logs\"]).select(\"XmlaSessionId\", \"ExecutingUser\").distinct()\n","                hist_logs_pd = hist_logs_df.toPandas()\n","            except Exception:\n","                print(\"⚠️ detailed_logs table missing; using empty historical data.\")\n","                hist_logs_pd = pd.DataFrame(columns=[\"XmlaSessionId\", \"ExecutingUser\"])\n","            if not hist_kql_pd.empty and not hist_logs_pd.empty:\n","                # Prepare the historical mapping by merging current and historical log data.\n","                hist_kql = hist_kql_pd.rename(columns={\"ExecutingUser\": \"ActualUser\"})\n","                hist_kql[\"ActualUser\"] = hist_kql[\"ActualUser\"].apply(normalize_user)\n","                hist_logs = hist_logs_pd.rename(columns={\"ExecutingUser\": \"MaskedUser\"})\n","                merged = pd.merge(hist_kql, hist_logs, on=\"XmlaSessionId\", how=\"inner\")\n","                merged[\"NormalizedActualUser\"] = merged[\"ActualUser\"].apply(normalize_user)\n","                merged = merged.drop_duplicates(subset=[\"NormalizedActualUser\"])\n","                historical_mapping = { row[\"NormalizedActualUser\"]: row[\"MaskedUser\"] for _, row in merged.iterrows() if pd.notnull(row[\"NormalizedActualUser\"]) }\n","            else:\n","                historical_mapping = {}\n","            print(f\"✅ Built historical mapping with {len(historical_mapping)} entries.\")\n","        except Exception as e:\n","            print(f\"⚠️ Historical mapping build failed: {e}\")\n","            historical_mapping = {}\n","    \n","    # A lock to ensure thread-safe updates to the historical mapping.\n","    mapping_lock = threading.Lock()\n","    \n","    def process_detail_result(main_result, start_ts):\n","        nonlocal historical_mapping\n","        # Mask user names based on the collection mode.\n","        if collect_principal_names == 2:\n","            main_result = main_result.withColumn(\"ExecutingUser\", lit(\"Masked\"))\n","        elif collect_principal_names == 1:\n","            try:\n","                new_users = [normalize_user(row[0]) for row in main_result.select(\"ExecutingUser\").distinct().collect()]\n","            except Exception as e:\n","                print(f\"❌ Error retrieving distinct users: {e}\")\n","                new_users = []\n","            with mapping_lock:\n","                for user in new_users:\n","                    if user and user not in historical_mapping:\n","                        # Generate a new masked value for unseen users.\n","                        historical_mapping[user] = str(uuid4())\n","            # Broadcast the historical mapping for efficient usage in Spark transformations.\n","            broadcast_map = spark.sparkContext.broadcast(historical_mapping)\n","            def mask_user(actual):\n","                # Replace the actual user with the masked version if available.\n","                return broadcast_map.value.get(normalize_user(actual), actual)\n","            mask_udf = udf(mask_user, StringType())\n","            main_result = main_result.withColumn(\"ExecutingUser\", mask_udf(col(\"ExecutingUser\")))\n","        # Save the detailed log result along with metadata about the interval.\n","        save_dataframe_to_delta_table(\n","            data=main_result,\n","            table_name=historical_table_names[\"detailed_logs\"],\n","            context=context,\n","            AsOfDate=start_ts.date(),\n","            AsOfDateTime=start_ts,\n","        )\n","        try:\n","            # Extract distinct ReportIds from the result and update the report_ids set.\n","            distinct_ids = set(main_result.select(\"ReportId\").rdd.flatMap(lambda x: x).collect())\n","            report_ids.update(distinct_ids)\n","            print(f\"✅ Collected {len(distinct_ids)} ReportIds for interval starting at {start_ts}.\")\n","        except Exception:\n","            print(f\"❌ Failed to extract ReportIds for interval starting at {start_ts}.\")\n","    \n","    # Process intervals for each day within the specified range.\n","    for days_ago in range(1, max_days_ago_to_collect + 1):\n","        process_intervals_for_day_generic(context, log_tbl, qcollector, days_ago,\n","                                          apply_cutoff=(days_ago == 1),\n","                                          generate_test_query=generate_test_query,\n","                                          generate_main_query=generate_main_query,\n","                                          process_result=process_detail_result,\n","                                          label=\"detailed logs\")\n","    \n","    if mask_principal_names_after_days > 0:\n","        # Calculate cutoff date for masking older user names.\n","        cutoff_date = context[\"as_of_date\"] - timedelta(days=mask_principal_names_after_days)\n","        update_query = f\"\"\"\n","            UPDATE {historical_table_names[\"detailed_logs\"]}\n","            SET ExecutingUser = 'Masked'\n","            WHERE AsOfDate < '{cutoff_date}'\n","        \"\"\"\n","        try:\n","            spark.sql(update_query)\n","            print(f\"✅ Masked user names for records older than {mask_principal_names_after_days} days (before {cutoff_date}).\")\n","        except Exception as e:\n","            print(f\"❌ Failed to mask user names for historical detailed logs: {e}\")\n","    \n","    print(f\"✅ capture_detailed_logs complete. Total ReportIds: {len(report_ids)}\")\n","    return report_ids\n","\n","\n","@log_function_calls\n","def capture_all_query_counts_and_mappings(context: dict, model_columns: pd.DataFrame, model_measures: pd.DataFrame) -> list:\n","    \"\"\"\n","    Captures query counts for columns, measures, and report measures, and processes them into mappings.\n","    \n","    It performs the following steps:\n","      1. Validates that all required context keys are present.\n","      2. Processes columns and measures separately to standardize their metadata.\n","      3. Saves the standardized mappings to Delta tables.\n","      4. Captures query counts for each object type using the helper functions.\n","      5. Processes detailed logs to extract ReportIds and updates mappings for report measures.\n","    \n","    Returns:\n","      A list of ReportIds extracted from the detailed logs.\n","    \"\"\"\n","    # Verify that all required keys exist in the context dictionary.\n","    required = {\"source_model_uuid\", \"source_model_workspace_uuid\", \"source_model_name\", \"log_analytics_kusto_uri\", \"log_analytics_kusto_database\"}\n","    missing = required - context.keys()\n","    if missing:\n","        raise KeyError(f\"❌ Missing context keys: {', '.join(missing)}\")\n","    \n","    report_ids = set()\n","    try:\n","        print(\"📊 Processing columns...\")\n","        # Process columns to generate standardized mappings.\n","        processed_cols = process_semantic_model_objects(model_columns, \"COLUMN\")\n","        save_dataframe_to_delta_table(\n","            data=processed_cols,\n","            table_name=historical_table_names[\"object_mapping\"],\n","            context=context,\n","        )\n","        capture_query_counts_by_object(context, \"COLUMN\", processed_cols)\n","    except Exception as e:\n","        print(f\"❌ Columns processing failed. Error: {e}\")\n","    \n","    try:\n","        print(\"📊 Processing measures...\")\n","        # Process measures to generate standardized mappings.\n","        processed_measures = process_semantic_model_objects(model_measures, \"MEASURE\")\n","        save_dataframe_to_delta_table(\n","            data=processed_measures,\n","            table_name=historical_table_names[\"object_mapping\"],\n","            context=context,\n","        )\n","        capture_query_counts_by_object(context, \"MEASURE\", processed_measures)\n","    except Exception as e:\n","        print(f\"❌ Measures processing failed. Error: {e}\")\n","    \n","    try:\n","        print(\"📊 Processing REPORT MEASUREs...\")\n","        # Process report measures (without a DataFrame of model objects).\n","        capture_query_counts_by_object(context, \"REPORT MEASURE\", None)\n","    except Exception as e:\n","        print(f\"❌ REPORT MEASURE processing failed. Error: {e}\")\n","    \n","    try:\n","        # Capture detailed logs and extract ReportIds.\n","        detailed_ids = capture_detailed_logs(context)\n","        if detailed_ids:\n","            report_ids.update(detailed_ids)\n","            print(f\"✅ Updated ReportIds with {len(detailed_ids)} detailed ReportIds.\")\n","        else:\n","            print(\"⚠️ No detailed ReportIds captured.\")\n","    except Exception as e:\n","        print(f\"❌ Detailed log capture failed. Error: {e}\")\n","    # Return the collected ReportIds as a list.\n","    return list(report_ids)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"94df99f2-15a8-4994-addd-965ac72721d2"},{"cell_type":"markdown","source":["### Capturing Unused Delta Table Columns & Source Mappings"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"5cc01c2c-e912-4fe9-890d-bc9298d599d7"},{"cell_type":"code","source":["@log_function_calls\n","def capture_unused_delta_columns(context: dict) -> None:\n","    \"\"\"\n","    Captures unused columns from the Delta tables by comparing lakehouse metadata with model usage.\n","\n","    If lakehouse information is missing (i.e., no lakehouse name provided), the function inserts placeholder rows.\n","    Otherwise, it connects to the lakehouse to retrieve column metadata from Delta tables, compares it to the\n","    columns actually used in the model (obtained via the Table Object Model (TOM)), and saves the differences\n","    (unused columns) to designated Delta tables.\n","    \"\"\"\n","    # Define the required context keys for retrieving lakehouse and model metadata.\n","    required_keys = [\n","        \"source_lakehouse_name\",\n","        \"source_lakehouse_workspace_uuid\",\n","        \"source_lakehouse_uuid\",\n","        \"source_model_uuid\",\n","        \"source_model_workspace_uuid\",\n","    ]\n","    # Ensure all required keys are present; if not, raise a KeyError.\n","    for key in required_keys:\n","        if key not in context:\n","            raise KeyError(f\"❌ Missing required context key: '{key}'\")\n","            \n","    # If lakehouse details are missing, insert placeholder rows into the target Delta tables and exit.\n","    if not context[\"source_lakehouse_name\"]:\n","        save_dataframe_to_delta_table(\n","            data=spark.createDataFrame(\n","                [\n","                    {\n","                        \"TableName\": \"N/A\",\n","                        \"SourceTableName\": \"N/A\",\n","                        \"SourceColumnName\": \"N/A\",\n","                    }\n","                ]\n","            ),\n","            table_name=historical_table_names[\"unused_columns\"],\n","            context=context,\n","        )\n","        save_dataframe_to_delta_table(\n","            data=spark.createDataFrame(\n","                [\n","                    {\n","                        \"TableName\": \"N/A\",\n","                        \"ColumnName\": \"N/A\",\n","                        \"SourceTableName\": \"N/A\",\n","                        \"SourceColumnName\": \"N/A\",\n","                    }\n","                ]\n","            ),\n","            table_name=historical_table_names[\"source_mapping\"],\n","            context=context,\n","        )\n","        return\n","\n","    tom_tables_info = []\n","    # Connect to the semantic model using the TOM API to retrieve table metadata.\n","    with labs.tom.connect_semantic_model(\n","        dataset=context[\"source_model_uuid\"],\n","        readonly=True,\n","        workspace=context[\"source_model_workspace_uuid\"],\n","    ) as tom:\n","        # Iterate over tables in the semantic model.\n","        for tbl in tom.model.Tables:\n","            # Get the first partition (if any) to extract source information.\n","            partition = next(iter(tbl.Partitions), None)\n","            if partition and partition.Source:\n","                tom_tables_info.append(\n","                    {\n","                        \"tom_table\": tbl,\n","                        \"schema_name\": getattr(partition.Source, \"SchemaName\", None),\n","                        \"source_table_name\": getattr(partition.Source, \"EntityName\", None),\n","                    }\n","                )\n","\n","    def get_lakehouse_columns():\n","        \"\"\"\n","        Retrieves columns from lakehouse Delta tables by manually reading data from known storage paths.\n","        \n","        For each table from the TOM metadata, constructs the path and loads the Delta table,\n","        then extracts the column names.\n","        \"\"\"\n","        data = []\n","        # Loop through each table metadata record obtained from TOM.\n","        for item in tom_tables_info:\n","            schema = item[\"schema_name\"]\n","            entity = item[\"source_table_name\"]\n","            if entity:\n","                # Build a folder path using schema and table name; schema is appended if present.\n","                schema_part = f\"{schema}/\" if schema else \"\"\n","                path = (\n","                    f\"abfss://{context['source_lakehouse_workspace_uuid']}\"\n","                    f\"@{abfss_base_path}/\"\n","                    f\"{context['source_lakehouse_uuid']}/Tables/{schema_part}{entity}/\"\n","                )\n","                try:\n","                    # Load the Delta table from the specified path.\n","                    df = spark.read.format(\"delta\").load(path)\n","                    # For each column in the Delta table schema, append its name along with the table.\n","                    for col_name in df.schema.fieldNames():\n","                        data.append({\"Table Name\": entity, \"Column Name\": col_name})\n","                except Exception as ex:\n","                    print(f\"⚠️ Unable to read from {path}: {ex}\")\n","        return spark.createDataFrame(data)\n","\n","    # Retrieve all columns from the lakehouse using the defined helper function.\n","    all_cols_df = get_lakehouse_columns()\n","\n","    # Ensure the result is a Spark DataFrame (convert if necessary).\n","    if not isinstance(all_cols_df, pyspark.sql.DataFrame):\n","        all_cols_df = spark.createDataFrame(all_cols_df)\n","\n","    # Group the retrieved columns by table name and collect them into sets.\n","    grouped_df = (\n","        all_cols_df.groupBy(\"Table Name\")\n","        .agg(collect_set(\"Column Name\").alias(\"columns\"))\n","        .collect()\n","    )\n","    # Create a dictionary mapping each table to its set of columns.\n","    table_columns = {row[\"Table Name\"]: set(row[\"columns\"]) for row in grouped_df}\n","\n","    remaining_columns = []  # To store columns present in the lakehouse but unused in the model.\n","    source_mapping = []     # To store mapping between model columns and source columns as defined in TOM.\n","\n","    for info in tom_tables_info:\n","        tbl = info[\"tom_table\"]\n","        src_table = info[\"source_table_name\"]\n","        if not src_table:\n","            continue\n","        # Get the set of columns available in the lakehouse for this source table.\n","        delta_cols = table_columns.get(src_table, set())\n","        # Get the set of columns used in the model from TOM metadata.\n","        used_cols = {col.SourceColumn for col in tbl.Columns if hasattr(col, \"SourceColumn\")}\n","        # Build the source mapping for each column that has a SourceColumn attribute.\n","        for col in tbl.Columns:\n","            if hasattr(col, \"SourceColumn\"):\n","                source_mapping.append(\n","                    {\n","                        \"TableName\": tbl.Name,\n","                        \"ColumnName\": col.Name,\n","                        \"SourceTableName\": src_table,\n","                        \"SourceColumnName\": col.SourceColumn,\n","                    }\n","                )\n","        # Identify columns in the lakehouse that are not used in the model.\n","        for unused in delta_cols - used_cols:\n","            remaining_columns.append(\n","                {\n","                    \"TableName\": tbl.Name,\n","                    \"SourceTableName\": src_table,\n","                    \"SourceColumnName\": unused,\n","                }\n","            )\n","    # Convert the lists of unused columns and source mappings to Spark DataFrames.\n","    unused_df = spark.createDataFrame(remaining_columns)\n","    mapping_df = spark.createDataFrame(source_mapping)\n","    # Save the unused columns data to the Delta table for unused columns.\n","    save_dataframe_to_delta_table(\n","        data=unused_df,\n","        table_name=historical_table_names[\"unused_columns\"],\n","        context=context,\n","    )\n","    # Save the source mapping data to the Delta table for source mappings.\n","    save_dataframe_to_delta_table(\n","        data=mapping_df,\n","        table_name=historical_table_names[\"source_mapping\"],\n","        context=context,\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8f924f87-9550-47b5-8168-ce3308f531f1"},{"cell_type":"markdown","source":["### Capturing and Processing Reports"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"bf8064f9-1cfd-46fc-9713-d116141ce85e"},{"cell_type":"code","source":["@log_function_calls\n","def get_reports(context: dict, all_workspaces: pd.DataFrame, report_ids_to_keep: list) -> None:\n","    \"\"\"\n","    Retrieves report information from each workspace and filters to retain only the specified ReportIds.\n","    \n","    For each workspace:\n","      - Calls the Fabric API to list reports.\n","      - Selects and renames columns for consistency.\n","      - Adds workspace identifiers.\n","      - Filters out reports not in the report_ids_to_keep list.\n","      - For any missing ReportIds, creates placeholder rows with default \"Unknown\" values.\n","    The final combined DataFrame is saved to the Delta table for reports.\n","    \"\"\"\n","    reports_list = []  # Initialize a list to store DataFrames of reports from each workspace.\n","    try:\n","        # Iterate over each workspace provided in the all_workspaces DataFrame.\n","        for _, workspace in all_workspaces.iterrows():\n","            ws_id = workspace[\"Id\"]\n","            ws_name = workspace[\"Name\"]\n","            try:\n","                # Retrieve the list of reports for the current workspace using the Fabric API.\n","                reports_df = fabric.list_reports(workspace=ws_id)\n","            except Exception as e:\n","                # If report listing fails for a workspace, log the error and skip to the next workspace.\n","                print(f'❌ Failed to list reports for workspace \"{ws_name}\" (ID: {ws_id}). Error: {e}')\n","                continue\n","            \n","            # Select only the relevant columns and rename them for consistency.\n","            reports_df = reports_df[[\"Id\", \"Name\", \"Web Url\"]].rename(\n","                columns={\"Id\": \"ReportId\", \"Name\": \"ReportName\", \"Web Url\": \"WebUrl\"}\n","            )\n","            # Add workspace-specific identifiers to the report DataFrame.\n","            reports_df[\"WorkspaceId\"] = ws_id\n","            reports_df[\"WorkspaceName\"] = ws_name\n","            # Filter the reports to keep only those whose ReportId is in the report_ids_to_keep list.\n","            reports_df = reports_df[reports_df[\"ReportId\"].isin(report_ids_to_keep)]\n","            # Append the filtered DataFrame to the list.\n","            reports_list.append(reports_df)\n","        \n","        # Combine all report DataFrames into one; if none were found, create an empty DataFrame with the required columns.\n","        combined_reports = (\n","            pd.concat(reports_list, ignore_index=True)\n","            if reports_list\n","            else pd.DataFrame(\n","                columns=[\n","                    \"ReportId\",\n","                    \"ReportName\",\n","                    \"WebUrl\",\n","                    \"WorkspaceId\",\n","                    \"WorkspaceName\",\n","                ]\n","            )\n","        )\n","        # Determine which ReportIds from the desired list are missing in the combined reports.\n","        missing_ids = set(report_ids_to_keep) - set(combined_reports[\"ReportId\"])\n","        if missing_ids:\n","            # For any missing ReportIds, create placeholder rows with default \"Unknown\" values.\n","            missing_df = pd.DataFrame(\n","                [\n","                    {\n","                        \"ReportId\": rid,\n","                        \"ReportName\": \"Unknown\",\n","                        \"WebUrl\": \"Unknown\",\n","                        \"WorkspaceId\": \"Unknown\",\n","                        \"WorkspaceName\": \"Unknown\",\n","                    }\n","                    for rid in missing_ids\n","                ]\n","            )\n","            # Append the placeholder rows to the combined DataFrame.\n","            combined_reports = pd.concat(\n","                [combined_reports, missing_df], ignore_index=True\n","            )\n","        # Save the final combined reports DataFrame to the Delta table for source reports.\n","        save_dataframe_to_delta_table(\n","            data=combined_reports,\n","            table_name=historical_table_names[\"source_reports\"],\n","            context=context,\n","        )\n","        print(f\"✅ Retrieved and saved {len(combined_reports)} reports.\")\n","    except Exception as e:\n","        print(f\"❌ get_reports encountered an error: {e}\")\n","        raise"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8d224f01-da6b-4af9-9cbf-0e0a573d97fe"},{"cell_type":"markdown","source":["### Cold Cache Helpers: Log Table, Model Refresh, Cache Clear, and Timing Capture"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"ae23b213-57a3-4654-997e-b1aa8ba88e38"},{"cell_type":"code","source":["@log_function_calls\n","def fetch_log_table(context: dict, table_name: str) -> pyspark.sql.DataFrame:\n","    \"\"\"\n","    Attempts to fetch a log table for today's QueryEnd events for the current model.\n","\n","    Returns:\n","      A Spark DataFrame filtered by ModelUuid, AsOfDate, and EventClass,\n","      or None if the table does not exist.\n","    \"\"\"\n","    try:\n","        # Read the table from Spark using its name.\n","        raw_tbl = spark.read.table(table_name)\n","        # Apply filters: match the current model, today's date, and ensure the event class is \"QueryEnd\".\n","        filters = (\n","            (col(\"ModelUuid\") == context[\"source_model_uuid\"]) &\n","            (col(\"AsOfDate\") == context[\"as_of_date\"]) &\n","            (col(\"EventClass\") == \"QueryEnd\")\n","        )\n","        return raw_tbl.filter(filters)\n","    except Exception:\n","        # Log informational message if the table is not found; it may be created later.\n","        print(f\"ℹ️ Log table `{table_name}` does not exist. It will be created if needed.\")\n","        return None\n","\n","\n","@log_function_calls\n","def wait_for_model_creation() -> None:\n","    \"\"\"\n","    Polls the target workspace until the cloned model is created.\n","\n","    Continues checking every 5 seconds until the cloned model appears in the dataset list.\n","    \"\"\"\n","    # Continuously check if the cloned model is present in the dataset list from the target workspace.\n","    while (\n","        cloned_model_name\n","        not in fabric.list_datasets(workspace=cold_cache_target_workspace_name, mode=\"rest\")[\"Dataset Name\"].to_list()\n","    ):\n","        print(\"⌛ Waiting for cloned model creation...\")\n","        time.sleep(5)\n","\n","\n","@log_function_calls\n","def refresh_dataset(model_name: str, refresh_type: str) -> None:\n","    \"\"\"\n","    Initiates a dataset refresh using the specified refresh type (e.g., \"clearValues\" or \"full\").\n","\n","    Waits until the refresh operation completes with a status in valid_refresh_statuses.\n","    \"\"\"\n","    attempts = 0\n","    # Start the refresh process and obtain a refresh status identifier.\n","    refresh_status = fabric.refresh_dataset(model_name, refresh_type=refresh_type)\n","    # Poll until the refresh status indicates completion or failure.\n","    while (\n","        fabric.get_refresh_execution_details(model_name, refresh_status).status not in valid_refresh_statuses\n","    ):\n","        attempts += 1\n","        if attempts >= max_attempts:\n","            raise Exception(f\"❌ Refresh failed after {attempts} attempts.\")\n","        # Wait briefly before the next check.\n","        time.sleep(3)\n","\n","\n","@log_function_calls\n","def clear_cache(model_name: str) -> None:\n","    \"\"\"\n","    Clears the VertiPaq cache for the specified model.\n","\n","    It calls a helper function to clear the cache and then verifies the operation\n","    by executing a trivial DAX query. The process is retried until successful or\n","    until the maximum number of attempts is reached.\n","    \"\"\"\n","    attempts = 0\n","    while True:\n","        try:\n","            # Attempt to clear the cache via a helper function.\n","            labs.clear_cache(model_name)\n","            # Verify the cache clear by executing a simple DAX query.\n","            fabric.evaluate_dax(model_name, \"EVALUATE {1}\")\n","            print(f\"✅ Cache cleared for model `{model_name}`.\")\n","            break\n","        except Exception as e:\n","            attempts += 1\n","            print(f\"⚠️ Cache clear attempt failed: {e}\")\n","            if attempts >= max_attempts:\n","                raise Exception(\"❌ Failed to clear VertiPaq cache.\")\n","            # Refresh the TOM cache for the target workspace to ensure consistency.\n","            fabric.refresh_tom_cache(cold_cache_target_workspace_name)\n","            time.sleep(5)\n","\n","\n","def capture_cold_cache_timings(column_name: str, trace) -> None:\n","    \"\"\"\n","    Executes a DAX query for a specific column to measure cold cache performance.\n","\n","    After running the query, it checks the trace logs for a QueryEnd event that\n","    references the given column name. Raises an exception if the expected event\n","    is not found after a specified number of attempts.\n","    \"\"\"\n","    # Construct a DAX expression to query a sample of the column's values.\n","    dax_expr = f\"EVALUATE TOPN(1, VALUES({column_name}))\"\n","    try:\n","        # Execute the DAX query on the cloned model.\n","        fabric.evaluate_dax(cloned_model_name, dax_expr)\n","    except Exception as e:\n","        print(f\"❌ DAX evaluation error for {column_name}: {e}\")\n","        raise Exception(f\"Failed to evaluate DAX for {column_name}: {e}\")\n","    attempts = 0\n","    while attempts < max_attempts:\n","        try:\n","            # Retrieve trace logs from the trace object.\n","            trace_logs = trace.get_trace_logs()\n","            if trace_logs is None:\n","                raise Exception(f\"Trace logs are None for column {column_name}\")\n","            # Look for a QueryEnd event that includes the column name in its Text Data.\n","            matching_logs = trace_logs[\n","                (trace_logs[\"Event Class\"] == \"QueryEnd\") &\n","                (trace_logs[\"Text Data\"].str.contains(re.escape(column_name), na=False))\n","            ]\n","            if not matching_logs.empty:\n","                break  # Expected trace log found; exit loop.\n","        except Exception as e:\n","            print(f\"❌ Error reading trace logs for {column_name}: {e}\")\n","            raise Exception(f\"Failed to access trace logs for {column_name}: {e}\")\n","        attempts += 1\n","        if attempts >= max_attempts:\n","            raise Exception(f\"❌ Failed after {attempts} attempts for column {column_name}\")\n","        time.sleep(3)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"df5a826e-017e-4e28-97c4-67549f954e20"},{"cell_type":"markdown","source":["### Capturing Cold Cache Performance Metrics"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"94fbe3c0-3113-4639-a8c5-66f1741f0626"},{"cell_type":"code","source":["@log_function_calls\n","def capture_cold_cache_performance(context: dict, model_columns: pd.DataFrame) -> set:\n","    \"\"\"\n","    Measures cold cache performance for eligible columns by deploying a cloned model and executing parallel DAX queries.\n","\n","    Steps:\n","      1. Set up and deploy a cloned version of the model.\n","      2. Refresh and clear the cache of the cloned model.\n","      3. Create a trace to capture QueryEnd events and measure performance.\n","      4. Execute DAX queries in parallel for each eligible column.\n","      5. Save the trace logs with performance metrics.\n","\n","    Returns:\n","      A set of column identifiers (formatted strings) that were successfully processed.\n","    \"\"\"\n","    global cloned_model_name, valid_refresh_statuses, max_attempts\n","\n","    # Define the cloned model's name based on the source model.\n","    cloned_model_name = f\"{context['source_model_name']} - Semantic Model Audit\"\n","    # Valid statuses indicating the refresh operation has completed.\n","    valid_refresh_statuses = [\"Completed\", \"Failed\"]\n","    # Maximum number of attempts for refresh and cache clearing.\n","    max_attempts = 120\n","\n","    # Try to get existing cold cache log data, if available.\n","    log_tbl = fetch_log_table(context, historical_table_names[\"cold_cache_measurements\"])\n","\n","    # Filter out columns not eligible for cold cache measurement (e.g., those starting with \"RowNumber-\").\n","    eligible_df = model_columns[~model_columns[\"ColumnName\"].str.startswith(\"RowNumber-\")]\n","    # Format each eligible column as a string representation: 'TableName'[ColumnName]\n","    eligible_columns = [\n","        f\"'{row['TableName']}'[{row['ColumnName']}]\"\n","        for _, row in eligible_df.iterrows()\n","    ]\n","\n","    if log_tbl is not None:\n","        # Group the existing log data by column name and count entries.\n","        counts_df = log_tbl.groupBy(\"ColumnName\").count()\n","        # Identify columns that have already reached the maximum queries per day.\n","        columns_to_skip = {\n","            row[\"ColumnName\"]\n","            for row in counts_df.filter(col(\"count\") >= max_queries_daily)\n","            .select(\"ColumnName\")\n","            .collect()\n","        }\n","        # Exclude columns that should be skipped.\n","        filtered_columns = [col for col in eligible_columns if col not in columns_to_skip]\n","        num_skipped = len(columns_to_skip)\n","    else:\n","        filtered_columns = eligible_columns\n","        num_skipped = 0\n","\n","    num_query = len(filtered_columns)\n","    print(f\"📊 {num_query} columns to query; {num_skipped} columns skipped.\")\n","\n","    # If cold cache measurements should be collected and there are columns to query.\n","    if collect_cold_cache_measurements and num_query > 0:\n","        try:\n","            # Deploy a cloned version of the model for cold cache testing.\n","            labs.deploy_semantic_model(\n","                source_dataset=context[\"source_model_name\"],\n","                source_workspace=context[\"source_model_workspace_name\"],\n","                target_dataset=cloned_model_name,\n","                target_workspace=cold_cache_target_workspace_name,\n","                refresh_target_dataset=False,\n","                overwrite=True,\n","            )\n","            # Refresh the TOM cache for the target workspace.\n","            fabric.refresh_tom_cache(cold_cache_target_workspace_name)\n","            time.sleep(30)  # Wait for the cache refresh to settle.\n","            wait_for_model_creation()  # Poll until the cloned model is created.\n","            # Refresh the cloned model's dataset with a \"clearValues\" and then \"full\" refresh.\n","            refresh_dataset(cloned_model_name, \"clearValues\")\n","            refresh_dataset(cloned_model_name, \"full\")\n","            # Clear the VertiPaq cache for the cloned model.\n","            clear_cache(cloned_model_name)\n","            time.sleep(5)  # Short delay after clearing the cache.\n","\n","            # Set up a trace connection to capture QueryEnd events for performance metrics.\n","            trace_conn = fabric.create_trace_connection(\n","                dataset=cloned_model_name, workspace=cold_cache_target_workspace_name\n","            )\n","            trace_conn.drop_traces()  # Clear any existing traces.\n","            trace_name = f\"Simple DAX Trace {uuid4()}\"\n","            event_schema = {\n","                \"QueryEnd\": [\"EventClass\", \"TextData\", \"Duration\", \"CpuTime\", \"Success\"]\n","            }\n","            # Create a trace within a context manager to ensure proper resource management.\n","            with fabric.create_trace_connection(\n","                dataset=cloned_model_name, workspace=cold_cache_target_workspace_name\n","            ) as trace_conn:\n","                with trace_conn.create_trace(event_schema=event_schema, name=trace_name) as trace:\n","                    trace.start()\n","                    # Wait until the trace has started.\n","                    while not trace.is_started:\n","                        time.sleep(2)\n","                    print(\"🔄 Querying columns in parallel...\")\n","                    total_cols = len(filtered_columns)\n","                    if total_cols == 0:\n","                        print(\"ℹ️ No columns to query after filtering.\")\n","                        return set()\n","                    # Set up progress tracking.\n","                    progress_interval = math.ceil(total_cols / 10)\n","                    next_progress = progress_interval\n","                    completed = 0\n","                    successful = set()\n","                    failed = set()\n","                    # Execute queries in parallel using a thread pool.\n","                    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","                        future_to_col = {\n","                            executor.submit(capture_cold_cache_timings, col, trace): col\n","                            for col in filtered_columns\n","                        }\n","                        for future in as_completed(future_to_col):\n","                            col_name = future_to_col[future]\n","                            try:\n","                                # This will raise an exception if the query for this column fails.\n","                                future.result()\n","                                successful.add(col_name)\n","                            except Exception as e:\n","                                print(f\"❌ Error processing {col_name}: {e}\")\n","                                failed.add(col_name)\n","                                continue\n","                            completed += 1\n","                            # Print progress updates at regular intervals.\n","                            if completed >= next_progress:\n","                                print(f\"✅ {completed / total_cols * 100:.0f}% of columns completed.\")\n","                                next_progress += progress_interval\n","                    try:\n","                        # Stop the trace and retrieve the captured trace logs.\n","                        trace_logs = trace.stop()\n","                        if trace_logs is not None and not trace_logs.empty:\n","                            # Extract the column name from the \"Text Data\" field using a regex.\n","                            trace_logs[\"ColumnName\"] = trace_logs[\"Text Data\"].str.extract(r\"VALUES\\s*\\(\\s*(.+?)\\s*\\)\\s*\\)\")\n","                            # Save the trace logs to the Delta table for cold cache measurements.\n","                            save_dataframe_to_delta_table(\n","                                data=trace_logs,\n","                                table_name=historical_table_names[\"cold_cache_measurements\"],\n","                                context=context,\n","                                QueryUuid=str(uuid4()),\n","                            )\n","                    except Exception as e:\n","                        print(f\"❌ Failed to process trace logs: {e}\")\n","                    if failed:\n","                        print(f\"⚠️ {len(failed)} columns failed: {', '.join(failed)}\")\n","                    else:\n","                        print(\"✅ All columns processed successfully.\")\n","                    print(f\"✅ Cold cache performance capture complete. {len(successful)} columns queried successfully.\")\n","                    return successful\n","        except Exception as e:\n","            print(f\"❌ Error during cold cache performance capture: {e}\")\n","            return set()\n","    else:\n","        print(\"ℹ️ No columns to query after filtering; inserting placeholder.\")\n","        try:\n","            # If no columns are eligible, insert a placeholder record to maintain table structure.\n","            save_dataframe_to_delta_table(\n","                data=pd.DataFrame({\n","                    \"EventClass\": [\"N/A\"],\n","                    \"TextData\": [\"N/A\"],\n","                    \"Duration\": [0],\n","                    \"CpuTime\": [0],\n","                    \"Success\": [\"N/A\"],\n","                    \"ColumnName\": [\"N/A\"],\n","                }),\n","                table_name=historical_table_names[\"cold_cache_measurements\"],\n","                context=context,\n","                QueryUuid=str(uuid4()),\n","            )\n","        except Exception as e:\n","            print(f\"❌ Failed to insert placeholder for cold cache measurements: {e}\")\n","        return set()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"45fbefda-b9cb-4a89-9efe-2e6aa7846335"},{"cell_type":"markdown","source":["### Capturing Resident Column Statistics"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"5a8d5cfd-613c-4b4d-b3da-4c8717cacfd8"},{"cell_type":"code","source":["@log_function_calls\n","def capture_resident_statistics(context: dict, queried_columns: set) -> None:\n","    \"\"\"\n","    Captures resident statistics (e.g., whether columns are loaded in memory, sizes) for model columns.\n","    \n","    It compares current model columns (using the Fabric API) with historical resident statistics,\n","    and saves only new records for columns that have not been recorded yet.\n","    \"\"\"\n","    \n","    def format_column(row: dict) -> str:\n","        # Standardize the column identifier by combining the table and column names.\n","        return f\"'{row['TableName']}'[{row['ColumnName']}]\"\n","    \n","    try:\n","        # Read previously captured resident statistics for the current date from the Delta table.\n","        existing_stats = (\n","            spark.read.table(historical_table_names[\"resident_statistics\"])\n","            .filter(\n","                (col(\"ModelUuid\") == context[\"source_model_uuid\"]) &\n","                (col(\"AsOfDate\") == context[\"as_of_date\"])\n","            )\n","            .select(\"TableName\", \"ColumnName\")\n","            .collect()\n","        )\n","        # Create a set of standardized identifiers for the existing resident statistics.\n","        existing = {format_column(row.asDict()) for row in existing_stats}\n","    except Exception:\n","        print(\"⚠️ Could not read existing resident statistics; proceeding without.\")\n","        existing = set()\n","    \n","    # Determine which model to query: if cold cache was measured, use the cloned model; otherwise, use the source model.\n","    resident_model = (\n","        cloned_model_name\n","        if collect_cold_cache_measurements\n","        else context[\"source_model_name\"]\n","    )\n","    resident_workspace = (\n","        cold_cache_target_workspace_name\n","        if collect_cold_cache_measurements\n","        else context[\"source_model_workspace_name\"]\n","    )\n","    \n","    # Retrieve the current list of model columns using the Fabric API.\n","    model_columns_resident = fabric.list_columns(\n","        dataset=resident_model,\n","        extended=True,\n","        workspace=resident_workspace,\n","    )\n","    # Remove spaces from column names to ensure consistency in identifiers.\n","    model_columns_resident.columns = model_columns_resident.columns.str.replace(\" \", \"\", regex=True)\n","    # Build a set of standardized identifiers for the current model columns.\n","    model_set = {format_column(row) for _, row in model_columns_resident.iterrows()}\n","    \n","    # Identify columns that are new (i.e., not present in the historical resident statistics).\n","    to_capture = model_set - existing\n","    if collect_cold_cache_measurements:\n","        # Optionally restrict to only columns that were previously queried for cold cache metrics.\n","        to_capture = to_capture.intersection(queried_columns)\n","    to_capture = list(to_capture)\n","    \n","    if to_capture:\n","        # Filter the current model columns DataFrame to include only those columns that are new.\n","        filtered = [\n","            row\n","            for _, row in model_columns_resident.iterrows()\n","            if format_column(row) in to_capture\n","        ]\n","        filtered_df = pd.DataFrame(filtered)\n","        print(f\"📈 Capturing resident statistics for {len(filtered_df)} new columns.\")\n","        # Save the new resident statistics to the designated Delta table.\n","        save_dataframe_to_delta_table(\n","            data=filtered_df,\n","            table_name=historical_table_names[\"resident_statistics\"],\n","            context=context,\n","        )\n","    else:\n","        print(\"ℹ️ No new resident statistics to capture for this run.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9dde9ed4-001a-4c14-b85a-52bf3693c866"},{"cell_type":"markdown","source":["### Workspace Monitoring Information"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"3b0250cc-64ff-4b3e-9d66-71b9f4b3377b"},{"cell_type":"code","source":["@log_function_calls\n","def get_workspace_monitoring_info(workspace: str) -> tuple[str, str]:\n","    \"\"\"\n","    Retrieves the Query Service URI and KQL Database Id for the Monitoring KQL database in the given workspace.\n","    \n","    This function queries the list of KQL databases in the workspace using the Fabric API (via labs.list_kql_databases).\n","    It then filters the result to find the database named \"Monitoring KQL database\". If such a database is not found,\n","    it raises a ValueError. Otherwise, it extracts and returns the Query Service URI and the KQL Database Id as a tuple.\n","    \n","    Returns:\n","      A tuple (kusto_uri, kusto_db_guid) where:\n","        - kusto_uri: The URI for querying the KQL database.\n","        - kusto_db_guid: The unique identifier (GUID) for the KQL database.\n","    \n","    Raises:\n","      ValueError: If no KQL databases or the specific \"Monitoring KQL database\" is found in the workspace.\n","    \"\"\"\n","    # Retrieve a DataFrame containing all KQL databases for the given workspace.\n","    df = labs.list_kql_databases(workspace=workspace)\n","    # Check if the DataFrame is empty; if so, no KQL databases exist in the workspace.\n","    if df.empty:\n","        raise ValueError(f\"❌ No KQL databases found in workspace `{workspace}`.\")\n","    # Filter the DataFrame to find the row where the KQL Database Name matches \"Monitoring KQL database\".\n","    df_monitor = df[df[\"KQL Database Name\"] == \"Monitoring KQL database\"]\n","    # If no matching database is found, raise an error.\n","    if df_monitor.empty:\n","        raise ValueError(\n","            f\"❌ Monitoring KQL database not found in workspace `{workspace}`.\"\n","        )\n","    # Extract the Query Service URI from the first (and expected only) matching row.\n","    kusto_uri = df_monitor.iloc[0][\"Query Service URI\"]\n","    # Extract the KQL Database Id (GUID) from the same row.\n","    kusto_db_uuid = df_monitor.iloc[0][\"KQL Database Id\"]\n","    # Return the extracted URI and Database Id as a tuple.\n","    return kusto_uri, kusto_db_uuid\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"61b01fec-eaf4-4b25-be1e-07dc19179834"},{"cell_type":"markdown","source":["### Main Orchestration: Collecting Statistics for Each Semantic Model"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"a3d4dddd-c67e-4de4-bdb7-b7ebcdefbc16"},{"cell_type":"code","source":["@log_function_calls\n","def collect_model_statistics(models: list) -> None:\n","    \"\"\"\n","    Main orchestration function that processes each semantic model to capture various statistics.\n","    \n","    For each model in the provided list, the function performs the following steps:\n","      1. Clean up incomplete historical data if configured (by dropping tables or removing incomplete runs).\n","      2. Build a context dictionary containing all necessary model and workspace details.\n","      3. Record the start of the run in the run_history table.\n","      4. Retrieve workspace monitoring information (using lakehouse details if available).\n","      5. Capture model objects (columns and measures) and save them for historical tracking.\n","      6. Capture measure dependencies via a DAX query.\n","      7. Capture query counts for various model objects and update mappings accordingly.\n","      8. Retrieve detailed query logs and extract ReportIds.\n","      9. Capture unused Delta table columns and cold cache performance.\n","      10. Capture resident column statistics (e.g., column residency in memory).\n","      11. Record the run completion status (marking the run as completed or failed).\n","    \n","    If any critical step fails for a model, the function marks the run as failed and proceeds with the next model.\n","    \"\"\"\n","    # Step 1: Clean up historical data if configured.\n","    if force_delete_historical_tables:\n","        print(\"⚠️ Force-deleting historical tables. All data will be lost.\")\n","        drop_historical_tables()\n","    elif force_delete_incomplete_runs:\n","        print(\"ℹ️ Removing records for incomplete runs.\")\n","        cleanup_incomplete_runs()\n","\n","    # Step 2: Retrieve all workspaces available in the system.\n","    all_workspaces = fabric.list_workspaces()\n","\n","    # Process each model from the provided models list.\n","    for model in models:\n","        now = datetime.now()  # Capture current timestamp for this run.\n","        # Step 2: Build the context dictionary with required metadata.\n","        context = {\n","            \"run_uuid\": str(uuid4()),\n","            \"as_of_datetime\": now,\n","            \"as_of_date\": now.date(),\n","            \"source_model_workspace_name\": model[\"model_workspace_name\"],\n","            \"source_model_workspace_uuid\": fabric.resolve_workspace_id(model[\"model_workspace_name\"]),\n","            \"source_model_name\": model[\"model_name\"],\n","            \"source_model_uuid\": fabric.resolve_dataset_id(model[\"model_name\"], model[\"model_workspace_name\"]),\n","        }\n","        # Include lakehouse details in the context if they are provided.\n","        if \"lakehouse_name\" in model and \"lakehouse_workspace_name\" in model:\n","            context.update({\n","                \"source_lakehouse_name\": model[\"lakehouse_name\"],\n","                \"source_lakehouse_workspace_name\": model[\"lakehouse_workspace_name\"],\n","                \"source_lakehouse_uuid\": labs.resolve_lakehouse_id(model[\"lakehouse_name\"], model[\"lakehouse_workspace_name\"]),\n","                \"source_lakehouse_workspace_uuid\": fabric.resolve_workspace_id(model[\"lakehouse_workspace_name\"]),\n","            })\n","        else:\n","            # Use empty strings if lakehouse details are not provided.\n","            context.update({\n","                \"source_lakehouse_name\": \"\",\n","                \"source_lakehouse_workspace_name\": \"\",\n","                \"source_lakehouse_uuid\": \"\",\n","                \"source_lakehouse_workspace_uuid\": \"\",\n","            })\n","        try:\n","            print(f\"📁 Processing model `{model['model_name']}`\")\n","            # Step 3: Record the start of the run.\n","            record_run_start(context)\n","\n","            # Step 4: Determine workspace monitoring info.\n","            if model[\"log_analytics_kusto_uri\"] or model[\"log_analytics_kusto_database_uuid\"]:\n","                context[\"log_analytics_kusto_uri\"] = model[\"log_analytics_kusto_uri\"]\n","                context[\"log_analytics_kusto_database\"] = model[\"log_analytics_kusto_database_uuid\"]\n","            elif context[\"source_model_workspace_uuid\"]:\n","                (context[\"log_analytics_kusto_uri\"],\n","                 context[\"log_analytics_kusto_database\"]) = get_workspace_monitoring_info(context[\"source_model_workspace_uuid\"])\n","            else:\n","                context[\"log_analytics_kusto_uri\"] = \"\"\n","                context[\"log_analytics_kusto_database\"] = \"\"\n","\n","            report_ids = set()  # To accumulate ReportIds from detailed logs.\n","\n","            try:\n","                # Step 5: Capture model objects (columns and measures).\n","                model_columns, model_measures = capture_semantic_model_objects(context)\n","                # Step 6: Capture measure dependencies.\n","                capture_semantic_model_dependencies(context, model_measures)\n","                # Step 7: Capture query counts and update object mappings.\n","                report_ids = set(capture_all_query_counts_and_mappings(context, model_columns, model_measures))\n","                if report_ids:\n","                    # Step 8: Retrieve and save reports using the collected ReportIds.\n","                    get_reports(context, all_workspaces, list(report_ids))\n","                else:\n","                    print(\"ℹ️ No ReportIds found to process.\")\n","            except Exception as critical_err:\n","                print(f\"❌ Critical step failed for model `{model['model_name']}`: {critical_err}\")\n","                try:\n","                    record_run_completion(context, \"failed\")\n","                    print(f\"🔴 Run UUID: {context['run_uuid']} marked as failed.\")\n","                except Exception as update_err:\n","                    print(f\"❌ Failed to mark run UUID: {context['run_uuid']} as failed. Error: {update_err}\")\n","                continue  # Skip to next model if a critical step fails.\n","\n","            try:\n","                # Step 9: Capture unused Delta table columns.\n","                capture_unused_delta_columns(context)\n","            except Exception as e:\n","                print(f\"⚠️ Failed to capture unused Delta columns for model `{model['model_name']}`: {e}\")\n","\n","            try:\n","                # Step 9 (continued): Capture cold cache performance metrics.\n","                queried_cols = capture_cold_cache_performance(context, model_columns)\n","            except Exception as e:\n","                print(f\"⚠️ Cold cache performance capture failed for model `{model['model_name']}`: {e}\")\n","                queried_cols = set()\n","\n","            try:\n","                # Step 10: Capture resident statistics for columns.\n","                capture_resident_statistics(context, queried_cols)\n","            except Exception as e:\n","                print(f\"⚠️ Resident statistics capture failed for model `{model['model_name']}`: {e}\")\n","\n","            try:\n","                # Step 11: Mark the run as completed.\n","                record_run_completion(context, \"completed\")\n","                print(f\"✅ Run UUID: {context['run_uuid']} completed successfully.\")\n","            except Exception as e:\n","                print(f\"❌ Failed to mark run UUID: {context['run_uuid']} as completed. Error: {e}\")\n","        except Exception as e:\n","            # Log any unexpected error during processing and move to the next model.\n","            print(f\"❌ Unexpected error processing model `{model['model_name']}`: {e}\")\n","            continue  # Continue with next model on error"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3e158cd5-41b8-46e2-9819-f77d44c87f38"},{"cell_type":"markdown","source":["### Execute the Statistics Collection for All Models"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"240a0322-7bc8-4dd9-8b23-60e7929c9b1d"},{"cell_type":"code","source":["# Execute the main function to process all models defined in the models list.\n","collect_model_statistics(models)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"f4bb0e6a-b53e-44d5-bb6f-69bde5b2e449\",\"activityId\":\"70bb5c8c-7698-4eeb-8837-503f08c96015\",\"applicationId\":\"application_1741620896689_0001\",\"jobGroupId\":\"24\",\"advices\":{\"warn\":20}}"}},"id":"2d4ae15f-3f81-42ef-b051-78b75a2a0795"},{"cell_type":"markdown","source":["### Generate Star Schema\n","### Helper Function for Star Schema: Generate Table Key\n","Used throughout the star schema creation SQL to produce unique keys."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"7ff7213b-d4aa-4e49-8ddb-f99be5009347"},{"cell_type":"code","source":["def generate_table_key(*columns) -> str:\n","    \"\"\"\n","    Generates a SQL expression to produce a unique key from the concatenated values of the given columns.\n","    \n","    It uses IFNULL to replace NULLs, CONCAT to join values, MD5 for hashing,\n","    and CONV to convert the hash to a BIGINT.\n","    \n","    Returns:\n","      A string containing the SQL expression for the unique key.\n","    \"\"\"\n","    # Create IFNULL expressions for each column to avoid NULL issues.\n","    ifnull_parts = [f\"IFNULL({col}, '')\" for col in columns]\n","    # Build the SQL expression.\n","    sql_expr = f\"\"\"\n","        CAST(CONV(\n","            RIGHT(MD5(CONCAT({\", \".join(ifnull_parts)})), 16),\n","            16,\n","            -10\n","        ) AS BIGINT)\n","    \"\"\"\n","    return sql_expr.strip()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c2f49953-1a3e-4256-baba-dbbd0551c57b"},{"cell_type":"markdown","source":["### Create ```DIM_ModelObject```\n","Collects the most recent column, measure, and unused column definitions."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"20cbf4b0-00dd-4e33-8386-1827daaa4433"},{"cell_type":"code","source":["query_result = spark.sql(f\"\"\"\n","    -- Get the latest report measures\n","    WITH latest_report_measures AS (\n","        SELECT\n","            mapping.TableName,\n","            mapping.ObjectName,\n","            mapping.ObjectType,\n","            mapping.ModelObject AS Expression,\n","            '' AS Description,\n","            NULL AS ModifiedDate,\n","            'True' AS DeletedFromModelFlag,\n","            query_count.RunUuid,\n","            query_count.ModelUuid,\n","            query_count.AsOfDate,\n","            query_count.AsOfDateTime\n","        FROM {historical_table_names[\"object_query_count\"]} AS query_count\n","        JOIN (\n","            SELECT\n","                ModelUuid,\n","                ModelObject,\n","                MAX(AsOfDateTime) AS MaxAsOfDateTime\n","            FROM {historical_table_names[\"object_query_count\"]}\n","            GROUP BY ALL\n","        ) AS latest ON\n","            latest.ModelObject = query_count.ModelObject\n","            AND latest.ModelUuid = query_count.ModelUuid\n","            AND latest.MaxAsOfDateTime = query_count.AsOfDateTime\n","        LEFT JOIN {historical_table_names[\"object_mapping\"]} AS mapping ON\n","            mapping.RunUuid = query_count.RunUuid\n","            AND mapping.ModelUuid = query_count.ModelUuid\n","            AND mapping.ObjectType = query_count.ObjectType\n","            AND mapping.ModelObject = query_count.ModelObject\n","        WHERE query_count.ObjectType = 'REPORT MEASURE'\n","    ),\n","\n","    -- Get the latest model columns\n","    latest_model_columns AS (\n","        SELECT\n","            mapping.TableName,\n","            mapping.ObjectName,\n","            mapping.ObjectType,\n","            '' AS Expression,\n","            model_column.Description,\n","            CAST(ModifiedTime AS DATE) AS ModifiedDate,\n","            CASE \n","                WHEN MAX(model_column.AsOfDate) OVER() = model_column.AsOfDate THEN 'False'\n","                ELSE 'True'\n","            END AS DeletedFromModelFlag,\n","            model_column.RunUuid,\n","            model_column.ModelUuid,\n","            model_column.AsOfDate,\n","            model_column.AsOfDateTime\n","        FROM {historical_table_names[\"model_columns\"]} AS model_column\n","        JOIN (\n","            SELECT\n","                ModelUuid,\n","                TableName,\n","                ColumnName,\n","                MAX(AsOfDateTime) AS MaxAsOfDateTime\n","            FROM {historical_table_names[\"model_columns\"]}\n","            GROUP BY ALL\n","        ) AS latest ON\n","            latest.ModelUuid = model_column.ModelUuid\n","            AND latest.TableName = model_column.TableName\n","            AND latest.ColumnName = model_column.ColumnName\n","            AND latest.MaxAsOfDateTime = model_column.AsOfDateTime\n","        LEFT JOIN {historical_table_names[\"object_mapping\"]} AS mapping ON\n","            mapping.RunUuid = model_column.RunUuid\n","            AND mapping.ModelUuid = model_column.ModelUuid\n","            AND mapping.ObjectType = 'COLUMN'\n","            AND mapping.TableName = model_column.TableName\n","            AND mapping.ObjectName = model_column.ColumnName\n","    ),\n","\n","    -- Get the latest model measures\n","    latest_model_measures AS (\n","        SELECT\n","            mapping.TableName,\n","            mapping.ObjectName,\n","            mapping.ObjectType,\n","            model_measure.MeasureExpression AS Expression,\n","            model_measure.MeasureDescription AS Description,\n","            NULL AS ModifiedDate,\n","            CASE \n","                WHEN MAX(model_measure.AsOfDate) OVER() = model_measure.AsOfDate THEN 'False'\n","                ELSE 'True'\n","            END AS DeletedFromModelFlag,\n","            model_measure.RunUuid,\n","            model_measure.ModelUuid,\n","            model_measure.AsOfDate,\n","            model_measure.AsOfDateTime\n","        FROM {historical_table_names[\"model_measures\"]} AS model_measure\n","        JOIN (\n","            SELECT\n","                ModelUuid,\n","                TableName,\n","                MeasureName,\n","                MAX(AsOfDateTime) AS MaxAsOfDateTime\n","            FROM {historical_table_names[\"model_measures\"]}\n","            GROUP BY ALL\n","        ) AS latest ON\n","            latest.ModelUuid = model_measure.ModelUuid\n","            AND latest.TableName = model_measure.TableName\n","            AND latest.MeasureName = model_measure.MeasureName\n","            AND latest.MaxAsOfDateTime = model_measure.AsOfDateTime\n","        LEFT JOIN {historical_table_names[\"object_mapping\"]} AS mapping ON\n","            mapping.ModelUuid = model_measure.ModelUuid\n","            AND mapping.RunUuid = model_measure.RunUuid\n","            AND mapping.ObjectType = 'MEASURE'\n","            AND mapping.TableName = model_measure.TableName\n","            AND mapping.ObjectName = model_measure.MeasureName\n","    ),\n","\n","    -- Get unused columns\n","    latest_unused_columns AS (\n","        SELECT\n","            unused_column.TableName,\n","            IFNULL(model_column_with_mapping.ObjectName, unused_column.SourceColumnName) AS ObjectName,\n","            'COLUMN' AS ObjectType,\n","            '' AS Expression,\n","            model_column_with_mapping.Description,\n","            model_column_with_mapping.ModifiedDate,\n","            'True' AS DeletedFromModelFlag,\n","            unused_column.RunUuid,\n","            unused_column.ModelUuid,\n","            unused_column.AsOfDate,\n","            unused_column.AsOfDateTime\n","        FROM {historical_table_names[\"unused_columns\"]} AS unused_column\n","        LEFT JOIN (\n","            SELECT\n","                ModelUuid,\n","                SourceTableName,\n","                SourceColumnName,\n","                MAX(AsOfDateTime) AS MaxAsOfDateTime\n","            FROM {historical_table_names[\"unused_columns\"]}\n","            GROUP BY ALL\n","        ) AS latest ON\n","            latest.ModelUuid = unused_column.ModelUuid\n","            AND latest.SourceTableName = unused_column.SourceTableName\n","            AND latest.SourceColumnName = unused_column.SourceColumnName\n","            AND latest.MaxAsOfDateTime = unused_column.AsOfDateTime\n","        LEFT JOIN (\n","            SELECT\n","                latest_model_columns.ModelUuid,\n","                latest_model_columns.TableName,\n","                latest_model_columns.ObjectName,\n","                latest_model_columns.Description,\n","                latest_model_columns.ModifiedDate,\n","                source_mapping.SourceTableName,\n","                source_mapping.SourceColumnName,\n","                source_mapping.RunUuid\n","            FROM latest_model_columns\n","            LEFT JOIN {historical_table_names[\"source_mapping\"]} AS source_mapping ON\n","                source_mapping.ModelUuid = latest_model_columns.ModelUuid\n","                AND source_mapping.RunUuid = latest_model_columns.RunUuid\n","                AND source_mapping.TableName = latest_model_columns.TableName\n","                AND source_mapping.ColumnName = latest_model_columns.ObjectName\n","        ) AS model_column_with_mapping ON\n","            model_column_with_mapping.ModelUuid = unused_column.ModelUuid\n","            AND model_column_with_mapping.RunUuid = unused_column.RunUuid\n","            AND model_column_with_mapping.SourceTableName = unused_column.SourceTableName\n","            AND model_column_with_mapping.SourceColumnName = unused_column.SourceColumnName\n","    ),\n","\n","    -- Union all objects\n","    union_all_objects AS (\n","        SELECT * FROM latest_report_measures\n","        UNION\n","        SELECT * FROM latest_model_columns\n","        UNION\n","        SELECT * FROM latest_model_measures\n","        UNION\n","        SELECT * FROM latest_unused_columns\n","    ),\n","\n","    -- Enrich and keep the latest records\n","    keep_latest_record_and_enrich AS (\n","        SELECT\n","            union_all_objects.TableName,\n","            union_all_objects.ObjectName,\n","            union_all_objects.ObjectType,\n","            union_all_objects.Expression,\n","            union_all_objects.Description,\n","            union_all_objects.ModifiedDate,\n","            union_all_objects.DeletedFromModelFlag,\n","            union_all_objects.ModelUuid,\n","            CASE \n","                WHEN MAX(union_all_objects.AsOfDate) OVER() = union_all_objects.AsOfDate THEN 'False'\n","                ELSE 'True'\n","            END AS DeletedFromLakehouseFlag,\n","            {\n","                generate_table_key(\n","                    \"union_all_objects.ModelUuid\",\n","                    \"union_all_objects.TableName\",\n","                    '''\n","                        CASE \n","                            WHEN union_all_objects.ObjectType = 'REPORT MEASURE'\n","                            THEN union_all_objects.Expression\n","                            ELSE union_all_objects.ObjectName\n","                        END\n","                    ''',\n","                )\n","            } AS ModelObjectId\n","        FROM union_all_objects\n","        JOIN (\n","            SELECT\n","                ModelUuid,\n","                TableName,\n","                ObjectName,\n","                Expression,\n","                MAX(AsOfDateTime) AS MaxAsOfDateTime\n","            FROM union_all_objects\n","            GROUP BY ALL\n","        ) AS latest ON\n","            latest.ModelUuid = union_all_objects.ModelUuid\n","            AND latest.TableName = union_all_objects.TableName\n","            AND latest.ObjectName = union_all_objects.ObjectName\n","            AND latest.Expression = union_all_objects.Expression\n","            AND latest.MaxAsOfDateTime = union_all_objects.AsOfDateTime\n","    )\n","    SELECT * FROM keep_latest_record_and_enrich\n","\"\"\")\n","\n","query_result.write.mode(\"overwrite\").format(\"delta\").option(\n","    \"overwriteSchema\", \"true\"\n",").saveAsTable(star_schema_table_names[\"dim_model_object\"])"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"47061cf3-2f39-4f9d-9e1c-3195c3a56abd"},{"cell_type":"markdown","source":["### Create ```DIM_Model```"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"71f59597-9975-4ba1-83ac-e9e5a541c3b1"},{"cell_type":"code","source":["query_result = spark.sql(f\"\"\"\n","    SELECT\n","        models.source_model_workspace_name AS WorkspaceName,\n","        models.source_model_name AS ModelName,\n","        models.ModelUuid AS ModelUuid,\n","        models.source_lakehouse_name AS LakehouseName,\n","        models.source_lakehouse_uuid AS LakehouseUUid,\n","        models.source_lakehouse_workspace_name AS LakehouseWorkspaceName,\n","        models.source_lakehouse_workspace_uuid AS LakehouseWorkspaceUuid\n","    FROM {historical_table_names[\"run_history\"]} AS models\n","    JOIN (\n","        SELECT\n","            ModelUuid,\n","            MAX(AsOfDateTime) AS AsOfDateTime\n","        FROM {historical_table_names[\"run_history\"]}\n","        WHERE Status = 'completed'\n","        GROUP BY ModelUuid\n","    ) AS latest ON\n","        latest.ModelUuid = models.ModelUuid\n","        AND latest.AsOfDateTime = models.AsOfDateTime\n","\"\"\")\n","\n","query_result.write.mode(\"overwrite\").format(\"delta\").option(\n","    \"overwriteSchema\", \"true\"\n",").saveAsTable(star_schema_table_names[\"dim_model\"])"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bb4c06fa-6030-4ffd-885e-998ffc6dd0c3"},{"cell_type":"markdown","source":["### Create ```DIM_Report```"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"15d514d5-2a03-42f7-b489-bf683f642e64"},{"cell_type":"code","source":["if len(report_uuid_mapping) == 1 and report_uuid_mapping[0][\"ReportUuid\"] == \"\" and report_uuid_mapping[0][\"MapToReportUuid\"] == \"\":\n","    query = f\"\"\"\n","        SELECT\n","            CASE\n","                WHEN src_reports.ReportId = '' THEN 'Non-Report'\n","                ELSE src_reports.ReportName\n","            END AS ReportName,\n","            src_reports.WebUrl AS WebUrl,\n","            src_reports.WorkspaceId AS WorkspaceUuid,\n","            src_reports.WorkspaceName AS WorkspaceName,\n","            src_reports.ReportId AS ReportUuid\n","        FROM {historical_table_names[\"source_reports\"]} AS src_reports\n","        JOIN (\n","            SELECT\n","                ReportId,\n","                MAX(AsOfDateTime) AS MaxAsOfDateTime\n","            FROM {historical_table_names[\"source_reports\"]}\n","            GROUP BY ReportId\n","        ) AS latest_reports\n","            ON latest_reports.ReportId = src_reports.ReportId\n","            AND latest_reports.MaxAsOfDateTime = src_reports.AsOfDateTime\n","        UNION ALL\n","        SELECT\n","            'N/A' AS ReportName,\n","            'N/A' AS WebUrl,\n","            'N/A' AS WorkspaceUuid,\n","            'N/A' AS WorkspaceName,\n","            'N/A' AS ReportUuid\n","    \"\"\"\n","else:\n","    spark.createDataFrame(report_uuid_mapping).createOrReplaceTempView(\"report_uuid_mapping\")\n","    query = f\"\"\"\n","        WITH src_reports AS (\n","            SELECT\n","                sr.ReportId,\n","                sr.ReportName,\n","                sr.WebUrl,\n","                sr.WorkspaceId AS WorkspaceUuid,\n","                sr.WorkspaceName,\n","                sr.AsOfDateTime\n","            FROM {historical_table_names[\"source_reports\"]} AS sr\n","            JOIN (\n","                SELECT\n","                    ReportId,\n","                    MAX(AsOfDateTime) AS MaxAsOfDateTime\n","                FROM {historical_table_names[\"source_reports\"]}\n","                GROUP BY ReportId\n","            ) AS latest_reports\n","                ON sr.ReportId = latest_reports.ReportId\n","                AND sr.AsOfDateTime = latest_reports.MaxAsOfDateTime\n","        ),\n","        final_reports AS (\n","            SELECT\n","                src.ReportId AS ReportUuid,\n","                COALESCE(mapping_data.MappedReportName, src.ReportName) AS ReportName,\n","                COALESCE(mapping_data.MappedWebUrl, src.WebUrl) AS WebUrl,\n","                COALESCE(mapping_data.MappedWorkspaceUuid, src.WorkspaceUuid) AS WorkspaceUuid,\n","                COALESCE(mapping_data.MappedWorkspaceName, src.WorkspaceName) AS WorkspaceName\n","            FROM src_reports AS src\n","            LEFT JOIN (\n","                SELECT\n","                    m.ReportUuid AS OldId,\n","                    mapped.ReportName AS MappedReportName,\n","                    mapped.WebUrl AS MappedWebUrl,\n","                    mapped.WorkspaceUuid AS MappedWorkspaceUuid,\n","                    mapped.WorkspaceName AS MappedWorkspaceName\n","                FROM report_uuid_mapping AS m\n","                LEFT JOIN src_reports AS mapped\n","                    ON m.MapToReportUuid = mapped.ReportId\n","            ) AS mapping_data\n","                ON src.ReportId = mapping_data.OldId\n","        )\n","        SELECT\n","            ReportUuid,\n","            CASE WHEN ReportUuid = '' THEN 'Non-Report' ELSE ReportName END AS ReportName,\n","            WebUrl,\n","            WorkspaceUuid,\n","            WorkspaceName\n","        FROM final_reports\n","        UNION ALL\n","        SELECT\n","            'N/A' AS ReportUuid,\n","            'N/A' AS ReportName,\n","            'N/A' AS WebUrl,\n","            'N/A' AS WorkspaceUuid,\n","            'N/A' AS WorkspaceName\n","    \"\"\"\n","    \n","query_result = spark.sql(query)\n","query_result.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(star_schema_table_names[\"dim_report\"])"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d6c91c17-5dca-4a7c-ac12-521c05518854"},{"cell_type":"markdown","source":["### Create ```DIM_User```"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"c66fb43e-12e0-41f7-b654-7fd432c2b640"},{"cell_type":"code","source":["query_result = spark.sql(f\"\"\"\n","    WITH union_data AS (\n","        SELECT\n","            'Masked' AS ExecutingUser,\n","            ExecutingUserGroup\n","        FROM {historical_table_names[\"object_query_count\"]}\n","        UNION ALL\n","        SELECT\n","            ExecutingUser,\n","            ExecutingUserGroup\n","        FROM {historical_table_names[\"detailed_logs\"]}\n","    ),\n","    remove_duplicates_and_add_key AS (\n","        SELECT DISTINCT\n","            ExecutingUser,\n","            ExecutingUserGroup,\n","            {generate_table_key(\"ExecutingUser\", \"ExecutingUserGroup\")} AS UserId\n","        FROM union_data\n","    )\n","    SELECT * FROM remove_duplicates_and_add_key\n","\"\"\")\n","\n","query_result.write.mode(\"overwrite\").format(\"delta\").option(\n","    \"overwriteSchema\", \"true\"\n",").saveAsTable(star_schema_table_names[\"dim_user\"])"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5414e1c-1d6c-4c0e-b96e-ca2a039d01d0"},{"cell_type":"markdown","source":["### Create ```FACT_ModelObjectQueryCount```\n","Maps queries back to model objects, including dependencies."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"8a708f15-2563-4e16-8e3d-78d0f70b3d05"},{"cell_type":"code","source":["query_result = spark.sql(f\"\"\"\n","    -- Map query counts with model objects\n","    WITH query_counts_with_mapping AS (\n","        SELECT\n","            query_count.AsOfDate,\n","            query_count.AsOfHour,\n","            query_count.ExecutingUserGroup,\n","            IFNULL(query_count.ReportId, 'N/A') AS ReportUuid,\n","            query_count.QueryCount,\n","            query_count.RunUuid,\n","            query_count.ObjectType,\n","            query_count.ModelUuid,\n","            query_count.ModelObject,\n","            mapping.TableName,\n","            mapping.ObjectName,\n","            {\n","                generate_table_key(\n","                    \"query_count.ModelUuid\",\n","                    \"mapping.TableName\",\n","                    '''\n","                        CASE \n","                            WHEN query_count.ObjectType = 'REPORT MEASURE'\n","                            THEN query_count.ModelObject\n","                            ELSE mapping.ObjectName\n","                        END\n","                    ''',\n","                )\n","            } AS ModelObjectId\n","        FROM\n","            {historical_table_names[\"object_query_count\"]} AS query_count\n","        LEFT JOIN\n","            {historical_table_names[\"object_mapping\"]} AS mapping ON\n","                mapping.ModelUuid = query_count.ModelUuid\n","                AND mapping.RunUuid = query_count.RunUuid\n","                AND mapping.ModelObject = query_count.ModelObject\n","    ),\n","\n","    -- Identify dependencies\n","    dependencies AS (\n","        SELECT DISTINCT\n","            ModelUuid,\n","            TableName,\n","            ObjectName,\n","            ReferencedTableName,\n","            ReferencedObjectName,\n","            RunUuid,\n","            ObjectType\n","        FROM {historical_table_names[\"dependencies\"]}\n","    ),\n","\n","    -- Join dependencies with query counts\n","    dependencies_join_query_count AS (\n","        SELECT\n","            dependencies.ModelUuid,\n","            query_count.AsOfDate,\n","            query_count.AsOfHour,\n","            {\n","                generate_table_key(\n","                    \"dependencies.ModelUuid\",\n","                    \"dependencies.ReferencedTableName\",\n","                    \"dependencies.ReferencedObjectName\",\n","                )\n","            } AS ModelObjectId,\n","            IFNULL(query_count.ReportUuid, 'N/A') AS ReportUuid,\n","            query_count.ExecutingUserGroup,\n","            query_count.QueryCount\n","        FROM \n","            dependencies\n","        LEFT JOIN\n","            query_counts_with_mapping AS query_count ON\n","                query_count.ModelUuid = dependencies.ModelUuid\n","                AND query_count.RunUuid = dependencies.RunUuid\n","                AND query_count.TableName = dependencies.TableName\n","                AND query_count.ObjectName = dependencies.ObjectName\n","        WHERE\n","            query_count.ObjectName IS NOT NULL\n","    ),\n","\n","    -- Union data to create the final fact table\n","    union_model_and_report_objects AS (\n","        SELECT\n","            ModelUuid,\n","            AsOfDate,\n","            AsOfHour,\n","            ModelObjectId,\n","            ReportUuid,\n","            {generate_table_key(\"'Masked'\", \"ExecutingUserGroup\")} AS UserId,\n","            'True' AS DirectReferenceFlag,\n","            QueryCount\n","        FROM\n","            query_counts_with_mapping\n","        UNION ALL\n","        SELECT\n","            ModelUuid,\n","            AsOfDate,\n","            AsOfHour,\n","            ModelObjectId,\n","            ReportUuid,\n","            {generate_table_key(\"'Masked'\", \"ExecutingUserGroup\")} AS UserId,\n","            'False' AS DirectReferenceFlag,\n","            QueryCount\n","        FROM\n","            dependencies_join_query_count\n","    )\n","\n","    -- Select all records from the final union\n","    SELECT * FROM union_model_and_report_objects\n","\"\"\")\n","\n","query_result.write.mode(\"overwrite\").format(\"delta\").option(\n","    \"overwriteSchema\", \"true\"\n",").saveAsTable(star_schema_table_names[\"fact_model_object_query_count\"])"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"377a8846-e2ca-41ca-b2a5-151034563f01"},{"cell_type":"markdown","source":["### Create ```FACT_ModelLogs```\n","Stores detailed DAX query logs for performance analysis."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"32ebb8fd-f1fc-4d09-aff0-ac310bcc4680"},{"cell_type":"code","source":["query_result = spark.sql(f\"\"\"\n","    SELECT\n","        AsOfDate,\n","        OperationName,\n","        OperationDetailName,\n","        ReportId AS ReportUuid,\n","        ModelUuid,\n","        Timestamp AS DateTime,\n","        {generate_table_key(\"ExecutingUser\", \"ExecutingUserGroup\")} AS UserId,\n","        DurationMs,\n","        CpuTimeMs,\n","        EventText,\n","        OperationId,\n","        XmlaSessionId,\n","        ActivityId,\n","        RequestId,\n","        CurrentActivityId,\n","        StatusCode,\n","        Status\n","    FROM\n","        {historical_table_names[\"detailed_logs\"]} AS query_count\n","\"\"\")\n","\n","query_result.write.mode(\"overwrite\").format(\"delta\").option(\n","    \"overwriteSchema\", \"true\"\n",").saveAsTable(star_schema_table_names[\"fact_detailed_logs\"])"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"195be479-27a9-4a5a-b710-67933751c9b2"},{"cell_type":"markdown","source":["### Create ```FACT_ModelObjectStatistics```\n","Blends cold-cache data, table residency, and table sizes for columns."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"05c3948f-66d0-4e88-94d1-edf76d27cbfd"},{"cell_type":"code","source":["query_result = spark.sql(f\"\"\"\n","    -- Create distinct combinations of object and date\n","    WITH distinct_object_date_combo AS (\n","        SELECT DISTINCT\n","            AsOfDate,\n","            ModelUuid,\n","            TableName,\n","            ColumnName AS ObjectName,\n","            {generate_table_key(\"ModelUuid\", \"TableName\", \"ColumnName\")} AS ModelObjectId\n","        FROM\n","            {historical_table_names[\"model_columns\"]}\n","    ),\n","\n","    -- Map cold cache with object mapping\n","    cold_cache_with_mapping AS (\n","        SELECT\n","            mapping.TableName,\n","            mapping.ObjectName,\n","            cold_cache.ModelUuid,\n","            cold_cache.AsOfDate,\n","            cold_cache.Duration,\n","            cold_cache.CpuTime\n","        FROM\n","            {historical_table_names[\"cold_cache_measurements\"]} AS cold_cache\n","        LEFT JOIN\n","            {historical_table_names[\"object_mapping\"]} AS mapping ON\n","                mapping.RunUuid = cold_cache.RunUuid\n","                AND mapping.ModelObject = cold_cache.ColumnName\n","        WHERE \n","            cold_cache.Success = 'Success'\n","            AND cold_cache.ColumnName IS NOT NULL\n","    ),\n","\n","    -- Join facts and aggregate metrics\n","    join_facts AS (\n","        SELECT\n","            combos.ModelUuid,\n","            combos.AsOfDate,\n","            combos.ModelObjectId,\n","            COUNT(residency.IsResident) AS ColumnResidencyMeasuredCount,\n","            SUM(CASE WHEN residency.IsResident = True THEN 1 ELSE 0 END) AS ColumnResidencyTrueCount,\n","            AVG(data_size.TotalSize) AS TotalSize,\n","            AVG(data_size.DataSize) AS DataSize,\n","            AVG(data_size.DictionarySize) AS DictionarySize,\n","            AVG(data_size.HierarchySize) AS HierarchySize,\n","            AVG(cold_cache.Duration) AS DurationTime,\n","            AVG(cold_cache.CpuTime) AS CpuTime\n","        FROM\n","            distinct_object_date_combo AS combos\n","        LEFT JOIN\n","            {historical_table_names[\"model_columns\"]} AS residency ON\n","                residency.ModelUuid = combos.ModelUuid\n","                AND residency.TableName = combos.TableName\n","                AND residency.ColumnName = combos.ObjectName\n","                AND residency.AsOfDate = combos.AsOfDate\n","        LEFT JOIN\n","            {historical_table_names[\"resident_statistics\"]} AS data_size ON\n","                data_size.ModelUuid = combos.ModelUuid\n","                AND data_size.TableName = combos.TableName\n","                AND data_size.ColumnName = combos.ObjectName\n","                AND data_size.AsOfDate = combos.AsOfDate\n","        LEFT JOIN\n","            cold_cache_with_mapping AS cold_cache ON\n","                cold_cache.ModelUuid = combos.ModelUuid\n","                AND cold_cache.TableName = combos.TableName\n","                AND cold_cache.ObjectName = combos.ObjectName\n","                AND cold_cache.AsOfDate = combos.AsOfDate\n","        GROUP BY ALL\n","    )\n","\n","    -- Select all results from the final join\n","    SELECT * FROM join_facts\n","\"\"\")\n","\n","query_result.write.mode(\"overwrite\").format(\"delta\").option(\n","    \"overwriteSchema\", \"true\"\n",").saveAsTable(star_schema_table_names[\"fact_model_statistics\"])"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"dbd23920-dc4c-4a5b-8d47-7f914d5bce6d"},{"cell_type":"code","source":["mssparkutils.session.stop()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2442ba88-d460-4344-a380-d8380a8a628d"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"state":{},"version":"0.1"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":null,"default_lakehouse_name":"","default_lakehouse_workspace_id":""},"environment":{}}},"nbformat":4,"nbformat_minor":5}