{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d900c-566d-41b7-8426-d101a9b4d883",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#This is a parameters cell.  Any args passed in from the driver notebook will overwrite these defaults\n",
    "\n",
    "xmla_endpoint = None #None = Current Workspace's XMLA Endpoint\n",
    "workspace = \"Fabric Load Test\"\n",
    "perf_analyzer_filename = '/lakehouse/default/Files/PerfScenarios/Queries/PowerBIPerformanceData.json'\n",
    "model = \"DIAD Final Report with RLS\"\n",
    "roles = None  #Used with customdata to force active roles.  Not needed with effective_username\n",
    "customdata = \"foo\" #customdata for use in RLS without impersonation\n",
    "effective_username = None #for RLS with impersionation.  Must be the UPN of a user with read+build for the model\n",
    "iterations = 3  #number of times to run the perf_analyzer_filename in this scenario\n",
    "delay_sec = 1  #number of seconds to wait between iterations\n",
    "loadtestId = \"localtesting\" # name of load test for logging\n",
    "threadId = 1 #id of this virtual user to seperate logging for multiuser-testing\n",
    "concurrent_threads = 1  #number of concurrent threads in this test; passed in here only for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075198b-3a0b-4fb2-8962-45df93973b37",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from typing import Iterable\n",
    "import json\n",
    "import time\n",
    "import sempy.fabric as fabric\n",
    "tom = fabric.create_tom_server() #get CLR loaded\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from Microsoft.AnalysisServices.AdomdClient import AdomdConnection\n",
    "\n",
    "def run_query(con: AdomdConnection, query:str) -> int:\n",
    "    cmd = con.CreateCommand()\n",
    "    cmd.CommandText = query\n",
    "    rows = 0\n",
    "    rdr = cmd.ExecuteReader()\n",
    "    while rdr.Read():\n",
    "        rows=rows+1\n",
    "    rdr.Close()\n",
    "    return rows\n",
    "\n",
    "def load_queries(fn: str) -> list:\n",
    "    queries=[]\n",
    "    # print(f'ppp {fn}')\n",
    "    # print('--------------------------------')\n",
    "    # notebookutils.fs.ls('/lakehouse/default/Files/PerfScenarios/Queries/PowerBIPerformanceData.json')\n",
    "    # print('--------------------------------')\n",
    "\n",
    "    with open(fn, encoding='utf-8-sig') as f:\n",
    "        json_string = f.read()\n",
    "        d = json.loads(json_string)\n",
    "\n",
    "        visual_name=\"\"\n",
    "        query_text = \"\"\n",
    "        for e in d[\"events\"]:\n",
    "            \n",
    "\n",
    "            if e[\"name\"] == \"Visual Container Lifecycle\":\n",
    "                visual_name = e[\"metrics\"][\"visualTitle\"]\n",
    "                visual_id = e[\"metrics\"][\"visualId\"]\n",
    "\n",
    "            if e[\"name\"] == \"Execute DAX Query\":\n",
    "                query_text = e[\"metrics\"][\"QueryText\"]\n",
    "                queries.append({\"visual_name\":visual_name, \"visual_id\":visual_id, \"query_text\":query_text})\n",
    "    return queries\n",
    "\n",
    "def run_perf_scenario(con: AdomdConnection, queries: Iterable[str], i: int) -> list:\n",
    "    results = []\n",
    "    for qn, q in enumerate(queries, start=1):\n",
    "        start = time.time()\n",
    "        query_text = q[\"query_text\"]\n",
    "        visual_name = q[\"visual_name\"]\n",
    "        visual_id = q[\"visual_id\"]\n",
    "        rows = run_query(con, query_text)\n",
    "        duration = time.time() - start\n",
    "        \n",
    "        result = {\n",
    "            \"loadtest_id\": loadtestId,\n",
    "            \"model\": model,\n",
    "            \"concurrent_threads\": concurrent_threads,\n",
    "            \"iterations\": iterations,\n",
    "            \"delay_sec\": delay_sec,\n",
    "            \"query_number\": qn,\n",
    "            \"visual_name\": visual_name,\n",
    "            \"visual_id\": visual_id,\n",
    "            \"iteration\": i,\n",
    "            \"query\": query_text,\n",
    "            \"rows\": rows,\n",
    "            \"duration\": duration,\n",
    "            \"start_time\": start,\n",
    "            \"start_time_dt\": datetime.fromtimestamp(start),\n",
    "            \"customdata\": customdata,\n",
    "            \"effective_username\": effective_username,\n",
    "            \"thread_id\": threadId\n",
    "        }\n",
    "        results.append(result)\n",
    "        time.sleep(delay_sec)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091da2b-bb22-4e5f-8006-8124c345932f",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "import pandas \n",
    "import csv\n",
    "import uuid\n",
    "tom = fabric.create_tom_server() #get CLR loaded\n",
    "from Microsoft.AnalysisServices.AdomdClient import AdomdConnection \n",
    "\n",
    "\n",
    "token = notebookutils.credentials.getToken(\"pbi\")\n",
    "notebookutils.fs.mkdirs(f'/lakehouse/default/Files/PerfScenarios/logs/{loadtestId}') #This should already exist, but useful here when running in standalong mode\n",
    "\n",
    "# if (xmla_endpoint == None):\n",
    "#     xmla_endpoint = f\"powerbi://api.powerbi.com/v1.0/myorg/{notebookutils.runtime.context['currentWorkspaceName']}\"\n",
    "xmla_endpoint = f\"powerbi://api.powerbi.com/v1.0/myorg/{workspace}\"\n",
    "\n",
    "\n",
    "constr = f\"Data Source={xmla_endpoint};Initial Catalog={model};password={token};Timeout=7200;\"\n",
    "if (effective_username != None):\n",
    "    constr = constr + f\"EffectiveUserName={effective_username};\"\n",
    "if (customdata != None):\n",
    "    constr = constr + f\"CustomData={customdata};\"\n",
    "    # constr = constr + f\"CustomData=;\"\n",
    "if (roles != None):\n",
    "     constr = constr + f\"Roles={roles};\"\n",
    "\n",
    "print(f\"The connection string is :{constr}\")\n",
    "con = AdomdConnection(constr)\n",
    "try:\n",
    "    con.Open()\n",
    "\n",
    "    queries = load_queries(perf_analyzer_filename)\n",
    "\n",
    "    all_results = []\n",
    "    print(f\"starting {iterations} iterations with {delay_sec} think time between\")\n",
    "    for i in range(iterations):\n",
    "        #print(f\"Starting iteration {i}\")\n",
    "        results:list=run_perf_scenario(con,queries,i)\n",
    "        if (i<iterations):\n",
    "            print(f\"Completed iteration, starting {delay_sec}sec think time\")\n",
    "            time.sleep(delay_sec)\n",
    "        all_results += results\n",
    "\n",
    "    \n",
    "    # Write results to CSV files in lakehouse for analyzing later\n",
    "    filename = f\"{loadtestId}_user_{threadId}\"\n",
    "    with open(f\"/lakehouse/default/Files/PerfScenarios/logs/{loadtestId}/{filename}.csv\", 'w') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=all_results[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_results)\n",
    "\n",
    "    con.Close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    with open(f\"/lakehouse/default/Files/PerfScenarios/logs/{loadtestId}/error.log\", 'a') as file:\n",
    "         file.write(f'{constr}  : {e}\\n')\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    },
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
