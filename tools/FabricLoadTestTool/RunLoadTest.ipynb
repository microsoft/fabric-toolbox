{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7e11c3-ae97-470b-8117-c2aa73ba1548",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "To run with higer concurrency you'll need to scale up the VM.  By default it runs with 4 cores.  To add a code cell below this that looks like:\n",
    "\n",
    "```\n",
    "%%configure -f\n",
    "{\n",
    "    \"vCores\": 16,// Recommended values: [4, 8, 16, 32, 64], Fabric will allocate matched memory according to the specified vCores.\n",
    "}\n",
    "```\n",
    "\n",
    "Note that this will increase the notebook startup time as a VM will need to be spun-up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29649ef-f374-4e4d-afa4-d9881c0837ec",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"vCores\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d30be-21a7-428d-84aa-25ca26d6ac3f",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import sempy.fabric\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import os\n",
    "from deltalake import DeltaTable\n",
    "from deltalake.writer import write_deltalake\n",
    "import sempy\n",
    "import time \n",
    "\n",
    "\n",
    "\n",
    "load_test_name = \"<load test name here>\"                                                    #name of test for logging\n",
    "dataset = \"<semantic model name here>\"                                                      #name of the semantic model to test\n",
    "workspace = None                                                           #workspace containing the semantic model to test \"None\" for the current workspace\n",
    "queryfile = \"/lakehouse/default/Files/PerfScenarios/Queries/PowerBIPerformanceData.json\"    #Power BI Desktop performance analyzer file containing the use case to test\n",
    "concurrent_threads = 6                                                                      #number of concurrent threads to test\n",
    "delay_sec = 4                                                                               #number of seconds each user waits between iterations of the use case\n",
    "iterations = 1                                                                              #number of iterations run by each user\n",
    "\n",
    "workspace = workspace if workspace is not None else notebookutils.runtime.context['currentWorkspaceName']\n",
    "\n",
    "if 'Collapse Here':\n",
    "    ts=time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    loadtestId:str = f\"{load_test_name}-{ts}\"\n",
    "    print(loadtestId)\n",
    "\n",
    "    lakehouseId = sempy.fabric.get_lakehouse_id()\n",
    "    workspaceId = sempy.fabric.get_workspace_id()\n",
    "\n",
    "    # Path for storing CSV files during run\n",
    "    folder_path = f\"/lakehouse/default/Files/PerfScenarios/logs/{loadtestId}\"\n",
    "    # Used for writing combined logs\n",
    "\n",
    "\n",
    "    #Create folder dedicated to logs from this run only\n",
    "    notebookutils.fs.mkdirs(folder_path)\n",
    "\n",
    "\n",
    "    users = []      # example when populated could be \n",
    "                    # \n",
    "                    # users = [\"user1@domain.com\",\"user@domain.com\",\"user3@domain.com\"]\n",
    "                \n",
    "\n",
    "    args = { \n",
    "        \"xmla_endpoint\" : None, #defaults to current workspace's endpoint\n",
    "        \"workspace\" : workspace,\n",
    "        \"perf_analyzer_filename\" : queryfile , \n",
    "        \"model\" : dataset,\n",
    "        \"roles\" : None,\n",
    "        \"customdata\" : None,\n",
    "        \"effective_username\" : None,\n",
    "        \"iterations\" : iterations,\n",
    "        \"delay_sec\" : 1,\n",
    "        \"loadtestId\" : loadtestId ,\n",
    "        \"threadId\" : 0 ,\n",
    "        \"concurrent_threads\" : concurrent_threads,\n",
    "        \"useRootDefaultLakehouse\": True\n",
    "    }\n",
    "\n",
    "    activity =  {\n",
    "                \"name\": \"RunPerfScenario\", # activity name, must be unique\n",
    "                \"path\": \"RunPerfScenario\", # notebook path\n",
    "                \"timeoutPerCellInSeconds\": 9000, # max timeout for each cell, default to 90 seconds\n",
    "                \"args\": {\"param1\": \"value1\"}, # notebook parameters\n",
    "                \"workspace\": None, # workspace name, default to current workspace\n",
    "                \"retry\": 0, # max retry times, default to 0\n",
    "                \"retryIntervalInSeconds\": 0, # retry interval, default to 0 seconds\n",
    "                \"dependencies\": [] # list of activity names that this activity depends on\n",
    "            }\n",
    "\n",
    "\n",
    "    DAG = {\n",
    "        \"activities\": [],\n",
    "        \"timeoutInSeconds\": 43200, # max timeout for the entire pipeline, default to 12 hours\n",
    "        \"concurrency\": 25 # max number of notebooks to run concurrently, default to 25\n",
    "    }\n",
    "    DAG[\"concurrency\"] = concurrent_threads\n",
    "\n",
    "    for i in range(concurrent_threads):\n",
    "        a = activity.copy()\n",
    "        a[\"name\"] = f\"{a['name']}_{i}\"\n",
    "        this_args = args.copy()\n",
    "        if (len(users)>0):\n",
    "            user = users[i%len(users)]\n",
    "            this_args[\"effective_username\"] = user \n",
    "            #this_args[\"customdata\"] = user\n",
    "\n",
    "        this_args[\"threadId\"] = i\n",
    "        a[\"args\"] = this_args\n",
    "\n",
    "        DAG[\"activities\"].append(a)\n",
    "        \n",
    "    # print(DAG)\n",
    "    print('running load test')\n",
    "    try:\n",
    "\n",
    "        ####################################\n",
    "        ##  Run the parallel threads here ##\n",
    "        ####################################\n",
    "\n",
    "        results = notebookutils.notebook.runMultiple(DAG) \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        first_result = results[list(e.result)[0]]\n",
    "        print('Notebook failed.  First Result Exception: ')\n",
    "        print(first_result.get('exception',None))\n",
    "    print('load test complete')\n",
    "\n",
    "    # Consolodate CSV log files of results into single dataframe\n",
    "    df_list = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            polars_df = pl.read_csv(file_path)\n",
    "            # Need to store Time using format that will work for Direct Lake\n",
    "\n",
    "            polars_df = (polars_df\n",
    "                .with_columns([\n",
    "                    pl.col(\"start_time_dt\").str.to_datetime(time_unit=\"ms\").dt.truncate(\"1s\").alias(\"start_time_s\").dt.replace_time_zone(time_zone=\"UTC\")\n",
    "                    ])\n",
    "                )\n",
    "\n",
    "            polars_df = (polars_df\n",
    "                .with_columns([\n",
    "                    pl.col(\"start_time_dt\").str.to_datetime(time_unit=\"ms\").alias(\"start_time_dt\").dt.replace_time_zone(time_zone=\"UTC\")\n",
    "                    ])\n",
    "                )\n",
    "\n",
    "            df_list.append(polars_df)\n",
    "\n",
    "    combined_df = pl.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04a7c8-b884-4d4e-8622-548484c2f3a5",
   "metadata": {
    "jupyterDisplayViewState": {
     "chartOptions": {
      "aggregationType": "sum",
      "binsNumber": 10,
      "categoryFieldKeys": [],
      "chartType": "bar",
      "isStacked": false,
      "seriesFieldKeys": [],
      "wordFrequency": "-1"
     },
     "tableOptions": {},
     "viewOptionsGroup": [
      {
       "tabItems": [
        {
         "key": "0",
         "name": "Table",
         "options": {},
         "type": "table"
        },
        {
         "key": "2-4hxp",
         "name": "Chart 2",
         "options": {
          "aggregationType": "avg",
          "binsNumber": 10,
          "categoryFieldKeys": [
           "12",
           "17"
          ],
          "chartType": "line",
          "isStacked": false,
          "seriesFieldKeys": [
           "10",
           "18"
          ],
          "showDataLabels": true,
          "subtitle": "Description of the chart",
          "theme": "default",
          "title": "Avg of start_time by visual_name",
          "wordFrequency": "-1",
          "xAxisStyle": {
           "scale": "category"
          }
         },
         "type": "chart"
        }
       ]
      }
     ]
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "combined_df = combined_df.with_columns(pl.col(\"start_time_s\").cast(pl.Datetime).alias(\"start_time_s_dt\"))\n",
    "\n",
    "\n",
    "combined_df = combined_df.with_columns(\n",
    "    combined_df.group_by(\"start_time_s_dt\")\n",
    "      .agg(pl.col(\"duration\").mean().alias(\"avg1\"))\n",
    "      .join(combined_df, on=\"start_time_s_dt\")\n",
    "      .select(\"avg1\")\n",
    ")\n",
    "\n",
    "combined_df = combined_df.with_columns(\n",
    "    combined_df.group_by(\"start_time_s_dt\")\n",
    "      .agg(pl.col(\"duration\").quantile(0.9).alias(\"p90\"))\n",
    "      .join(combined_df, on=\"start_time_s_dt\")\n",
    "      .select(\"p90\")\n",
    ")\n",
    "\n",
    "combined_df = combined_df.with_columns(\n",
    "    combined_df.group_by(\"start_time_s_dt\")\n",
    "      .agg(pl.col(\"duration\").quantile(0.5).alias(\"p50\"))\n",
    "      .join(combined_df, on=\"start_time_s_dt\")\n",
    "      .select(\"p50\")\n",
    ")\n",
    "\n",
    "display(combined_df)\n"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
