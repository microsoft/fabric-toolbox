{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85f5796-2251-4ba1-aca8-a37f559ca989",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# <span style=\"color: blue;\">**Dataflow Gen 2 Migration Accelerator**</span>\n",
    "\n",
    "## About the solution\n",
    "This solution leverages the new Save As Fabric REST API for Dataflow Gen 2 along with a Fabric notebook to accelerate migration of Gen 1 Dataflows to Gen 2.\n",
    "\n",
    "To use it:\n",
    "- Download the ipynb file and import it into a new Fabric Workspace\n",
    "- Run the notebook. This will:\n",
    "\t- create a Lakehouse to store migration files\n",
    "\t- deploy notebooks and a semantic model/report to the workspace\n",
    "\t- Note: this notebook will access this Fabric Toolbox folder to read in the json file with the item definitions. If you are not able to access the internet from your notebook, please download the json file and update the path accordingly.\n",
    "- If desired, run the \"Dataflow Inventory\" notebook to get information on the Dataflows in your tenant\n",
    "\t- View the Inventory Report to decide which Dataflows to migrate\n",
    "- Use the \"Create DFG2s from DFG1s\" notebook to automatically create Gen 2 Dataflows from your Gen 1 Dataflows\n",
    "\t- Use the appropriate cell based on your migration approach\n",
    "\t\t- Add any additional code to filter to a subset of dataflows if the default scope is too broad\n",
    "- Note: The Save As REST API will maintain the connections and refresh schedule from your Gen 1 Dataflows but any incremental refresh settings will need to be recreated prior to initial refresh\n",
    "- Check back for later versions of this tool, as additional notebooks are planned to help update downstream items with the GUIDs from the newly created Gen 2 Dataflows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fe170-cde5-4e91-a993-6a70d8ebc046",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import sempy.fabric as fabric\n",
    "from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "try:\n",
    "    import sempy_labs as labs\n",
    "    print('labs already installed')\n",
    "except:\n",
    "    print('installing labs')\n",
    "    %pip install semantic-link-labs\n",
    "    import sempy_labs as labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c479f-0618-4bfb-bf1c-80425d392be4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "newids = {\n",
    "    'Workspace_GUID': '',\n",
    "    'Lakehouse_GUID': '',\n",
    "    'Notebook_1 Dataflow Inventory_GUID': '',\n",
    "    'Notebook_4 Create DFG2s from DFG1s': '', \n",
    "    'SemanticModel_2 Inventory Model_GUID': '',\n",
    "    'Report_3 Inventory Report_GUID': '',\n",
    "    'Notebook_5 Create or Update Downstream Items_GUID': '', \n",
    "    'Notebook_6 Create DFG2 Model from Import Model_GUID': '', \n",
    "    'Lakehouse_SQLEndpoint': '',\n",
    "    'Lakehouse_DatabaseId': '',\n",
    "    'OneLakeRegionPrefix': ''\n",
    "}\n",
    "thisworkspaceid = spark.conf.get(\"trident.workspace.id\")\n",
    "newids['Workspace_GUID'] = thisworkspaceid\n",
    "\n",
    "newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d9078-6da7-42ce-8a6c-74ba4e7176d1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Create DF Market Lakehouse\n",
    "access_token = notebookutils.credentials.getToken(\"pbi\")\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
    "            \"Content-Type\": \"application/json\"}\n",
    "url = f\"https://api.fabric.microsoft.com/v1/workspaces/{thisworkspaceid}/lakehouses\"\n",
    "body = {\n",
    "  \"displayName\": \"DFG2_Migration\",\n",
    "  \"description\": \"Lakehouse for Dataflow Gen 2 Migration Data\"\n",
    "}\n",
    "response = requests.post(url, headers=headers, json=body)\n",
    "jsonresponse = response.json()\n",
    "print(jsonresponse)\n",
    "lakehouseid = jsonresponse['id']\n",
    "\n",
    "# Add new LH id to newids\n",
    "newids['Lakehouse_GUID'] = lakehouseid\n",
    "newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8aeda-311a-4581-8af1-1c76fb22cdec",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Get Lakehouse SQL Endpoint\n",
    "time.sleep(60) #gives time to create lakehouse and sql endpoint if \"Run All\" is used. Comment it out if you run each cell manually and repeat it until you see sqlendpoint and databaseid values in the output.\n",
    "access_token = notebookutils.credentials.getToken(\"pbi\")\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
    "            \"Content-Type\": \"application/json\"}\n",
    "url = f\"https://api.fabric.microsoft.com/v1/workspaces/{thisworkspaceid}/lakehouses/{lakehouseid}\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "jsonresponse = response.json()\n",
    "# print(jsonresponse)\n",
    "\n",
    "# Add new LH info to newids\n",
    "newids['Lakehouse_SQLEndpoint'] = jsonresponse['properties']['sqlEndpointProperties']['connectionString']\n",
    "newids['Lakehouse_DatabaseId'] = jsonresponse['properties']['sqlEndpointProperties']['id']\n",
    "newids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ab4cd-e063-4263-ac2b-f6be6665e796",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### *Note - Sometimes the SQL Endpoint generation takes longer than the time.sleep wait time. Make sure the sql endpoint and datamart id values are populated in the output of the cell above before continuing. Wait 10-20s and rerun the cell until you see values populated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086459f4-08ce-47ee-b1d5-2d858c628fe3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Get Lakehouse Region Prefix\n",
    "onelakeblob = fabric.FabricRestClient().get(f\"/v1/workspaces/{thisworkspaceid}\").json()['oneLakeEndpoints']['blobEndpoint']\n",
    "regionprefix = onelakeblob.split(\"//\")[1].split(\"onelake\")[0]\n",
    "newids['OneLakeRegionPrefix'] = regionprefix\n",
    "regionprefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e87b3-f2f0-48e0-a48b-976f9bcbfaf6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/microsoft/fabric-toolbox/refs/heads/main/accelerators/DFG2-migration-accelerator/DFG2_Migration_Items.json\"\n",
    "# url = \"https://raw.githubusercontent.com/hoosierbi/fileshare/refs/heads/main/DFG2_Migration_Accelarator/DFG2_Migration_Items.json\" # backup storage location\n",
    "deployjson = requests.get(url).text\n",
    "deploy_df = pd.read_json(deployjson)\n",
    "deploy_df['ReplaceString'] = deploy_df['type'] + '_' + deploy_df['displayName'].replace(' ', '_') + '_GUID'\n",
    "deploy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d3256-5e26-41f1-bc8f-3096880bd16c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Define Functions\n",
    "def tobase64(textstring):\n",
    "    textstring_bytes = textstring.encode(\"ascii\")\n",
    "    ascii_bytes = base64.b64encode(textstring_bytes)\n",
    "    base64_output = ascii_bytes.decode(\"ascii\")\n",
    "    return base64_output\n",
    "\n",
    "def convertpayloadstobase64(definitionjson):\n",
    "    asjson = json.loads(definitionjson)\n",
    "    for load in asjson['parts']:\n",
    "        load['payload'] = tobase64(load['payload'])\n",
    "    return asjson\n",
    "\n",
    "def ReplaceGUIDs(defnstring):\n",
    "    jsonstring = defnstring # json.dumps(defnstring)\n",
    "    for guid1 in newids.keys():\n",
    "        jsonstring = jsonstring.replace(guid1, newids[guid1])\n",
    "    return jsonstring\n",
    "\n",
    "\n",
    "# Create Item Function\n",
    "\n",
    "def CreateItemFromDefinition(wsid, itemname, itemtype, itemdefinition):\n",
    "    access_token = notebookutils.credentials.getToken(\"pbi\")\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
    "                \"Content-Type\": \"application/json\"}\n",
    "    workspaceId = wsid     \n",
    "    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/items\"\n",
    "    body = {\n",
    "        \"displayName\": itemname, \n",
    "        \"type\": itemtype, \n",
    "        \"definition\": itemdefinition\n",
    "     }  \n",
    "    response = requests.post(url, headers=headers, json = body)\n",
    "    # return response.json()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655d042-8282-48c6-9f0c-ac143a02752b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "deploylist =  [\n",
    "    'Notebook_1 Dataflow Inventory_GUID'\n",
    "    ,'Notebook_4 Create DFG2s from DFG1s_GUID'\n",
    "    ,'SemanticModel_2 Inventory Model_GUID'\n",
    "    ,'Report_3 Inventory Report_GUID'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "access_token = notebookutils.credentials.getToken(\"pbi\")\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
    "            \"Content-Type\": \"application/json\"}\n",
    "\n",
    "for replacestring in deploylist:\n",
    "    itemrecord = deploy_df[deploy_df['ReplaceString'] == replacestring]\n",
    "    definitionstring = itemrecord.iloc[0]['Definition']\n",
    "    convertedstring = convertpayloadstobase64(ReplaceGUIDs(definitionstring))\n",
    "    createitem = CreateItemFromDefinition(thisworkspaceid, itemrecord.iloc[0]['displayName'], itemrecord.iloc[0]['type'], convertedstring)\n",
    "    # createitem = CreateItemFromDefinition(thisworkspaceid, 'SMtest', itemrecord.iloc[0]['type'], convertedstring) # for troubleshooting\n",
    "\n",
    "    print(createitem.status_code)\n",
    "\n",
    "    if createitem.status_code in { 200, 201 }:\n",
    "        newitemid = createitem.json()['id']\n",
    "        newids[replacestring] = newitemid\n",
    "        print(replacestring + \" - \" + newitemid)\n",
    "\n",
    "    elif createitem.status_code==202:\n",
    "        while True:\n",
    "            url = createitem.headers[\"Location\"]\n",
    "            retry_after = createitem.headers.get(\"Retry-After\",0)\n",
    "            time.sleep(int(retry_after))\n",
    "\n",
    "            headers = {\"Authorization\": f\"Bearer {access_token}\" }\n",
    "            createitem = requests.get(url, headers=headers)\n",
    "            createitem.raise_for_status()\n",
    "\n",
    "            body = createitem.json()\n",
    "            status = body[\"status\"]\n",
    "            if status == \"Succeeded\":\n",
    "                url = createitem.headers[\"Location\"]\n",
    "                createitem = requests.get(url,headers=headers)\n",
    "                newitemid = createitem.json()['id']\n",
    "                newids[replacestring] = newitemid\n",
    "                print(replacestring + \" - \" + newitemid)\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
