{"cells":[{"cell_type":"markdown","source":["****Attach lakehouse where you want to create your tables manually on the left by clicking \"Add data items\"****"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea36ce48-f12f-4868-833c-1fb7c1a6a5f7"},{"cell_type":"code","source":["# -----------------------------\n","# User configuration (edit these)\n","WORKSPACE_NAME        = \"workspacename\"\n","SEMANTIC_MODEL_NAME   = \"semantic model name\"\n","LAKEHOUSE_DATABASE    = \"lakehousename.dbo\"  # use just lakehouse name if there is no schema\n","INCLUDE_TABLES         = []\n","#INCLUDE_TABLES        = [\"table1\"]  # [] = discover all tables; or e.g., [\"dim_customer\", \"fact_sales\"]\n","\n","\n","# Optional mappings (Semantic Model â†’ Lakehouse)\n","TABLE_NAME_MAP = {\n","    # \"Dim Customer\": \"dim_customer\",\n","    # \"Fact Sales\":   \"fact_sales\"\n","}\n","COLUMN_NAME_MAP = { \n","    # (\"Dim Customer\",\"CustomerId\"): \"cust_id\",\n","    # (\"Dim Customer\",\"Customer Name\"): \"cust_name\"\n","}\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d65f402a-b673-4200-9e5b-f384483b3f29"},{"cell_type":"code","source":["%pip install semantic-link-labs --quiet"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[3,4,5,6,7],"state":"finished","livy_statement_state":"available","session_id":"4ea5aeea-e5f9-4ab0-8ff7-40e63d63e9f6","normalized_state":"finished","queued_time":"2025-12-08T20:36:24.2614866Z","session_start_time":"2025-12-08T20:36:24.2617915Z","execution_start_time":"2025-12-08T20:36:37.4251134Z","execution_finish_time":"2025-12-08T20:36:59.2722019Z","parent_msg_id":"10a2e9ec-f33b-49ef-b967-69ab92687cb0"},"text/plain":"StatementMeta(, 4ea5aeea-e5f9-4ab0-8ff7-40e63d63e9f6, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\nNote: you may need to restart the kernel to use updated packages.\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false}},"id":"abae4617-f9e7-4ac0-926f-dd8300b2aa1f"},{"cell_type":"code","source":["#imports & config\n","from pyspark.sql import functions as F\n","from pyspark.sql import types as T\n","import json\n","\n","from sempy_labs.tom import connect_semantic_model\n","\n","\n","# -----------------------------\n","# Helper: resolve semantic model names\n","def map_table_name(sm_table: str) -> str:\n","    return TABLE_NAME_MAP.get(sm_table, sm_table)\n","\n","def map_column_name(sm_table: str, sm_col: str) -> str:\n","    return COLUMN_NAME_MAP.get((sm_table, sm_col), sm_col)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"4ea5aeea-e5f9-4ab0-8ff7-40e63d63e9f6","normalized_state":"finished","queued_time":"2025-12-08T20:44:00.6264055Z","session_start_time":null,"execution_start_time":"2025-12-08T20:44:00.6276064Z","execution_finish_time":"2025-12-08T20:44:01.1422089Z","parent_msg_id":"eabb6227-b577-45e7-9b9b-0adb0b6fd972"},"text/plain":"StatementMeta(, 4ea5aeea-e5f9-4ab0-8ff7-40e63d63e9f6, 16, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0ffd9766-816f-45a5-b665-40802dc7ff6c"},{"cell_type":"code","source":["# harvest Lakehouse comments (table + column)\n","def get_table_and_column_comments(database: str, table_name: str):\n","    \"\"\"\n","    Returns:\n","      table_comment: str or \"\"\n","      col_comment_map: dict[col_name] = comment_str\n","    \"\"\"\n","    # DESCRIBE TABLE EXTENDED gives rows that include column details and properties; we filter for what we need.\n","    df = spark.sql(f\"DESCRIBE TABLE EXTENDED {database}.{table_name}\")\n","    df = df.fillna(\"\")  # avoid None comparisons\n","\n","    # Column-level comments are in rows where 'col_name' is a column and 'comment' holds text.\n","    # On Fabric/Spark, schema typical columns: col_name, data_type, comment\n","    cols_df = df.where((F.col(\"col_name\") != \"\") & (F.col(\"data_type\") != \"\")).select(\"col_name\", \"comment\")\n","\n","    col_comment_map = {}\n","    for r in cols_df.collect():\n","        c = (r[\"comment\"] or \"\").strip()\n","        if c:\n","            col_comment_map[r[\"col_name\"]] = c\n","\n","    # Table-level comment lives in a property row where col_name == 'Comment' (varies by runtime)\n","    tbl_comment_row = df.where(F.col(\"col_name\") == \"Comment\").select(\"data_type\").limit(1).collect()\n","    table_comment = (tbl_comment_row[0][\"data_type\"].strip() if tbl_comment_row else \"\")\n","\n","    return table_comment, col_comment_map\n","\n","# Discover tables in database (or use INCLUDE_TABLES)\n","if INCLUDE_TABLES:\n","    lakehouse_tables = INCLUDE_TABLES\n","else:\n","    show_df = spark.sql(f\"SHOW TABLES IN {LAKEHOUSE_DATABASE}\")\n","    lakehouse_tables = [r[\"tableName\"] for r in show_df.collect()]\n","\n","print(f\"Found {len(lakehouse_tables)} Lakehouse tables\")\n","print(lakehouse_tables)\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"4ea5aeea-e5f9-4ab0-8ff7-40e63d63e9f6","normalized_state":"finished","queued_time":"2025-12-08T20:44:18.4664884Z","session_start_time":null,"execution_start_time":"2025-12-08T20:44:18.4676185Z","execution_finish_time":"2025-12-08T20:44:19.3225496Z","parent_msg_id":"3a2957bb-4f79-415e-8eb3-6e91b44af233"},"text/plain":"StatementMeta(, 4ea5aeea-e5f9-4ab0-8ff7-40e63d63e9f6, 18, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Found 7 Lakehouse tables\n['accident', 'adjuster', 'claim', 'driver_telemetry_data', 'policy', 'policyholder', 'vehicle']\n"]}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"51dcb7f8-3e42-4998-89e5-1b8e10e936ea"},{"cell_type":"code","source":["updates_applied = []\n","\n","# Loop through semantic model tables and columns\n","with connect_semantic_model(dataset=SEMANTIC_MODEL_NAME, workspace=WORKSPACE_NAME, readonly=False) as tom:\n","    for t in tom.model.Tables:\n","        lh_table = map_table_name(t.Name)\n","        #display(lh_table)\n","        \n","        #check if table is present in the config list\n","        if lh_table in lakehouse_tables:\n","\n","            #Fetch the table and column comments from the lakehouse\n","            table_comment, col_comment_map = get_table_and_column_comments(LAKEHOUSE_DATABASE, lh_table)\n","\n","            # Update table description\n","            if table_comment:\n","                t.Description = table_comment\n","                updates_applied.append((\"TABLE\", t.Name, table_comment))\n","            \n","                #Loop through columns\n","                for c in t.Columns:\n","                    lh_col = map_column_name(t.Name, c.Name)\n","                    #display(lh_col)\n","\n","                    # Update column descriptions\n","                    if lh_col in col_comment_map:\n","                        col_description = col_comment_map.get(lh_col,\"\")\n","                        c.Description = col_description\n","                        updates_applied.append((\"COLUMN\", f\"{t.Name}.{c.Name}\", col_description))\n","\n","tables_updated = [\n","    item[1]  # Get the table name (index 1)\n","    for item in updates_applied\n","    if item[0] == \"TABLE\"  # Check if the type (index 0) is 'TABLE'\n","]\n","\n","print(f\"Comments applied to these tables in the semantic model: '{SEMANTIC_MODEL_NAME}' : {tables_updated}\")\n","\n","columns_updated = [\n","    item[1]  # Get the table name (index 1)\n","    for item in updates_applied\n","    if item[0] == \"COLUMN\"  # Check if the type (index 0) is 'TABLE'\n","]\n","\n","print(f\"Comments applied to these columns in the semantic model: '{SEMANTIC_MODEL_NAME}' : {columns_updated}\")\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b8632c3-d68c-4b72-8e17-fd925dbe9b25"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}