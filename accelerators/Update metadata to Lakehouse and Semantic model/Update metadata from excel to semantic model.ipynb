{"cells":[{"cell_type":"markdown","source":["****Attach lakehouse where you want to create your tables manually on the left by clicking \"Add data items\"****"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78838dad-0beb-4f72-b728-9d1a0e42a0af"},{"cell_type":"code","source":["# -----------------------------\n","# User configuration (edit these)\n","WORKSPACE_NAME        = \"<workspacename>\"\n","SEMANTIC_MODEL_NAME   = \"<semantic model>\"\n","EXCEL_FILE_PATH       = \"abfss://<workspacename>@onelake.dfs.fabric.microsoft.com/<lakehousename>.Lakehouse/Files/<path>/sample_data_catalog.xlsx\"\n","\n","# Optional: Filter specific tables (empty list = all tables from Excel)\n","INCLUDE_TABLES = []  # e.g., [\"businessunitreference\", \"portfolio\"]"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5faf5ed1-a8d2-4906-8a69-c5cd2d204d1a"},{"cell_type":"code","source":["%pip install semantic-link-labs --quiet"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[3,4,5,6,7],"state":"finished","livy_statement_state":"available","session_id":"0bdfba23-0cec-47ef-90aa-50fdb43b8841","normalized_state":"finished","queued_time":"2025-12-08T20:49:07.2022091Z","session_start_time":"2025-12-08T20:49:07.2025641Z","execution_start_time":"2025-12-08T20:49:19.867766Z","execution_finish_time":"2025-12-08T20:49:39.7578889Z","parent_msg_id":"bdfd15d6-d207-45df-b99b-8290327807d0"},"text/plain":"StatementMeta(, 0bdfba23-0cec-47ef-90aa-50fdb43b8841, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false}},"id":"abae4617-f9e7-4ac0-926f-dd8300b2aa1f"},{"cell_type":"code","source":["#imports & config\n","from pyspark.sql import functions as F\n","from pyspark.sql import types as T\n","import json\n","import pandas as pd\n","from collections import defaultdict\n","\n","from sempy_labs.tom import connect_semantic_model\n","\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"0bdfba23-0cec-47ef-90aa-50fdb43b8841","normalized_state":"finished","queued_time":"2025-12-08T20:49:47.2433603Z","session_start_time":null,"execution_start_time":"2025-12-08T20:49:53.0902304Z","execution_finish_time":"2025-12-08T20:49:56.5351852Z","parent_msg_id":"7592c051-6a39-4a3d-a6d2-54aadb9a8f69"},"text/plain":"StatementMeta(, 0bdfba23-0cec-47ef-90aa-50fdb43b8841, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0ffd9766-816f-45a5-b665-40802dc7ff6c"},{"cell_type":"code","source":["# -----------------------------\n","# Load and process Excel file\n","print(\"Loading Excel file...\")\n","df_excel = pd.read_excel(EXCEL_FILE_PATH)\n","df_excel = df_excel.replace({r'\\r\\n|\\n|\\r': ' '}, regex=True)\n","\n","# Convert to Spark DataFrame\n","df_spark = spark.createDataFrame(df_excel)\n","df_spark = df_spark.select(\"Logical Table Name\", \"Logical Field Name\", \"Data Team Definition\").dropDuplicates()\n","\n","print(f\"Loaded {df_spark.count()} rows from Excel\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"0bdfba23-0cec-47ef-90aa-50fdb43b8841","normalized_state":"finished","queued_time":"2025-12-08T20:50:16.3797802Z","session_start_time":null,"execution_start_time":"2025-12-08T20:50:16.3809335Z","execution_finish_time":"2025-12-08T20:50:22.5404435Z","parent_msg_id":"458ef0c4-f735-42bc-a5c8-8333ec1cca1e"},"text/plain":"StatementMeta(, 0bdfba23-0cec-47ef-90aa-50fdb43b8841, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading Excel file...\nLoaded 65 rows from Excel\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3d2b892-fe42-47e5-9ccd-5112a308bb95"},{"cell_type":"code","source":["# Group metadata by table and column\n","metadata_map = {}\n","\n","rows = df_spark.collect()\n","for row in rows:\n","    table_name = row['Logical Table Name']\n","    field_name = row['Logical Field Name']\n","    description = row['Data Team Definition']\n","    \n","    if table_name is None or field_name is None:\n","        continue\n","    \n","    table_name = str(table_name).strip().lower()\n","    field_name = str(field_name).strip()\n","    description = str(description).strip() if description is not None else \"\"\n","    \n","    # Filter by INCLUDE_TABLES if specified\n","    if INCLUDE_TABLES and table_name not in [t.lower() for t in INCLUDE_TABLES]:\n","        continue\n","    \n","    # Set table description\n","    if table_name not in metadata_map:\n","        metadata_map[table_name] = {\n","            \"table_description\": f\"Table: {table_name}\",\n","            \"columns\": {}\n","        }\n","\n","    # Store column description\n","    if description:\n","        metadata_map[table_name][\"columns\"][field_name] = description\n","\n","print(f\"Processed metadata for {len(metadata_map)} tables from Excel\")\n","print(metadata_map)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"105bdd58-c1cf-4fd9-8f48-e15e51dac1c5"},{"cell_type":"code","source":["updates_applied = []\n","missing_tables = []\n","missing_columns = []\n","\n","# Loop through semantic model tables and columns\n","with connect_semantic_model(dataset=SEMANTIC_MODEL_NAME, workspace=WORKSPACE_NAME, readonly=False) as tom:\n","    \n","    # Get list of semantic model tables for comparison\n","    sem_model_tables = {t.Name.lower(): t for t in tom.model.Tables}\n","    \n","    for table_name, table_data in metadata_map.items():\n","        \n","        # Check if table exists in semantic model (case-insensitive)\n","        if table_name not in sem_model_tables:\n","            missing_tables.append(table_name)\n","            print(f\"⚠ Table not found in semantic model: {table_name}\")\n","            continue\n","        \n","        # Get the actual table object\n","        table_obj = sem_model_tables[table_name]\n","\n","        # Update table description\n","        table_description = table_data[\"table_description\"]\n","        if table_description:\n","            table_obj.Description = table_description\n","            updates_applied.append((\"TABLE\", table_obj.Name, table_description))        \n","        \n","        # Create a map of columns in the semantic model (case-insensitive)\n","        sem_columns = {c.Name.lower(): c for c in table_obj.Columns}\n","        \n","        # Update column descriptions from Excel\n","        for col_name, col_description in table_data[\"columns\"].items():\n","            col_name_lower = col_name.lower()\n","            \n","            if col_name_lower not in sem_columns:\n","                missing_columns.append(f\"{table_name}.{col_name}\")\n","                continue\n","            \n","            # Update column description\n","            column_obj = sem_columns[col_name_lower]\n","            if col_description:\n","                column_obj.Description = col_description\n","                updates_applied.append((\"COLUMN\", f\"{table_obj.Name}.{column_obj.Name}\", col_description))\n","        \n","        print(f\"✓ Processed table: {table_obj.Name}\")\n","\n","# Summary of updates\n","tables_with_updates = list(set([\n","    item[1].split('.')[0]\n","    for item in updates_applied\n","    if item[0] == \"COLUMN\"\n","]))\n","\n","columns_updated = [\n","    item[1]\n","    for item in updates_applied\n","    if item[0] == \"COLUMN\"\n","]\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"UPDATE SUMMARY\")\n","print(\"=\" * 80)\n","print(f\"\\nSemantic Model: '{SEMANTIC_MODEL_NAME}'\")\n","print(f\"Tables processed: {len(tables_with_updates)}\")\n","print(f\"Column descriptions updated: {len(columns_updated)}\")\n","\n","if missing_tables:\n","    print(f\"\\n⚠ TABLES NOT FOUND IN SEMANTIC MODEL ({len(missing_tables)}):\")\n","    for tbl in sorted(missing_tables):\n","        print(f\"  - {tbl}\")\n","\n","if missing_columns:\n","    print(f\"\\n⚠ COLUMNS NOT FOUND IN SEMANTIC MODEL ({len(missing_columns)}):\")\n","    for col in sorted(missing_columns):\n","        print(f\"  - {col}\")\n","\n","if tables_with_updates:\n","    print(f\"\\n✓ Tables updated: {', '.join(sorted(tables_with_updates))}\")\n","\n","print(\"\\n✓ Metadata update completed!\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"0bdfba23-0cec-47ef-90aa-50fdb43b8841","normalized_state":"finished","queued_time":"2025-12-08T20:50:44.412411Z","session_start_time":null,"execution_start_time":"2025-12-08T20:50:44.4136001Z","execution_finish_time":"2025-12-08T20:50:58.3365195Z","parent_msg_id":"c1412845-39d3-4b40-b028-82f646280e39"},"text/plain":"StatementMeta(, 0bdfba23-0cec-47ef-90aa-50fdb43b8841, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✓ Processed table: accident\n✓ Processed table: adjuster\n✓ Processed table: claim\n✓ Processed table: driver_telemetry_data\n⚠ Table not found in semantic model: payment\n✓ Processed table: policy\n✓ Processed table: policyholder\n✓ Processed table: vehicle\n\n================================================================================\nUPDATE SUMMARY\n================================================================================\n\nSemantic Model: 'sm_AutoClaims'\nTables processed: 7\nColumn descriptions updated: 60\n\n⚠ TABLES NOT FOUND IN SEMANTIC MODEL (1):\n  - payment\n\n✓ Tables updated: accident, adjuster, claim, driver_telemetry_data, policy, policyholder, vehicle\n\n✓ Metadata update completed!\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b8632c3-d68c-4b72-8e17-fd925dbe9b25"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}