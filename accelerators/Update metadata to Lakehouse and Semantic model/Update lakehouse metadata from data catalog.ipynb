{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, trim, lower\n","from collections import defaultdict\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"e9d3e66a-6ab4-409c-a626-97a84874faa7","normalized_state":"finished","queued_time":"2025-12-10T21:24:56.8855833Z","session_start_time":"2025-12-10T21:24:56.8865957Z","execution_start_time":"2025-12-10T21:25:10.9222952Z","execution_finish_time":"2025-12-10T21:25:11.3023602Z","parent_msg_id":"8f214113-33cf-4199-b1dc-5aca86aea926"},"text/plain":"StatementMeta(, e9d3e66a-6ab4-409c-a626-97a84874faa7, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fef52137-a1fe-40b0-a7c2-7d6dad8ac21a"},{"cell_type":"code","source":["# Enter list of tables for which metadata will be updated\n","INCLUDE_TABLES         = [] # [\"table1\",\"table2\"]\n","#Enter lakehouse path to data catalog excel file \n","CatalogFilePath = \"abfss://WS_AutoClaimsPOC@onelake.dfs.fabric.microsoft.com/lh_AutoClaims.Lakehouse/Files/Data Catalog/sample_data_catalog.xlsx\"\n","\n","# Convert to lowercase for case-insensitive comparison\n","include_tables_lower = [t.lower() for t in INCLUDE_TABLES]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"e9d3e66a-6ab4-409c-a626-97a84874faa7","normalized_state":"finished","queued_time":"2025-12-10T21:24:56.9895193Z","session_start_time":null,"execution_start_time":"2025-12-10T21:25:11.304378Z","execution_finish_time":"2025-12-10T21:25:11.6170322Z","parent_msg_id":"9aef8367-0680-4ee9-84e4-aa3fc2f333ab"},"text/plain":"StatementMeta(, e9d3e66a-6ab4-409c-a626-97a84874faa7, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1181f75e-ec01-4249-9570-1fa517964545"},{"cell_type":"code","source":["# code to ignore new line characters within the excel cells\n","import pandas as pd\n","\n","df = pd.read_excel(CatalogFilePath)\n","df_pandas = df.replace({r'\\r\\n|\\n|\\r': ' '}, regex=True)\n","\n","#display(df_pandas)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"e9d3e66a-6ab4-409c-a626-97a84874faa7","normalized_state":"finished","queued_time":"2025-12-10T21:24:57.0952193Z","session_start_time":null,"execution_start_time":"2025-12-10T21:25:11.6188353Z","execution_finish_time":"2025-12-10T21:25:14.7581395Z","parent_msg_id":"94fd566b-d31c-4829-a6e9-0e07c4424906"},"text/plain":"StatementMeta(, e9d3e66a-6ab4-409c-a626-97a84874faa7, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"45dfaa53-23e5-444f-a376-7d564b582c7b"},{"cell_type":"code","source":["#Convert to spark dataframe\n","df = spark.createDataFrame(df_pandas)\n","\n","# Keep only required columns and remove duplicates\n","df = df.select(\"Logical Table Name\", \"Logical Field Name\", \"Database Datatype\", \"Data Team Definition\").dropDuplicates()\n","#display(df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"e9d3e66a-6ab4-409c-a626-97a84874faa7","normalized_state":"finished","queued_time":"2025-12-10T21:24:57.3374431Z","session_start_time":null,"execution_start_time":"2025-12-10T21:25:14.760188Z","execution_finish_time":"2025-12-10T21:25:16.235262Z","parent_msg_id":"60c3d157-2d0a-4fad-a333-3555177e31f3"},"text/plain":"StatementMeta(, e9d3e66a-6ab4-409c-a626-97a84874faa7, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"outputs_hidden":true}},"id":"e593d5a0-98e0-4470-8423-6eb561fba179"},{"cell_type":"code","source":["# Group by Logical Table Name\n","tables = defaultdict(list)\n","\n","# Collect data to driver for processing\n","rows = df.select(\"Logical Table Name\", \"Logical Field Name\", \"Database Datatype\", \"Data Team Definition\").collect()\n","\n","for row in rows:\n","    table_name = row['Logical Table Name']\n","    field_name = row['Logical Field Name']\n","    datatype = row['Database Datatype']\n","    description = row['Data Team Definition']\n","    \n","    # Skip rows with missing critical data\n","    if table_name is None or field_name is None or datatype is None:\n","        print(f\"⚠ Table: {table_name} has one or more content missing\")\n","        continue\n","\n","    # Clean the values\n","    #table_name = str(table_name).strip()\n","    table_name = str(table_name).strip().lower()\n","    field_name = str(field_name).strip()\n","    datatype = str(datatype).strip()\n","    description = str(description).strip() if description is not None else \"\"\n","    \n","    # Skip tables not in the INLCUDE list\n","    if include_tables_lower and table_name not in include_tables_lower:\n","        continue \n","\n","    # Map SQL Server datatypes to Spark SQL datatypes\n","    datatype_mapping = {\n","        'nvarchar': 'STRING',\n","        'varchar': 'STRING',\n","        'nChar': 'STRING',\n","        'nchar': 'STRING',\n","        'char': 'STRING',\n","        'integer': 'INT',\n","        'int': 'INT',\n","        'decimal': 'DECIMAL(38,10)',\n","        'numeric': 'DECIMAL(38,10)',\n","        'date': 'DATE',\n","        'datetime': 'TIMESTAMP',\n","        'tinyint': 'TINYINT',\n","        'smallint': 'SMALLINT',\n","        'bigint': 'BIGINT',\n","        'float': 'DOUBLE',\n","        'bit': 'BOOLEAN'\n","    }\n","    \n","    # Parse datatype with precision/scale\n","    spark_datatype = datatype\n","    if '(' in datatype:\n","        base_type = datatype.split('(')[0].strip().lower()\n","        if base_type in ['nvarchar', 'varchar', 'nchar', 'char']:\n","            spark_datatype = 'STRING'\n","        elif base_type in ['decimal', 'numeric']:\n","            spark_datatype = datatype.upper().replace('NVARCHAR', 'DECIMAL').replace('VARCHAR', 'DECIMAL')\n","        else:\n","            spark_datatype = datatype_mapping.get(base_type, 'STRING')\n","    else:\n","        base_type = datatype.lower()\n","        spark_datatype = datatype_mapping.get(base_type, 'STRING')\n","    \n","    tables[table_name].append({\n","        'field_name': field_name,\n","        'datatype': spark_datatype,\n","        'description': description\n","    })\n","\n","print(f\"Filtered to {len(tables)} tables from INCLUDE list\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"e9d3e66a-6ab4-409c-a626-97a84874faa7","normalized_state":"finished","queued_time":"2025-12-10T21:24:57.5135278Z","session_start_time":null,"execution_start_time":"2025-12-10T21:25:16.2376991Z","execution_finish_time":"2025-12-10T21:25:17.7599549Z","parent_msg_id":"cafb0eeb-43a2-4916-bf47-0532c2bfa58b"},"text/plain":"StatementMeta(, e9d3e66a-6ab4-409c-a626-97a84874faa7, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Filtered to 8 tables from INCLUDE list\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f983904c-cb8f-4a18-a17e-245fc4ef5360"},{"cell_type":"code","source":["# Discover tables in database\n","existing_tables = [row.tableName.lower() for row in spark.sql(\"SHOW TABLES\").collect()]\n","print(f\"Found {len(existing_tables)} existing tables in lakehouse\\n\")\n","\n","# Track missing and updated items\n","missing_tables = []\n","missing_columns = {}\n","updated_tables = []\n","updated_columns = []\n","\n","print(f\"Processing {len(tables)} tables from Excel...\\n\")\n","\n","for table_name, columns in sorted(tables.items()):\n","    # Check if table exists\n","    if table_name not in existing_tables:\n","        missing_tables.append(table_name)\n","        print(f\"⚠ Table not found: {table_name}\")\n","        continue\n","    \n","    try:\n","        # Get existing columns in the table\n","        existing_columns = {row.col_name.lower(): row.col_name for row in spark.sql(f\"DESCRIBE TABLE {table_name}\").collect()}\n","        \n","        # Update table-level description\n","        table_comment = f\"Table: {table_name}\"\n","        try:\n","            spark.sql(f\"ALTER TABLE {table_name} SET TBLPROPERTIES ('comment' = '{table_comment}')\")\n","            updated_tables.append(table_name)\n","            print(f\"✓ Updated table description: {table_name}\")\n","        except Exception as e:\n","            print(f\"  ✗ Failed to update table description for {table_name}: {str(e)}\")\n","        \n","        # Update column descriptions\n","        table_missing_columns = []\n","        for col in columns:\n","            col_name_lower = col['field_name'].lower()\n","            \n","            # Check if column exists (case-insensitive)\n","            if col_name_lower not in existing_columns:\n","                table_missing_columns.append(col['field_name'])\n","                continue\n","            \n","            # Use the actual column name from the table\n","            actual_col_name = existing_columns[col_name_lower]\n","            col_desc = col['description'].replace(\"'\", \"\\\\'\")\n","            \n","            if col_desc:\n","                try:\n","                    spark.sql(f\"ALTER TABLE {table_name} ALTER COLUMN {actual_col_name} COMMENT '{col_desc}'\")\n","                    updated_columns.append(f\"{table_name}.{actual_col_name}\")\n","                except Exception as e:\n","                    print(f\"  ✗ Failed to update column {table_name}.{actual_col_name}: {str(e)}\")\n","        \n","        if table_missing_columns:\n","            missing_columns[table_name] = table_missing_columns\n","            print(f\"  ⚠ Missing columns in {table_name}: {', '.join(table_missing_columns)}\")\n","        \n","        if len(columns) - len(table_missing_columns) > 0:\n","            print(f\"  ✓ Updated {len(columns) - len(table_missing_columns)} column descriptions\")\n","        \n","    except Exception as e:\n","        print(f\"  ✗ Error processing table {table_name}: {str(e)}\")\n","    \n","    print()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"e9d3e66a-6ab4-409c-a626-97a84874faa7","normalized_state":"finished","queued_time":"2025-12-10T21:24:57.6523967Z","session_start_time":null,"execution_start_time":"2025-12-10T21:25:17.7620883Z","execution_finish_time":"2025-12-10T21:27:13.6062553Z","parent_msg_id":"dd831ddb-864e-41b8-bdbf-5bbf60419b95"},"text/plain":"StatementMeta(, e9d3e66a-6ab4-409c-a626-97a84874faa7, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Found 7 existing tables in lakehouse\n\nProcessing 8 tables from Excel...\n\n✓ Updated table description: accident\n  ✓ Updated 7 column descriptions\n\n✓ Updated table description: adjuster\n  ✓ Updated 4 column descriptions\n\n✓ Updated table description: claim\n  ✓ Updated 8 column descriptions\n\n✓ Updated table description: driver_telemetry_data\n  ✓ Updated 22 column descriptions\n\n⚠ Table not found: payment\n✓ Updated table description: policy\n  ✓ Updated 7 column descriptions\n\n✓ Updated table description: policyholder\n  ✓ Updated 8 column descriptions\n\n✓ Updated table description: vehicle\n  ✓ Updated 4 column descriptions\n\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false}},"id":"a8187977-ef95-4c51-8855-958db8d3cb7c"},{"cell_type":"code","source":["# Print summary\n","print(\"=\" * 80)\n","print(\"EXECUTION SUMMARY\")\n","print(\"=\" * 80)\n","print(f\"\\nTotal tables in Excel: {len(tables)}\")\n","print(f\"Tables updated: {len(updated_tables)}\")\n","print(f\"Column descriptions updated: {len(updated_columns)}\")\n","\n","if missing_tables:\n","    print(f\"\\n⚠ TABLES NOT FOUND IN LAKEHOUSE ({len(missing_tables)}):\")\n","    for table_name in sorted(missing_tables):\n","        print(f\"  - {table_name}\")\n","\n","if missing_columns:\n","    print(f\"\\n⚠ COLUMNS NOT FOUND IN TABLES ({sum(len(cols) for cols in missing_columns.values())} total):\")\n","    for table_name in sorted(missing_columns.keys()):\n","        print(f\"  Table: {table_name}\")\n","        for col_name in missing_columns[table_name]:\n","            print(f\"    - {col_name}\")\n","\n","if not missing_tables and not missing_columns:\n","    print(\"\\n✓ All tables and columns from Excel or in the INLCUDE list were found and updated!\")\n","else:\n","    print(f\"\\n⚠ Please review missing tables/columns listed above\")\n","\n","print(\"\\n✓ Description update completed!\")\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"e9d3e66a-6ab4-409c-a626-97a84874faa7","normalized_state":"finished","queued_time":"2025-12-10T21:24:57.8101761Z","session_start_time":null,"execution_start_time":"2025-12-10T21:27:13.608663Z","execution_finish_time":"2025-12-10T21:27:13.902577Z","parent_msg_id":"d95ec147-8295-4cf9-b63d-74c30e102de1"},"text/plain":"StatementMeta(, e9d3e66a-6ab4-409c-a626-97a84874faa7, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["================================================================================\nEXECUTION SUMMARY\n================================================================================\n\nTotal tables in Excel: 8\nTables updated: 7\nColumn descriptions updated: 60\n\n⚠ TABLES NOT FOUND IN LAKEHOUSE (1):\n  - payment\n\n⚠ Please review missing tables/columns listed above\n\n✓ Description update completed!\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ba5b197-4957-4b10-85cc-4d9e3ef8ce83"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"03795c9e-23af-4272-922d-5f4fec8a2e46"}],"default_lakehouse":"03795c9e-23af-4272-922d-5f4fec8a2e46","default_lakehouse_name":"lh_AutoClaims","default_lakehouse_workspace_id":"db7dcf85-001e-4277-a85e-3c92029900bc"}}},"nbformat":4,"nbformat_minor":5}