{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, trim, lower\n","from collections import defaultdict\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"fbc530e5-000a-40ef-bb71-208ca41550fe","normalized_state":"finished","queued_time":"2025-12-10T21:22:20.8904352Z","session_start_time":"2025-12-10T21:22:20.8914113Z","execution_start_time":"2025-12-10T21:22:33.8626057Z","execution_finish_time":"2025-12-10T21:22:34.2364496Z","parent_msg_id":"18ed4d31-ba63-4e7e-a013-9ace37c0e2c1"},"text/plain":"StatementMeta(, fbc530e5-000a-40ef-bb71-208ca41550fe, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fef52137-a1fe-40b0-a7c2-7d6dad8ac21a"},{"cell_type":"code","source":["#Enter lakehouse path to data catalog excel file \n","CatalogFilePath = \"abfss://WS_AutoClaimsPOC@onelake.dfs.fabric.microsoft.com/lh_AutoClaims.Lakehouse/Files/Data Catalog/sample_data_catalog.xlsx\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"fbc530e5-000a-40ef-bb71-208ca41550fe","normalized_state":"finished","queued_time":"2025-12-10T21:22:21.0137741Z","session_start_time":null,"execution_start_time":"2025-12-10T21:22:34.2383868Z","execution_finish_time":"2025-12-10T21:22:34.5769416Z","parent_msg_id":"1a9e3835-a2d0-4b2c-ba3c-50ab07a6014e"},"text/plain":"StatementMeta(, fbc530e5-000a-40ef-bb71-208ca41550fe, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8020125d-f61c-4b7f-8743-3c56b889e2f8"},{"cell_type":"code","source":["# code to ignore new line characters within the excel cells\n","import pandas as pd\n","\n","df = pd.read_excel(CatalogFilePath)\n","df_pandas = df.replace({r'\\r\\n|\\n|\\r': ' '}, regex=True)\n","\n","#display(df_pandas)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"fbc530e5-000a-40ef-bb71-208ca41550fe","normalized_state":"finished","queued_time":"2025-12-10T21:22:21.1365852Z","session_start_time":null,"execution_start_time":"2025-12-10T21:22:34.5791607Z","execution_finish_time":"2025-12-10T21:22:37.7265105Z","parent_msg_id":"271be494-e85e-4069-8ae9-ae1519f3e82e"},"text/plain":"StatementMeta(, fbc530e5-000a-40ef-bb71-208ca41550fe, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"45dfaa53-23e5-444f-a376-7d564b582c7b"},{"cell_type":"code","source":["#Convert to spark dataframe\n","df = spark.createDataFrame(df_pandas)\n","\n","# Keep only required columns and remove duplicates\n","df = df.select(\"Logical Table Name\", \"Logical Field Name\", \"Database Datatype\", \"Data Team Definition\").dropDuplicates()\n","#display(df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"fbc530e5-000a-40ef-bb71-208ca41550fe","normalized_state":"finished","queued_time":"2025-12-10T21:22:21.3989125Z","session_start_time":null,"execution_start_time":"2025-12-10T21:22:37.728483Z","execution_finish_time":"2025-12-10T21:22:39.2035759Z","parent_msg_id":"35485387-2c98-42db-9a07-94068c14d9ed"},"text/plain":"StatementMeta(, fbc530e5-000a-40ef-bb71-208ca41550fe, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"id":"e593d5a0-98e0-4470-8423-6eb561fba179"},{"cell_type":"code","source":["# Group by Logical Table Name\n","tables = defaultdict(list)\n","\n","# Collect data to driver for processing\n","rows = df.select(\"Logical Table Name\", \"Logical Field Name\", \"Database Datatype\", \"Data Team Definition\").collect()\n","\n","for row in rows:\n","    table_name = row['Logical Table Name']\n","    field_name = row['Logical Field Name']\n","    datatype = row['Database Datatype']\n","    description = row['Data Team Definition']\n","    \n","    # Skip rows with missing critical data\n","    if table_name is None or field_name is None or datatype is None:\n","        continue\n","    \n","    # Clean the values\n","    #table_name = str(table_name).strip()\n","    table_name = str(table_name).strip().lower()\n","    field_name = str(field_name).strip()\n","    datatype = str(datatype).strip()\n","    description = str(description).strip() if description is not None else \"\"\n","    \n","    # Map SQL Server datatypes to Spark SQL datatypes\n","    datatype_mapping = {\n","        'nvarchar': 'STRING',\n","        'varchar': 'STRING',\n","        'nChar': 'STRING',\n","        'nchar': 'STRING',\n","        'char': 'STRING',\n","        'integer': 'INT',\n","        'int': 'INT',\n","        'decimal': 'DECIMAL(38,10)',\n","        'numeric': 'DECIMAL(38,10)',\n","        'date': 'DATE',\n","        'datetime': 'TIMESTAMP',\n","        'tinyint': 'TINYINT',\n","        'smallint': 'SMALLINT',\n","        'bigint': 'BIGINT',\n","        'float': 'DOUBLE',\n","        'bit': 'BOOLEAN'\n","    }\n","    \n","    # Parse datatype with precision/scale\n","    spark_datatype = datatype\n","    if '(' in datatype:\n","        base_type = datatype.split('(')[0].strip().lower()\n","        if base_type in ['nvarchar', 'varchar', 'nchar', 'char']:\n","            spark_datatype = 'STRING'\n","        elif base_type in ['decimal', 'numeric']:\n","            spark_datatype = datatype.upper().replace('NVARCHAR', 'DECIMAL').replace('VARCHAR', 'DECIMAL')\n","        else:\n","            spark_datatype = datatype_mapping.get(base_type, 'STRING')\n","    else:\n","        base_type = datatype.lower()\n","        spark_datatype = datatype_mapping.get(base_type, 'STRING')\n","    \n","    tables[table_name].append({\n","        'field_name': field_name,\n","        'datatype': spark_datatype,\n","        'description': description\n","    })"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"fbc530e5-000a-40ef-bb71-208ca41550fe","normalized_state":"finished","queued_time":"2025-12-10T21:22:21.5775876Z","session_start_time":null,"execution_start_time":"2025-12-10T21:22:39.2056704Z","execution_finish_time":"2025-12-10T21:22:40.6704579Z","parent_msg_id":"1d8b4311-63b1-4b55-bfdb-6bd0926c29ec"},"text/plain":"StatementMeta(, fbc530e5-000a-40ef-bb71-208ca41550fe, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f983904c-cb8f-4a18-a17e-245fc4ef5360"},{"cell_type":"code","source":["# Execute CREATE TABLE scripts directly in Fabric lakehouse\n","print(f\"Starting to create {len(tables)} Delta tables in Fabric lakehouse...\\n\")\n","\n","created_tables = []\n","failed_tables = []\n","\n","for table_name, columns in sorted(tables.items()):\n","    try:\n","        # Build CREATE TABLE statement\n","        create_stmt = f\"CREATE TABLE IF NOT EXISTS {table_name} (\\n\"\n","        \n","        # Write column definitions\n","        col_defs = []\n","        for col in columns:\n","            col_name = col['field_name']\n","            col_type = col['datatype']\n","            #col_desc = col['description'].replace(\"'\", \"''\")[:500]  # Escape quotes and limit length\n","            # Escape single quotes\n","            col_desc = col['description'].replace(\"'\", \"\\\\'\")\n","\n","            \n","            col_def = f\"    {col_name} {col_type}\"\n","            if col_desc:\n","                col_def += f\" COMMENT '{col_desc}'\"\n","            col_defs.append(col_def)\n","        \n","        create_stmt += ',\\n'.join(col_defs)\n","        create_stmt += \"\\n)\\n\"\n","        create_stmt += \"USING DELTA\\n\"\n","        create_stmt += f\"COMMENT 'Table: {table_name}'\"\n","        \n","        # Execute the CREATE TABLE statement\n","        print(f\"Creating table: {table_name} ({len(columns)} columns)...\")\n","        spark.sql(create_stmt)\n","        created_tables.append(table_name)\n","        print(f\"  ✓ Successfully created {table_name}\\n\")\n","        \n","    except Exception as e:\n","        failed_tables.append((table_name, str(e)))\n","        print(f\"  ✗ Failed to create {table_name}\")\n","        print(f\"    Error: {str(e)}\\n\")\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"fbc530e5-000a-40ef-bb71-208ca41550fe","normalized_state":"finished","queued_time":"2025-12-10T21:22:21.7225756Z","session_start_time":null,"execution_start_time":"2025-12-10T21:22:40.6725176Z","execution_finish_time":"2025-12-10T21:22:40.9731817Z","parent_msg_id":"82589c60-7209-4408-81b1-a4063401f383"},"text/plain":"StatementMeta(, fbc530e5-000a-40ef-bb71-208ca41550fe, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting to create 8 Delta tables in Fabric lakehouse...\n\nCreating table: accident (7 columns)...\n  ✓ Successfully created accident\n\nCreating table: adjuster (4 columns)...\n  ✓ Successfully created adjuster\n\nCreating table: claim (8 columns)...\n  ✓ Successfully created claim\n\nCreating table: driver_telemetry_data (22 columns)...\n  ✓ Successfully created driver_telemetry_data\n\nCreating table: payment (5 columns)...\n  ✓ Successfully created payment\n\nCreating table: policy (7 columns)...\n  ✓ Successfully created policy\n\nCreating table: policyholder (8 columns)...\n  ✓ Successfully created policyholder\n\nCreating table: vehicle (4 columns)...\n  ✓ Successfully created vehicle\n\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false}},"id":"a8187977-ef95-4c51-8855-958db8d3cb7c"},{"cell_type":"code","source":["# Print summary\n","print(\"=\" * 80)\n","print(\"EXECUTION SUMMARY\")\n","print(\"=\" * 80)\n","print(f\"\\nTotal tables processed: {len(tables)}\")\n","print(f\"Successfully created: {len(created_tables)}\")\n","print(f\"Failed: {len(failed_tables)}\")\n","\n","if failed_tables:\n","    print(\"\\nFailed tables:\")\n","    for table_name, error in failed_tables:\n","        print(f\"  - {table_name}: {error}\")\n","\n","print(\"\\n✓ Delta table creation completed!\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"fbc530e5-000a-40ef-bb71-208ca41550fe","normalized_state":"finished","queued_time":"2025-12-10T21:22:21.8843409Z","session_start_time":null,"execution_start_time":"2025-12-10T21:22:40.9751189Z","execution_finish_time":"2025-12-10T21:22:41.2491905Z","parent_msg_id":"df82f6a7-b1ed-4644-b632-b4ffa17c9fac"},"text/plain":"StatementMeta(, fbc530e5-000a-40ef-bb71-208ca41550fe, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["================================================================================\nEXECUTION SUMMARY\n================================================================================\n\nTotal tables processed: 8\nSuccessfully created: 8\nFailed: 0\n\n✓ Delta table creation completed!\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ba5b197-4957-4b10-85cc-4d9e3ef8ce83"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"03795c9e-23af-4272-922d-5f4fec8a2e46"}],"default_lakehouse":"03795c9e-23af-4272-922d-5f4fec8a2e46","default_lakehouse_name":"lh_AutoClaims","default_lakehouse_workspace_id":"db7dcf85-001e-4277-a85e-3c92029900bc"}}},"nbformat":4,"nbformat_minor":5}