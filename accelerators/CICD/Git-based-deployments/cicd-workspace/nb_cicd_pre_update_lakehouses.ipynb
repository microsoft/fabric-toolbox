{"cells":[{"cell_type":"markdown","source":["**Helper notebook**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c2ee3b3a-f369-4f98-8e88-a04e999870cf"},{"cell_type":"code","source":["%run nb_helper"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"151a5189-710d-4f0a-ad72-5ad9c422c817"},{"cell_type":"markdown","source":["**Define a logging dataframe**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4788f8ff-57e3-44dc-82d2-225b1f99ba38"},{"cell_type":"code","source":["dfLogging = pd.DataFrame(columns = ['LoadId','NotebookId', 'NotebookName', 'WorkspaceId', 'CellId', 'Timestamp', 'ElapsedTime', 'Message', 'ErrorMessage'])\n","vContext = mssparkutils.runtime.context\n","vNotebookId = vContext[\"currentNotebookId\"]\n","vLogNotebookName = vContext[\"currentNotebookName\"]\n","vWorkspaceId = vContext[\"currentWorkspaceId\"] # where the notebook is running, to not confuse with source and target workspaces"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"121fc594-15d2-47d2-b4d0-f9609f50791b"},{"cell_type":"markdown","source":["**Parameters --> convert to code for debugging the notebook. otherwise, keep commented as parameters are passed from DevOps pipelines**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f8421d0a-f588-4f23-bd13-4696e6f2fcc5"},{"cell_type":"code","source":["pToken = \"\"\n","pSqlToken = \"\"\n","pSourceWorkspaceId = \"\"\n","pTargetWorkspaceId = \"\"\n","pDebugMode = \"yes\"\n","pOnelakeRoles = ''\n","pOnelakeRules = ''\n","pOnelakeEntraMembers = ''\n","pOnelakeItemMembers = ''"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"22da0c79-bfea-43ea-9d57-583771c4f22f"},{"cell_type":"markdown","source":["**Resolve source and target workspace ids**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11309f2b-0e81-45ac-8863-c8de477f4093"},{"cell_type":"code","source":["vSourceWorkspaceName = fabric.resolve_workspace_name(pSourceWorkspaceId)\n","vTargetWorkspaceName = fabric.resolve_workspace_name(pTargetWorkspaceId)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4637da24-8f50-480f-bf88-2c97a00a331b"},{"cell_type":"markdown","source":["**List source and target lakehouses**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"426304ae-789e-428b-a6fd-a3506ecfdbd9"},{"cell_type":"code","source":["df_source_lakehouses = labs.list_lakehouses(workspace=vSourceWorkspaceName)\n","df_target_lakehouses = labs.list_lakehouses(workspace=vTargetWorkspaceName)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"982de745-074d-4d85-89b5-9a81be826d5b"},{"cell_type":"markdown","source":["**Verify that there is a least one lakehouse in the source**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"86be88a9-34b0-4f99-812e-6c322a6e9cc6"},{"cell_type":"code","source":["if df_source_lakehouses.empty:\n","    vMessage = f\"workspace <vSourceWorkspaceName> have 0 lakehouse. pre-update is not required.\"\n","\n","    # Display an exit message\n","    display(Markdown(\"### âœ… Notebook execution stopped successfully!\"))\n","\n","    # Exit without error\n","    mssparkutils.notebook.exit(vMessage)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7323d7d7-a55d-4946-827f-8f743b58bde8"},{"cell_type":"markdown","source":["**Variables related to the logic**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f35a0c6a-fc49-4862-b52f-4b6a887c8476"},{"cell_type":"code","source":["vApiVersion = \"v1\"\n","vShortcutConflictPolicy = \"Abort\"\n","if pOnelakeRoles == \"\":\n","    vCustomRoles = \"no\"\n","else:\n","    vCustomRoles = \"yes\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":[]},"id":"668a4422-7fe8-4d2e-bdc0-4399f6908180"},{"cell_type":"markdown","source":["**Resolve source and target workspace ids**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f89dd60e-920e-4f53-bf45-c461d9c6dac3"},{"cell_type":"code","source":["vSourceWorkspaceId = pSourceWorkspaceId\n","vTargetWorkspaceId = pTargetWorkspaceId"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c3785a2b-a42b-4049-80d2-fa71a23a2a27"},{"cell_type":"markdown","source":["**Access token**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b3eda308-20f6-4ff1-87a3-5c71b228f3a1"},{"cell_type":"code","source":["vScope = \"https://analysis.windows.net/powerbi/api\"\n","\n","# get the access token \n","if pDebugMode == \"yes\":\n","    # in debug mode, use the token of the current user\n","    vAccessToken  = mssparkutils.credentials.getToken(vScope)\n","    vSqlAccessToken = vAccessToken\n","else:\n","    # when the code is run from DevOps, the token passed as a parameter\n","    vAccessToken = pToken \n","    vSqlAccessToken = pSqlToken"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3095581-b39f-40a9-8523-6800e182f4e9"},{"cell_type":"markdown","source":["**Base Url and Headers**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fddaf6ce-2a97-4dba-a0b4-fca5205acbb8"},{"cell_type":"code","source":["vBaseUrl = f\"https://api.fabric.microsoft.com/{vApiVersion}/\"\n","vHeaders = {'Authorization': f'Bearer {vAccessToken}'}"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7d9ffe60-f9f8-4a43-bc88-62b328dfdec0"},{"cell_type":"markdown","source":["**Functions**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"40817457-ef07-4ac4-90e1-4d68a93c9e4e"},{"cell_type":"code","source":["def input_for_full_deployment(df_source_lakehouse_columns):\n","\n","    # get the dataframe passed as parameter\n","    df = df_source_lakehouse_columns\n","\n","    # if the column has space in it, replace it with an underscore\n","    df[\"Column Name\"] = df[\"Column Name\"].str.replace(' ', '_')\n","\n","    # concat the column name and data tupe\n","    df[\"ColumnNameDataType\"] = df[\"Column Name\"] + \" \" + df[\"Data Type\"]\n","\n","    # group the columns\n","    df_grouped = df.groupby([\"WorkspaceTargetName\", \"Lakehouse Name\", \"Table Name\"])['ColumnNameDataType'].agg(','.join).reset_index()\n","\n","    # generate the sql statement\n","    df_grouped[\"SqlStatement\"] = \"CREATE TABLE \" + df_grouped[\"Lakehouse Name\"] + \".\" + df_grouped[\"Table Name\"] + \"(\" + df_grouped[\"ColumnNameDataType\"] + \")\"\n","\n","    # return the dataframe\n","    return df_grouped"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a61f3026-9518-44c2-b564-8dfd6f2f900b"},{"cell_type":"code","source":["def input_for_incremental_deployment(df_source_lakehouse_columns_incremental, incremental_type):\n","\n","\n","    # get the dataframe passed as parameter\n","    df = df_source_lakehouse_columns_incremental\n","\n","    # if the column has space in it, replace it with an underscore\n","    df[\"Column Name\"] = df[\"Column Name\"].str.replace(' ', '_')\n","\n","    # concat the column name and data tupe\n","    df[\"ColumnNameDataType\"] = df[\"Column Name\"] + \" \" + df[\"Data Type\"]\n","\n","\n","    if incremental_type == \"alter table add column\":\n","\n","        # group the columns\n","        df_grouped = df.groupby([\"WorkspaceTargetName\", \"Lakehouse Name\", \"Table Name\"])['ColumnNameDataType'].agg(','.join).reset_index()\n","\n","        # generate the sql statement for adding columns\n","        df_grouped[\"SqlStatement\"] = \"ALTER TABLE \" + df_grouped[\"Lakehouse Name\"] + \".\" + df_grouped[\"Table Name\"] + \" ADD COLUMNS(\" + df_grouped[\"ColumnNameDataType\"] + \")\"\n","\n","    elif incremental_type == \"alter table drop column\":\n","\n","        # ALTER TABLE ALTER COLUMN does not work as of 02.2025\n","        # add this statement before droping the columns: ALTER TABLE data_types SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.minReaderVersion' = '2','delta.minWriterVersion' = '5')        \n","\n","        df_0 = df.groupby([\"WorkspaceTargetName\", \"Lakehouse Name\", \"Table Name\"])['Column Name'].agg(','.join).reset_index()\n","        df_0['SqlStatement'] = \"ALTER TABLE \" + df_0[\"Lakehouse Name\"] + \".\" + df_0[\"Table Name\"] + \" SET TBLPROPERTIES ('delta.columnMapping.mode'='name', 'delta.minReaderVersion'='2','delta.minWriterVersion'='5');\"\n","        df_0.drop(columns='Column Name', inplace=True)\n","\n","        df_1 = df.groupby([\"WorkspaceTargetName\", \"Lakehouse Name\", \"Table Name\"])['Column Name'].agg(','.join).reset_index()\n","        df_1[\"SqlStatement\"] = \"ALTER TABLE \" + df_1[\"Lakehouse Name\"] + \".\" + df_1[\"Table Name\"] + \" DROP COLUMNS(\" + df_1[\"Column Name\"] + \")\"\n","\n","\n","        df_union = pd.concat([df_0, df_1], ignore_index=True)\n","        df_grouped = df_union.groupby([\"WorkspaceTargetName\", \"Lakehouse Name\", \"Table Name\"])['SqlStatement'].apply(lambda x: '\\n'.join(x)).reset_index()\n","        \n","\n","    else:\n","        # ALTER TABLE ALTER COLUMN does not work as of 02.2025\n","        # add this statement before droping the columns: ALTER TABLE data_types SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.minReaderVersion' = '2','delta.minWriterVersion' = '5')\n","        # the alternative is the following logic (example with 2 columns changing data types):\n","        # ALTER TABLE data_types SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.minReaderVersion' = '2','delta.minWriterVersion' = '5')\n","        # ALTER TABLE data_types ADD COLUMNS (C_New INT, D_New INT)\n","        # UPDATE data_types SET C_New = C, D_New = C\n","        # ALTER TABLE data_types DROP COLUMNS (C,D)\n","        # ALTER TABLE data_types RENAME COLUMN C_New TO C;\n","        # ALTER TABLE data_types RENAME COLUMN D_New TO D;\n","\n","        # add the required columns for the logic\n","        df['InputForAddingColumns'] = df['Column Name'] + '_New ' + df['Data Type']\n","        df['InputForUpdatingColumns'] = df['Column Name'] + '_New =' + df['Column Name']\n","        df['InputForRenamingColumns'] = df['Column Name'] + '_New TO ' + df['Column Name']\n","\n","        df_0 = df.groupby([\"WorkspaceTargetName\", \"Lakehouse Name\", \"Table Name\"])['Column Name'].agg(','.join).reset_index()\n","        df_0['SqlStatement'] = \"ALTER TABLE \" + df_0[\"Lakehouse Name\"] + \".\" + df_0[\"Table Name\"] + \" SET TBLPROPERTIES ('delta.columnMapping.mode'='name', 'delta.minReaderVersion'='2','delta.minWriterVersion'='5');\"\n","        df_0.drop(columns='Column Name', inplace=True)\n","\n","        df_1 = df.groupby([\"WorkspaceTargetName\", \"Lakehouse Name\", \"Table Name\"])['InputForAddingColumns'].agg(','.join).reset_index()\n","        df_1['SqlStatement'] = 'ALTER TABLE ' + df_1[\"Lakehouse Name\"] + '.' + df_1[\"Table Name\"] + ' ADD COLUMNS (' + df_1['InputForAddingColumns'] +');'\n","        df_1.drop(columns='InputForAddingColumns', inplace=True)\n","\n","        df_2 = df.groupby([\"WorkspaceTargetName\", \"Lakehouse Name\", \"Table Name\"])['InputForUpdatingColumns'].agg(','.join).reset_index()\n","        df_2['SqlStatement'] = 'UPDATE ' + df_2[\"Lakehouse Name\"] + '.' + df_2[\"Table Name\"] + ' SET ' + df_2['InputForUpdatingColumns'] + ';'\n","        df_2.drop(columns='InputForUpdatingColumns', inplace=True)\n","\n","        df_3 = df.groupby([\"WorkspaceTargetName\", \"Lakehouse Name\", \"Table Name\"])['Column Name'].agg(','.join).reset_index()\n","        df_3['SqlStatement'] = 'ALTER TABLE ' + df_2[\"Lakehouse Name\"] + '.' + df_2[\"Table Name\"] + ' DROP COLUMNS (' + df_3['Column Name'] +');'\n","        df_3.drop(columns='Column Name', inplace=True)\n","\n","        def generate_sql(group):\n","            return \";\\n\".join([f\"ALTER TABLE {row['Lakehouse Name']}.{row['Table Name']} RENAME COLUMN {row['InputForRenamingColumns']}\" for _, row in group.iterrows()]) + \";\"\n","\n","        df_4 = df.groupby([\"WorkspaceTargetName\", \"Lakehouse Name\", \"Table Name\"]).apply(generate_sql).reset_index(name='SqlStatement')\n","\n","        df_union = pd.concat([df_0, df_1, df_2, df_3, df_4], ignore_index=True)\n","        df_grouped = df_union.groupby([\"WorkspaceTargetName\", \"Lakehouse Name\", \"Table Name\"])['SqlStatement'].apply(lambda x: '\\n'.join(x)).reset_index()\n","\n","\n","    # return the dataframe\n","    return df_grouped"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8efca722-b5f5-4f5a-a2ee-2e0dd517afae"},{"cell_type":"code","source":["def shortcut_body(target_type, name, path, target_location, target_subpath, target_connection_id):\n","\n","    vTargetType = \"\"\n","    if target_type == \"AdlsGen2\":\n","        vTargetType = \"adlsGen2\"\n","    elif target_type == \"AmazonS3\":\n","        vTargetType = \"amazonS3\"\n","    elif target_type == \"GoogleCloudStorage\":\n","        vTargetType = \"googleCloudStorage\"\n","    elif target_type == \"S3Compatible\":\n","        vTargetType = \"s3Compatible\"\n","    else:\n","        vTargetType = \"\"\n","\n","    shortcut_body = {\n","        \"name\": name,\n","        \"path\": path\n","    }\n","\n","    shortcut_specific_template_temp = {\n","        \"target\": {\n","            f\"{vTargetType}\": {\n","                \"location\": \"{location_}\",\n","                \"subpath\": \"{subpath_}\",\n","                \"connectionId\": \"{connectionId_}\"\n","            }\n","        }\n","    }\n","\n","    inputs = {\n","        \"location_\": target_location,\n","        \"subpath_\": target_subpath,\n","        \"connectionId_\": target_connection_id\n","    }\n","\n","    # replace the placeholders\n","    shortcut_specific_template = replace_placeholders_in_json(shortcut_specific_template_temp, inputs)\n","    \n","    # inject the specific template\n","    shortcut_body.update(shortcut_specific_template)\n","    # print(json.dumps(shortcut_body, indent=4))\n","\n","    return shortcut_body"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e8531147-ecf7-4e0f-8d4c-7888c8b709f3"},{"cell_type":"code","source":["def onelake_shortcut_body(target_type, name, path, target_onelake_workspace_id, target_one_lake_item_id, target_onelake_path):\n","\n","    shortcut_body = {\n","        \"name\": name,\n","        \"path\": path\n","    }\n","\n","    shortcut_specific_template_temp = {\n","        \"target\": {\n","            \"oneLake\": {\n","                \"workspaceId\": \"{workspaceId_}\",\n","                \"itemId\": \"{itemId_}\",\n","                \"path\": \"{path_}\"\n","            }\n","        }\n","    }\n","\n","    inputs = {\n","        \"workspaceId_\": target_onelake_workspace_id,\n","        \"itemId_\": target_one_lake_item_id,\n","        \"path_\": target_onelake_path\n","    }\n","\n","    # replace the placeholders\n","    shortcut_specific_template = replace_placeholders_in_json(shortcut_specific_template_temp, inputs)\n","    \n","    # inject the specific template\n","    shortcut_body.update(shortcut_specific_template)\n","    # print(json.dumps(shortcut_body, indent=4))\n","\n","    return shortcut_body"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"080a9c11-1eff-43db-b2c2-d06a7fb4fdcc"},{"cell_type":"code","source":["def create_lakehouse(lakehouse_name, url, headers, operation, workspace_target_id, item_type, sleep_in_seconds, debug_mode):\n","    \n","    # create the json body\n","    body = {\n","        \"displayName\": f\"{lakehouse_name}\",\n","        \"type\": \"Lakehouse\",\n","        \"description\": f\"Lakehouse {lakehouse_name} created by fabric deployment notebook\"\n","    }\n","\n","    # create the lakehouse\n","    create_or_update_fabric_item(url, headers, body, 'post', operation, workspace_target_id, lakehouse_name, item_type, sleep_in_seconds, debug_mode)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"30c2d4d0-cc7c-4fd6-ad49-1db625af298b"},{"cell_type":"code","source":["def create_notebook(notebook_name, url, headers, operation, workspace_target_id, item_type, sleep_in_seconds, debug_mode):\n","\n","    # create the json body\n","    body = {\n","        \"displayName\": f\"{notebook_name}\",\n","        \"type\": \"Notebook\",\n","        \"description\": f\"Notebook {notebook_name} created by fabric deployment notebook\"\n","    }\n","\n","    # create the notebook\n","    create_or_update_fabric_item(url, headers, body, 'post', operation, workspace_target_id, notebook_name, item_type, sleep_in_seconds, debug_mode)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"82f2d13b-1f0a-4aba-af6f-5a140da2b9f0"},{"cell_type":"code","source":["# create the alchemy engine\n","def create_sqlalchemy_engine(connection_string : str):\n","    token = vSqlAccessToken\n","    SQL_COPT_SS_ACCESS_TOKEN = 1256\n","\n","    # the following code is required to structure the token for pyodbc.connect\n","    exptoken = b'';\n","    for i in bytes(token, \"UTF-8\"):\n","        exptoken += bytes({i});\n","        exptoken += bytes(1);\n","    tokenstruct = struct.pack(\"=i\", len(exptoken)) + exptoken;\n","\n","    return sqlalchemy.create_engine(\"mssql+pyodbc://\", creator=lambda: pyodbc.connect(connection_string, attrs_before = { SQL_COPT_SS_ACCESS_TOKEN:bytearray(tokenstruct) }))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8e3fa6a7-2e43-4acd-9512-175a8624ff74"},{"cell_type":"markdown","source":["**Identify source lakehouses**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"861bdf0d-c691-49a6-a021-4f96b7bcbe23"},{"cell_type":"code","source":["try:\n","\n","    # define the source lakehouse column dataframe\n","    df_source_lakehouse_columns = pd.DataFrame()\n","    df_source_lakehouse_tables = pd.DataFrame()\n","\n","    # iterate over the source lakehouses \n","    for lakehouse in df_source_lakehouses['Lakehouse Name']:\n","\n","        # 1. get the tables \n","        df = labs.lakehouse.get_lakehouse_tables(lakehouse = lakehouse, workspace = vSourceWorkspaceName)\n","\n","        # append the rows to the dataframe\n","        df_source_lakehouse_tables = pd.concat([df_source_lakehouse_tables, df], ignore_index=True)\n","\n","\n","        # 2. get the tables and columns\n","        df = labs.lakehouse.get_lakehouse_columns(lakehouse = lakehouse, workspace = vSourceWorkspaceName)\n","\n","        # append the rows to the dataframe\n","        df_source_lakehouse_columns = pd.concat([df_source_lakehouse_columns, df], ignore_index=True)\n","\n","    # add the target workspace to the lakehouse columns dataframe\n","    df_source_lakehouse_columns[\"WorkspaceTargetName\"] = vTargetWorkspaceName\n","\n","    # keep the required columns in the lakehouse tables datafreme\n","    columns_to_drop = [\"Workspace Name\", \"Format\", \"Type\", \"Location\"]\n","    df_source_lakehouse_tables = df_source_lakehouse_tables.drop(columns=columns_to_drop)\n","\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify source lakehouses', datetime.now(), None, vMessage, ''] \n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify source lakehouses', datetime.now(), None, vMessage, str(e) ] \n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5a89fdf-12ea-4cd0-9e5a-a35cea866569"},{"cell_type":"markdown","source":["**Create target lakehouses and notebooks --> at this stage, these would be empty shells**\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a4509d76-7fae-46de-a136-85f96a6e4d5e"},{"cell_type":"code","source":["try:\n","\n","    # sleep time before checking the operation's status in post requests\n","    vSleepInSeconds = 30\n","\n","    # perform the deployment\n","    for lakehouse in df_source_lakehouses['Lakehouse Name']:\n","\n","        # set the lakehouse name, and the notebooks used to define the lakehouse content\n","        vLakehouseName = lakehouse\n","        vTargetNotebookName = \"nb_\" + vLakehouseName + \"_definition\"\n","        vTargetSqlNotebookName = \"nb_\" + vLakehouseName + \"_sql_definition\"\n","\n","\n","        # filter the target lakehouse dataframe on the current lakehouse\n","        df_target_lakehouses_in_scope = df_target_lakehouses[df_target_lakehouses['Lakehouse Name']==vLakehouseName]       \n","\n","        # set the url\n","        vUrl = vBaseUrl + f\"workspaces/{vTargetWorkspaceId}/items\" \n","\n","        # if the target lakehouse dataframe is empty --> create the lakehouse\n","        if df_target_lakehouses_in_scope.empty:\n","\n","\n","            # create the lakehouse\n","            create_lakehouse(vLakehouseName, vUrl, vHeaders, \"creating\", vTargetWorkspaceId, \"lakehouse\", vSleepInSeconds, pDebugMode)\n","\n","            # create the correspondant notebook\n","            create_notebook(vTargetNotebookName, vUrl, vHeaders, \"creating\", vTargetWorkspaceId, \"notebook\", vSleepInSeconds, pDebugMode)\n","\n","            # create the correspondant sql notebook\n","            create_notebook(vTargetSqlNotebookName, vUrl, vHeaders, \"creating\", vTargetWorkspaceId, \"notebook\", vSleepInSeconds, pDebugMode)\n","\n","        else:\n","            # create the correspondant notebook\n","            create_notebook(vTargetNotebookName, vUrl, vHeaders, \"creating\", vTargetWorkspaceId, \"notebook\", vSleepInSeconds, pDebugMode)\n","\n","            # create the correspondant sql notebook\n","            create_notebook(vTargetSqlNotebookName, vUrl, vHeaders, \"creating\", vTargetWorkspaceId, \"notebook\", vSleepInSeconds, pDebugMode)     \n","\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'create target lakehouses and notebooks', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'create target lakehouses and notebooks', datetime.now(), None, vMessage, str(e) ] \n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"07b2f563-7fc4-4d79-b4e6-c40002ce7441"},{"cell_type":"markdown","source":["**Source and target sql analytics endpoints**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8d7f61c9-59bb-49e2-955f-66de779b1d9d"},{"cell_type":"code","source":["df_target_lakehouses = labs.list_lakehouses(workspace=vTargetWorkspaceName)\n","vSourceSqlEndpoint = df_source_lakehouses.loc[0, 'SQL Endpoint Connection String']\n","vTargetSqlEndpoint = df_target_lakehouses.loc[0, 'SQL Endpoint Connection String']"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26b83e75-53fd-4d4a-a782-39b19276966d"},{"cell_type":"markdown","source":["**Identify source shortcuts, folders, access roles and sql objects**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0b98147e-7e99-400d-8510-c10bc7d2dba2"},{"cell_type":"code","source":["try:\n","\n","    # define the dataframes\n","    df_shortcuts = pd.DataFrame()\n","    df_folders = pd.DataFrame()\n","    df_onelake_roles = pd.DataFrame()\n","    df_sql_objects = pd.DataFrame()\n","    df_routines = pd.DataFrame()\n","\n","    # get a token for storage\n","    vOnelakeHeaders = {\"authorization\": f\"bearer {mssparkutils.credentials.getToken('storage')}\"}\n","\n","    # iterate over the lakehouses\n","    for index, row in df_source_lakehouses.iterrows():\n","\n","        # set the lakehouse name and id\n","        vLakehouseName = row['Lakehouse Name']\n","        vLakehouseSourceId = row['Lakehouse ID']\n","        vLakehouseTargetId = labs.resolve_lakehouse_id(lakehouse=vLakehouseName, workspace=vTargetWorkspaceName)\n","\n","\n","        # 1. extract shortcuts \n","        vExtractionType = \"shortcuts\"\n","        vShortcutUrl = f\"workspaces/{vSourceWorkspaceId}/items/{vLakehouseSourceId}/shortcuts\"\n","        vUrl = vBaseUrl + vShortcutUrl\n","\n","        print(f\"extracting shortcuts for lakehouse {vLakehouseName}\")\n","\n","        # create the api global dataframe for shortcuts\n","        api_call_global_dataframe = pd.DataFrame()\n","\n","        try:\n","                \n","            # make the api call\n","            api_call_main(vUrl, vHeaders, pDebugMode, vExtractionType)\n","            api_call_global_dataframe['WorkspaceSourceId'] = vSourceWorkspaceId\n","            api_call_global_dataframe['WorkspaceTargetId'] = vTargetWorkspaceId   \n","            api_call_global_dataframe['LakehouseTargetName'] = vLakehouseName\n","            api_call_global_dataframe['LakehouseTargetId'] = vLakehouseTargetId\n","\n","\n","            # concat to the correspondant dataframe\n","            df_shortcuts = pd.concat([df_shortcuts, api_call_global_dataframe], ignore_index=True)\n","\n","            # logging\n","            vMessage = f\"extracting shortcuts for lakehouse {vLakehouseName} succeeded\"\n","            print(vMessage)\n","\n","        except Exception as e:\n","            vMessage = f\"extracting shortcuts for lakehouse {vLakehouseName} failed\"\n","            print(vMessage)\n","            print(str(e))\n","\n","\n","        # 2. extract folders\n","        vExtractionType = \"file_system\"\n","        vUrl = f'https://onelake.dfs.fabric.microsoft.com/{vSourceWorkspaceName}/{vLakehouseName}.lakehouse/Files?recursive=True&resource=filesystem'\n","        print(f\"extracting folders for lakehouse {vLakehouseName}\")\n","\n","        # create the api global dataframe\n","        api_call_global_dataframe = pd.DataFrame()\n","\n","        try:\n","                \n","            # make the api call\n","            api_call_main(vUrl, vOnelakeHeaders, pDebugMode, vExtractionType)\n","\n","            api_call_global_dataframe['FolderName'] = api_call_global_dataframe['name'].replace(vLakehouseSourceId + \"/\", '', regex=True)\n","            api_call_global_dataframe_new = api_call_global_dataframe[['FolderName','isDirectory']]\n","\n","            api_call_global_dataframe_new['WorkspaceSourceId'] = vSourceWorkspaceId\n","            api_call_global_dataframe_new['WorkspaceTargetId'] = vTargetWorkspaceId   \n","            api_call_global_dataframe_new['LakehouseTargetName'] = vLakehouseName\n","            api_call_global_dataframe_new['LakehouseTargetId'] = vLakehouseTargetId\n","\n","            df_folders_temp = api_call_global_dataframe_new[api_call_global_dataframe_new['isDirectory'] == 'true']\n","\n","            # concat to the correspondant dataframe\n","            df_folders = pd.concat([df_folders, df_folders_temp], ignore_index=True)\n","\n","            # logging\n","            vMessage = f\"extracting folders of lakehouse {vLakehouseName} succeeded\"\n","            print(vMessage)\n","\n","        except Exception as e:\n","            vMessage = f\"extracting files and folders of lakehouse {vLakehouseName} failed\"\n","            print(vMessage)\n","            print(str(e))\n","\n","        # 3. extract onelake access\n","        # if no custom roles are provided for the target lakehouses, use roles defined in the source lakehouses\n","        if vCustomRoles == \"no\":\n","\n","            vExtractionType = \"onelake_access\"\n","            vShortcutUrl = f\"workspaces/{vSourceWorkspaceId}/items/{vLakehouseSourceId}/dataAccessRoles\"\n","            vUrl = vBaseUrl + vShortcutUrl\n","            print(f\"extracting onelake access for lakehouse {vLakehouseName}\")  \n","\n","            # create the api global dataframe for shortcuts\n","            api_call_global_dataframe = pd.DataFrame()\n","\n","            try:\n","                    \n","                # make the api call\n","                api_call_main(vUrl, vHeaders, pDebugMode, vExtractionType)\n","\n","                api_call_global_dataframe['WorkspaceSourceId'] = vSourceWorkspaceId\n","                api_call_global_dataframe['WorkspaceTargetId'] = vTargetWorkspaceId   \n","                api_call_global_dataframe['lakehouse'] = vLakehouseName\n","                api_call_global_dataframe['LakehouseTargetId'] = vLakehouseTargetId\n","\n","\n","                # concat to the correspondant dataframe\n","                df_onelake_roles = pd.concat([df_onelake_roles, api_call_global_dataframe], ignore_index=True)\n","\n","                # prepare the rules, entra members and item members dataframes\n","                df_role_rules = flatten_nested_json_df(df_onelake_roles[['id', 'decisionRules']].explode('decisionRules').dropna())\n","                condition_1 = (df_role_rules[\"decisionRules.permission.attributeName\"] == \"Action\") & (df_role_rules[\"decisionRules.permission.attributeValueIncludedIn\"] != \"Read\")\n","                df_role_rules_1 = df_role_rules[~condition_1]\n","                condition_2 = (df_role_rules_1[\"decisionRules.permission.attributeName\"] == \"Path\") & (df_role_rules_1[\"decisionRules.permission.attributeValueIncludedIn\"] == \"Read\")\n","                df_role_rules_2 = df_role_rules_1[~condition_2]\n","                df_role_rules = df_role_rules_2\n","                df_entra_members = flatten_nested_json_df(df_onelake_roles[['id', 'members.microsoftEntraMembers']].explode('members.microsoftEntraMembers').dropna())\n","                df_item_members = flatten_nested_json_df(df_onelake_roles[['id', 'members.fabricItemMembers']].explode('members.fabricItemMembers').dropna())                \n","\n","                # logging\n","                vMessage = f\"extracting onelake access for lakehouse {vLakehouseName} succeeded\"\n","                print(vMessage)\n","\n","            except Exception as e:\n","                vMessage = f\"extracting onelake access for lakehouse {vLakehouseName} failed\"\n","                print(vMessage)\n","                print(str(e))\n","        else: # use the parameters provided for the roles\n","\n","            # onelake roles\n","            onelake_roles = json.loads(pOnelakeRoles)\n","            df_onelake_roles = pd.DataFrame(onelake_roles)\n","\n","            # onelake rules\n","            role_rules = json.loads(pOnelakeRules)\n","            df_role_rules = pd.DataFrame(role_rules)\n","\n","            # onelake entra members\n","            entra_members = json.loads(pOnelakeEntraMembers)\n","            df_entra_members = pd.DataFrame(entra_members)\n","\n","            # onelake item members\n","            item_members = json.loads(pOnelakeItemMembers)\n","            df_item_members = pd.DataFrame(item_members) \n","\n","\n","        # 4. extaction sql objects created in the sql endpoint\n","        print(f\"extracting routines and views for lakehouse {vLakehouseName}\")\n","        vSqlStatement = \"\"\"\n","        SELECT \n","            a.ROUTINE_CATALOG AS LakehouseName, \n","            a.ROUTINE_SCHEMA AS SchemaName, \n","            a.ROUTINE_NAME AS ObjectName, \n","            '' AS DropStatement,\n","            REPLACE(a.ROUTINE_DEFINITION, 'CREATE', 'CREATE OR ALTER') AS CreateStatement,\n","            'Routines' AS ObjectType\n","        FROM \n","            INFORMATION_SCHEMA.ROUTINES a\n","        UNION\n","        SELECT \n","            TABLE_CATALOG, \n","            TABLE_SCHEMA, \n","            TABLE_NAME, \n","            '' AS DropStatement,\n","            REPLACE(VIEW_DEFINITION, 'CREATE', 'CREATE OR ALTER') AS CreateStatement, \n","            'View' AS ObjectType\n","        FROM \n","            INFORMATION_SCHEMA.VIEWS\n","        WHERE \n","            TABLE_SCHEMA NOT IN ('sys','queryinsights')\n","        \"\"\"\n","\n","        spark_df_sql_objects = spark.read.option(Constants.WorkspaceId, vSourceWorkspaceId).option(Constants.DatabaseName, vLakehouseName).synapsesql(vSqlStatement)\n","        df_sql_objects_temp = spark_df_sql_objects.toPandas()\n","        df_sql_objects = pd.concat([df_sql_objects, df_sql_objects_temp], ignore_index=True)\n","\n","        # 5. extraction of security policies\n","        print(f\"extracting security for lakehouse {vLakehouseName}\")\n","        vSqlStatement = f\"\"\"\n","        SELECT \n","            '{vLakehouseName}' AS LakehouseName,\n","            schema_name AS SchemaName, \n","            policy_name AS ObjectName,\n","            'DROP SECURITY POLICY IF EXISTS ' + policy_name_new AS DropStatement,\n","            CONCAT(\n","                create_statement,\n","                policy_name_new,\n","                filter_predicate,\n","                CASE is_enabled\n","                    WHEN 0 THEN ' WITH (STATE = OFF)'\n","                    ELSE ' WITH (STATE = ON)'\n","                END \n","            ) AS CreateStatement,\n","            'Security Policy' AS ObjectType\n","        FROM \n","            (\n","            SELECT \n","                schema_name, policy_name, create_statement, policy_name_new, is_enabled, STRING_AGG(filter_predicate, ',') AS filter_predicate\n","            FROM \n","                (\n","                SELECT \n","                    pol_schema.name AS schema_name,\n","                    pol.name as policy_name,\n","                    'CREATE SECURITY POLICY ' AS create_statement,\n","                    '[' + pol_schema.name + '].[' + pol.name + ']' AS policy_name_new,\n","                    pol.is_enabled,\n","                    ' ADD FILTER PREDICATE ' \n","                    + RIGHT(LEFT(pre.predicate_definition, LEN(pre.predicate_definition)-1),LEN(pre.predicate_definition)-2)\n","                    + ' ON [' + obj_schema.name + '].[' + obj.name + ']' \n","                    AS filter_predicate\n","\n","                FROM \n","                    sys.security_policies pol\n","                    INNER JOIN sys.schemas pol_schema \n","                        ON pol_schema.schema_id = pol.schema_id\n","                    INNER JOIN sys.security_predicates pre\n","                        ON pre.object_id = pol.object_id\n","                    INNER JOIN sys.objects obj \n","                        ON obj.object_id = pre.target_object_id\n","                    INNER JOIN sys.schemas obj_schema \n","                        ON obj_schema.schema_id = obj.schema_id\n","                ) a\n","            GROUP BY \n","                schema_name, policy_name, create_statement, policy_name_new, is_enabled\n","            ) b\n","        \"\"\"\n","\n","        spark_df_sql_objects = spark.read.option(Constants.WorkspaceId, vSourceWorkspaceId).option(Constants.DatabaseName, vLakehouseName).synapsesql(vSqlStatement)\n","        df_sql_objects_temp = spark_df_sql_objects.toPandas()\n","        df_sql_objects = pd.concat([df_sql_objects, df_sql_objects_temp], ignore_index=True)\n","\n","\n","    # format shortcuts dataframe\n","    # get the column names\n","    columns = df_shortcuts.columns.values.tolist()\n","\n","    # iterate over the column name and rename after capitalizin the first letter\n","    for columnName in columns:\n","\n","        # split the column name, take the last item and upper case the first letter\n","        processed_column = process_column_name(columnName, '.')\n","\n","        # replace the column name in the dataframe\n","        df_shortcuts.rename(columns={columnName: processed_column}, inplace=True)\n","\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify shortcuts, folders and access roles', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify shortcuts, folders and access roles', datetime.now(), None, vMessage, str(e) ] \n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f5e765a7-7893-4910-8a4e-4bba1ff3f3c0"},{"cell_type":"markdown","source":["**Exclude shortcuts from tables**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"30d7b884-a8e4-45ac-a5c1-84c525486db0"},{"cell_type":"code","source":["try:\n","    shortcuts_columns_subset = ['LakehouseTargetName', 'Name']\n","    df_shortcuts_tables = df_shortcuts[df_shortcuts['Path']=='/Tables'][shortcuts_columns_subset]\n","    df_shortcuts_tables.rename(columns={\"Name\":\"Table Name\",\"LakehouseTargetName\":\"Lakehouse Name\"},inplace=True)\n","    df_tables_exclude_shortcut = df_source_lakehouse_tables.merge(df_shortcuts_tables, on=df_source_lakehouse_tables.columns.tolist(), how='left', indicator=True)\n","    df_tables_exclude_shortcut = df_tables_exclude_shortcut[df_tables_exclude_shortcut['_merge'] == 'left_only'].drop(columns=['_merge'])\n","    df_source_lakehouse_columns = pd.merge(df_source_lakehouse_columns, df_tables_exclude_shortcut, on=[\"Lakehouse Name\", \"Table Name\"], how=\"inner\")\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'exclude shortcuts from tables', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'exclude shortcuts from tables', datetime.now(), None, vMessage, str(e) ] \n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a9ec33b2-8cef-4cdf-b858-c219984deb16"},{"cell_type":"markdown","source":["**Exclude shortcuts from folders**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"aedfcece-cf90-4dce-8202-51ff4e28db79"},{"cell_type":"code","source":["try:\n","    shortcuts_columns_subset = ['WorkspaceSourceId','WorkspaceTargetId', 'LakehouseTargetName', 'LakehouseTargetId', 'Name', 'Path']\n","    df_shortcuts_folders = df_shortcuts[df_shortcuts['Path']=='/Files'][shortcuts_columns_subset]\n","    df_shortcuts_folders['FolderName'] = df_shortcuts_folders['Path'].replace(\"/\", '', regex=True) + '/' + df_shortcuts_folders['Name']\n","    df_shortcuts_folders['isDirectory'] = 'true'\n","    df_folders_exclude_shortcut = df_folders.merge(df_shortcuts_folders, on=df_folders.columns.tolist(), how='left', indicator=True)\n","    df_folders_exclude_shortcut = df_folders_exclude_shortcut[df_folders_exclude_shortcut['_merge'] == 'left_only'].drop(columns=['_merge'])\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'exclude shortcuts from folders', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'exclude shortcuts from folders', datetime.now(), None, vMessage, str(e)]\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f5dc886e-9c7c-424f-a9c2-4e9990e665b8"},{"cell_type":"code","source":["# define the dataframe containing the sql definitions for the deployment\n","df_input_deployment = pd.DataFrame()\n","\n","try:\n","\n","    # recreate the list of lakehouses \n","    df_target_lakehouses = labs.list_lakehouses(workspace = vTargetWorkspaceName)\n","    \n","\n","    # iterate over the target lakehouses \n","    for lakehouse in df_target_lakehouses['Lakehouse Name']:\n","\n","        # get the source lakehouse tables and columns \n","        df_source_lakehouse_columns_in_scope = df_source_lakehouse_columns[df_source_lakehouse_columns['Lakehouse Name']==lakehouse]\n","\n","        # get the lakehouse tables and columns\n","        # this call brings also table shortcut, they need to be excluded. df_shortcut_tables can be used to this effect.\n","        df_target_lakehouses_columns_temp = labs.lakehouse.get_lakehouse_columns(lakehouse = lakehouse, workspace = vTargetWorkspaceName)\n","        df_target_lakehouses_columns = df_target_lakehouses_columns_temp.merge(df_shortcuts_tables, on=['Lakehouse Name', 'Table Name'], how='left', indicator=True)\n","        df_target_lakehouses_columns = df_target_lakehouses_columns[df_target_lakehouses_columns['_merge'] == 'left_only'].drop(columns=['_merge'])\n","\n","        if df_target_lakehouses_columns.empty:\n","\n","            vMessage = f\"target lakehouse <{lakehouse} is empty. retrieve the full list of tables from the source lakehouse as an input for table definitions.\"\n","            print(vMessage)\n","\n","            # if the target lakehouse is empty, the source lakehouse definition is used for the target lakehouse\n","            df_target_lakehouses_columns = df_source_lakehouse_columns_in_scope\n","            # df_target_lakehouses_columns = pd.concat([df_target_lakehouses_columns, df_target_lakehouses_columns_temp], ignore_index=True)\n","\n","            # format the input for deployment\n","            df_input_deployment_temp = input_for_full_deployment(df_target_lakehouses_columns)\n","\n","            # concat to the dataframe\n","            df_input_deployment = pd.concat([df_input_deployment, df_input_deployment_temp], ignore_index=True)\n","\n","        else:\n","            \n","            vMessage = f\"target lakehouse <{lakehouse} is not empty. retrieve an increment list of changes from the source lakehouse as an input for table definitions.\"\n","            print(vMessage)\n","\n","            # align the structure of df to df_source_lakehouse_columns\n","            df_target_lakehouses_columns[\"WorkspaceTargetName\"] = vTargetWorkspaceName\n","\n","            # # replace the source workspace name by source workspace and the add the target workspace \n","            # # this will alow excluding rows in source not in target workspace\n","            # df_target_lakehouses_columns[\"WorkspaceName\"] = vSourceWorkspaceName\n","            # df_target_lakehouses_columns[\"WorkspaceTargetName\"] = vTargetWorkspaceName\n","\n","            # identify the incremental\n","\n","            # 1. tables in source but not in target lakehouse\n","            df_source_lakehouse_tables = df_source_lakehouse_columns_in_scope[['Lakehouse Name', 'Table Name']].drop_duplicates()\n","            df_target_lakehouse_tables = df_target_lakehouses_columns[['Lakehouse Name', 'Table Name']].drop_duplicates()\n","            df_tables_in_source_only_temp = pd.merge(df_source_lakehouse_tables, df_target_lakehouse_tables, on=['Lakehouse Name', 'Table Name'], how='left', indicator=True)\n","            df_tables_in_source_only_temp = df_tables_in_source_only_temp[df_tables_in_source_only_temp['_merge'] == 'left_only'].drop(columns=['_merge'])\n","            df_tables_in_source_only = pd.merge(df_source_lakehouse_columns_in_scope, df_tables_in_source_only_temp, on=['Lakehouse Name', 'Table Name'], how='inner')   \n","\n","            \n","            # 2. tables in both source and target lakehouses but with a structural change (added columns, deleted columns, changed Data Type)\n","            \n","            # 2.1 find common tables\n","            df_tables_in_common = pd.merge(df_source_lakehouse_tables, df_target_lakehouse_tables, on=['Lakehouse Name', 'Table Name'], how='inner', indicator=True)\n","\n","            # 2.2 tables in common, source columns\n","            df_tables_in_common_source_columns = pd.merge(df_source_lakehouse_columns_in_scope, df_tables_in_common[['Lakehouse Name', 'Table Name']], on=['Lakehouse Name', 'Table Name'], how='inner')  \n","\n","            # 2.2 tables in common, columns only in source --> ALTER TABLE ADD COLUMN\n","            df_tables_in_common_source_columns_only_temp = pd.merge(df_tables_in_common_source_columns,df_target_lakehouses_columns, on=['Lakehouse Name', 'Table Name', 'Column Name'], how='left', suffixes=('', '_target'))\n","            df_tables_in_common_source_columns_only_temp = df_tables_in_common_source_columns_only_temp[df_tables_in_common_source_columns_only_temp.isna().any(axis=1)]\n","            df_tables_in_common_source_columns_only = df_tables_in_common_source_columns_only_temp[df_tables_in_common_source_columns.columns]  \n","\n","            # 2.3 tables in common, columns only in target --> ALTER TABLE DROP COLUMN\n","            # dropping columns requires the table property delta.columnMapping.mode = name\n","            # example ALTER TABLE lake.Table Name SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name','delta.minReaderVersion' = '2','delta.minWriterVersion' = '5')\n","            df_tables_in_common_target_columns_only_temp = df_target_lakehouses_columns.merge(df_tables_in_common_source_columns, on=['Lakehouse Name', 'Table Name', 'Column Name'], how='left', suffixes=('', '_source'))\n","            df_tables_in_common_target_columns_only_temp = df_tables_in_common_target_columns_only_temp[df_tables_in_common_target_columns_only_temp.isna().any(axis=1)]\n","            df_tables_in_common_target_columns_only = df_tables_in_common_target_columns_only_temp[df_target_lakehouses_columns.columns]\n","            df_tables_in_common_target_columns_only\n","\n","            # 2.4, tables in common, columns in common, but data type changed --> ALTER TABLE ALTER COLUMN\n","            df_data_type_comparison = pd.merge(df_tables_in_common_source_columns, df_target_lakehouses_columns, on=[\"Lakehouse Name\", \"Table Name\", \"Column Name\"], suffixes=('', '_target'))\n","            df_data_type_comparison[\"is_different\"] = df_data_type_comparison[\"Data Type\"] != df_data_type_comparison[\"Data Type_target\"]\n","            df_tables_in_common_data_type_changed = df_data_type_comparison.loc[df_data_type_comparison[\"is_different\"], df_tables_in_common_source_columns.columns]\n","\n","\n","            if not df_tables_in_source_only.empty:\n","                df_input_deployment_1 = input_for_full_deployment(df_tables_in_source_only)\n","            else:\n","                df_input_deployment_1 = pd.DataFrame()\n","\n","            if not df_tables_in_common_source_columns_only.empty:\n","                df_input_deployment_2 = input_for_incremental_deployment(df_tables_in_common_source_columns_only, 'alter table add column')\n","            else:\n","                df_input_deployment_2 = pd.DataFrame()\n","\n","            if not df_tables_in_common_target_columns_only.empty:\n","                df_input_deployment_3 = input_for_incremental_deployment(df_tables_in_common_target_columns_only, 'alter table drop column')\n","            else:\n","                df_input_deployment_3 = pd.DataFrame()\n","\n","            if not df_tables_in_common_data_type_changed.empty:\n","                df_input_deployment_4 = input_for_incremental_deployment(df_tables_in_common_data_type_changed, 'alter table alter column')\n","            else:\n","                df_input_deployment_4 = pd.DataFrame()\n","\n","            # concatenate the different inputs if not empty\n","            dfs = [df_input_deployment_1, df_input_deployment_2, df_input_deployment_3, df_input_deployment_4]\n","            non_empty_dfs = [df for df in dfs if not df.empty]\n","            df_input_deployment_temp = pd.concat(non_empty_dfs, ignore_index=True) if non_empty_dfs else pd.DataFrame()\n","            \n","            # concat to the dataframe\n","            df_input_deployment = pd.concat([df_input_deployment, df_input_deployment_temp], ignore_index=True)\n","\n","    # # logging\n","    # vMessage = f\"succeeded\"\n","    # dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify tables in incremental mode', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify tables in incremental mode', datetime.now(), None, vMessage, str(e)]\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a903de67-b235-48e4-8dc3-7d23739e4779"},{"cell_type":"markdown","source":["**Update the notebook with the definition of the tables and run it against the target lakehouse to define the tables**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd58c13a-80f1-4bda-8e7f-f7f54c2554b4"},{"cell_type":"code","source":["try:\n","\n","\n","    # sleep time before checking the operation's status in post requests\n","    vSleepInSeconds = 30\n","\n","\n","    # perform the deployment\n","    for lakehouse in df_source_lakehouses['Lakehouse Name']:\n","\n","        # set the lakehouse name\n","        vLakehouseName = lakehouse\n","\n","        # get the create table statements related to the current lakehouse\n","        df_lakehouse_table_statements_current = df_input_deployment[df_input_deployment['Lakehouse Name'] == vLakehouseName]\n","\n","        # update the target notebook and run it only if there are sql statements to run\n","        if not df_lakehouse_table_statements_current.empty:\n","\n","            # define the target notebook name\n","            vTargetNotebookName = \"nb_\" + vLakehouseName + \"_definition\"\n","\n","\n","            # notebook definition template\n","            json_notebook_definition_temp = {\n","                \"nbformat\": 4,\n","                \"nbformat_minor\": 5,\n","                \"cells\": [],\n","                \"metadata\": {\n","                    \"language_info\": {\n","                        \"name\": \"sql\"\n","                    },\n","                    \"dependencies\": {\n","                        \"lakehouse\": {\n","                            \"default_lakehouse\": \"{default_lakehouse_}\",\n","                            \"default_lakehouse_name\": \"{default_lakehouse_name_}\",\n","                            \"default_lakehouse_workspace_id\": \"{default_lakehouse_workspace_id_}\"\n","                        }\n","                    }\n","                }\n","            }\n","\n","            # set the url\n","            vUrl = vBaseUrl + f\"workspaces/{vTargetWorkspaceId}/items\"\n","\n","            # this part of the code works for full and incremental deployment\n","            # resolve lakehouse and notebook id\n","            vLakehouseTargetId = labs.resolve_lakehouse_id(lakehouse=vLakehouseName, workspace=vTargetWorkspaceName)\n","            vNotebookTargetId = fabric.resolve_item_id( item_name=vTargetNotebookName, type=\"Notebook\", workspace=vTargetWorkspaceName)\n","            # print(vNotebookTargetId)\n","\n","            # prepare the default inputs for the notebook definition\n","            default_inputs_for_notebook_definition = {\n","                \"default_lakehouse_\" : vLakehouseTargetId,\n","                \"default_lakehouse_name_\" : vLakehouseName,\n","                \"default_lakehouse_workspace_id_\" : vTargetWorkspaceId\n","            }\n","\n","\n","\n","\n","            # add a new cell in the notebood definition \n","            for sql_statement in df_lakehouse_table_statements_current['SqlStatement']:\n","                new_cell = {\n","                    \"cell_type\": \"code\",\n","                    \"source\": [sql_statement]\n","                }\n","                json_notebook_definition_temp[\"cells\"].append(new_cell)\n","\n","            # get the folders of the current lakehouse\n","            df_folders_current = df_folders_exclude_shortcut[df_folders_exclude_shortcut['LakehouseTargetName'] == vLakehouseName]\n","\n","            if not df_folders_current.empty:\n","                for folder in df_folders_current['FolderName']:\n","                    new_cell = {\n","                        \"cell_type\": \"code\",\n","                        \"source\": [\n","        f\"\"\"%%pyspark\n","        mssparkutils.fs.mkdirs('{folder}')\"\"\"\n","                                ]\n","                    }\n","                    json_notebook_definition_temp[\"cells\"].append(new_cell)\n","\n","\n","            # replace the placeholders\n","            json_notebook_definition = replace_placeholders_in_json(json_notebook_definition_temp, default_inputs_for_notebook_definition)\n","\n","            # final json definition\n","            json_notebook_definition_new = json.loads(json.dumps(json_notebook_definition, indent=4))\n","            # print(json.dumps(json_notebook_definition, indent=4))\n","\n","            # base64 encoding for the api call\n","            json_notebook_definition_new_encoded = base64.b64encode(json.dumps(json_notebook_definition_new, indent=4).encode('utf-8')).decode('utf-8')\n","\n","\n","            # 3. update the notebook definition\n","\n","            # set the url for the update\n","            vUrl = vBaseUrl + f\"workspaces/{vTargetWorkspaceId}/notebooks/{vNotebookTargetId}/updateDefinition\"\n","\n","            # set the body\n","            vJsonBody = {\n","                \"definition\": {\n","                    \"format\": \"ipynb\",\n","                    \"parts\": [\n","                        {\n","                            \"path\": \"notebook-content.py\",\n","                            \"payload\": f\"{json_notebook_definition_new_encoded}\",\n","                            \"payloadType\": \"InlineBase64\"\n","                        }\n","                    ]\n","                }\n","            }\n","\n","\n","            # update the notebook definition\n","            # the update notebook definition as of 19.11.2024 has an issue when executin the operation url when the response status code is 202\n","            # it returns an error although the update is successful\n","            create_or_update_fabric_item(vUrl, vHeaders, vJsonBody, 'post', \"updating\", vTargetWorkspaceId, vTargetNotebookName, \"Notebook\", vSleepInSeconds, pDebugMode) \n","\n","            # 4. run the notebook\n","\n","            # set the url\n","            vUrl = vBaseUrl + f\"workspaces/{vTargetWorkspaceId}/items/{vNotebookTargetId}/jobs/instances?jobType=RunNotebook\"\n","\n","            # run the notebook\n","            create_or_update_fabric_item(vUrl, vHeaders, None, 'post', \"executing\", vTargetWorkspaceId, vTargetNotebookName, \"Notebook\", vSleepInSeconds, pDebugMode) \n","\n","        # logging\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'create tables and folders in target lakehouses', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'create tables and folders in target lakehouses', datetime.now(), None, vMessage, str(e)]\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"96b47a0f-823f-4ca9-8689-20b144398311"},{"cell_type":"markdown","source":["**Use the commented cell to check a notebook definition if required**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a222f20f-8a5e-4813-bf55-568a51a49599"},{"cell_type":"markdown","source":["nb = json.loads(\n","    notebookutils.notebook.getDefinition(\n","        \"nb_saleslake_definition_\", #nane of the notebook\n","        workspaceId=vTargetWorkspaceId\n","    )\n",")\n","print(json.dumps(nb, indent=4))"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4f01f115-0a6b-424e-92ab-439e84b8b0b1"},{"cell_type":"markdown","source":["**Create shortcuts in target lakehouses**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8860fb53-1728-4105-9c73-6165598f11e3"},{"cell_type":"code","source":["try:\n","    vSleepInSeconds = 30\n","    for index, row in df_shortcuts.iterrows():\n","\n","        # set the variables\n","        # common inputs \n","        vName = row['Name']\n","        vPath = row['Path']\n","        vTargetType = row['TargetType']\n","\n","        # specific to onelake\n","        vTargetOneLakeWorkspaceId = row['TargetOneLakeWorkspaceId']\n","        vTargetOneLakeItemId = row['TargetOneLakeItemId']\n","        vTargetOneLakePath = row['TargetOneLakePath']\n","\n","        # specific to adls gen2\n","        vTargetAdlsGen2Location = row['TargetAdlsGen2Location']\n","        vTargetAdlsGen2Subpath = row['TargetAdlsGen2Subpath']\n","        vTargetAdlsGen2ConnectionId = row['TargetAdlsGen2ConnectionId']\n","\n","        # todo\n","        # specific to AmazonS3\n","        # specific to GoogleCloudStorage\n","        # specific to S3Compatible\n","\n","        # target lakehouse id\n","        vLakehouseTargetId = row['LakehouseTargetId']\n","\n","        # shortcut url\n","        vShortcutUrl = f\"workspaces/{vTargetWorkspaceId}/items/{vLakehouseTargetId}/shortcuts?shortcutConflictPolicy={vShortcutConflictPolicy}\"\n","        vUrl = vBaseUrl + vShortcutUrl\n","\n","        # request body\n","        if vTargetType in [\"AdlsGen2\", \"AmazonS3\", \"GoogleCloudStorage\", \"S3Compatible\"]:\n","            vJsonBody = shortcut_body(vTargetType, vName, vPath, vTargetAdlsGen2Location, vTargetAdlsGen2Subpath, vTargetAdlsGen2ConnectionId)\n","        elif vTargetType == \"OneLake\":\n","            vJsonBody = onelake_shortcut_body(vTargetType, vName, vPath, vTargetOneLakeWorkspaceId, vTargetOneLakeItemId, vTargetOneLakePath)\n","        else:\n","            # use case to be implemented\n","            vJsonBody = \"\"\n","\n","        # create the shortcut\n","        create_or_update_fabric_item(vUrl, vHeaders, vJsonBody, 'post', \"creating\", vTargetWorkspaceId, vName, \"Shortcut\", vSleepInSeconds, pDebugMode)   \n","\n","        # logging\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'create shortcuts in target lakehouses', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'create shortcuts in target lakehouses', datetime.now(), None, vMessage, str(e)] \n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e3b8e454-ee46-4bdd-b1ac-2cee1665d98a"},{"cell_type":"markdown","source":["**Enable onelake security on target lakehouses**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"35d2e39b-7a61-4ebb-bd03-f9c5cfe3385a"},{"cell_type":"code","source":["try:\n","    # get a onelake token\n","    vOnelakeHeaders = {\"authorization\": f\"bearer {mssparkutils.credentials.getToken('storage')}\"}\n","\n","    # iterate over the target lakehouses\n","    for index, row in df_target_lakehouses.iterrows():\n","\n","        vLakehouseTargetId = row['Lakehouse ID']\n","\n","        vUrl = f'https://onelake.dfs.fabric.microsoft.com/v1.0/workspaces/{vTargetWorkspaceId}/artifacts/{vLakehouseTargetId}/security/enable'\n","\n","        vJsonBody = {\n","            \"enableOneSecurity\":\"true\"\n","        }\n","\n","        response = requests.post(vUrl, headers=vOnelakeHeaders, json=vJsonBody)\n","\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'enable onelake security on target lakehouses', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'enable onelake security on target lakehouses', datetime.now(), None, vMessage, str(e)]\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5f72c9e3-2877-4a72-9a16-e15feaf487f3"},{"cell_type":"markdown","source":["**Identify source lakehouses onelake roles**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c525cf5c-3ed2-48ad-b522-4ad67926e0dd"},{"cell_type":"code","source":["try:\n","\n","    # if custom roles are provided\n","    if vCustomRoles == \"yes\":\n","\n","        # load the csv files\n","        df_onelake_access = pd.read_csv(vOnelakeRolesCsvPath)\n","        df_role_rules = pd.read_csv(vRoleRulesCsvPath)\n","        df_item_members = pd.read_csv(vItemMembersCsvPath)\n","        df_entra_members = pd.read_csv(vEntraMembersCsvPath)\n","    else:\n","        # 2. prepare the inputs for the creation\n","        df_role_rules = flatten_nested_json_df(df_onelake_access[['id', 'decisionRules']].explode('decisionRules').dropna())\n","        condition_1 = (df_role_rules[\"decisionRules.permission.attributeName\"] == \"Action\") & (df_role_rules[\"decisionRules.permission.attributeValueIncludedIn\"] != \"Read\")\n","        df_role_rules_1 = df_role_rules[~condition_1]\n","        condition_2 = (df_role_rules_1[\"decisionRules.permission.attributeName\"] == \"Path\") & (df_role_rules_1[\"decisionRules.permission.attributeValueIncludedIn\"] == \"Read\")\n","        df_role_rules_2 = df_role_rules_1[~condition_2]\n","        df_role_rules = df_role_rules_2\n","        df_entra_members = flatten_nested_json_df(df_onelake_access[['id', 'members.microsoftEntraMembers']].explode('members.microsoftEntraMembers').dropna())\n","        df_item_members = flatten_nested_json_df(df_onelake_access[['id', 'members.fabricItemMembers']].explode('members.fabricItemMembers').dropna())\n","\n","        # logging\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify source lakehouses onelake roles', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify source lakehouses onelake roles', datetime.now(), None, vMessage, str(e)]\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"831e0e4b-379e-47ed-8c8a-0c05875b916d"},{"cell_type":"markdown","source":["**Create onelake roles in target lakehouses**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"faeb02fe-be45-4c64-a2a3-fc91905f1b2e"},{"cell_type":"code","source":["try:\n","\n","    vSleepInSeconds = 30\n","    for index, row in df_onelake_roles.iterrows():\n","\n","        vRoleId = row['id']\n","        vRoleName = row['name']\n","        vLakehouseTargetName = row['lakehouse']\n","        vLakehouseTargetId = labs.resolve_lakehouse_id(lakehouse=vLakehouseTargetName, workspace=vTargetWorkspaceName)\n","        # print(vRoleId, vRoleName, vLakehouseTargetName, vLakehouseTargetId)\n","\n","        # updating the DefaultReader role via the API deletes it\n","        if vRoleName != \"DefaultReader\":\n","            # role template with decision rules, entra members and item members\n","            role_template = {\n","                \"value\": [\n","                    {\n","                        \"name\": \"{name_}\",\n","                        \"decisionRules\": [\n","                        ],\n","                        \"members\": {\n","                            \"microsoftEntraMembers\": [\n","                            ],\n","                            \"fabricItemMembers\": [\n","                            ]\n","                        }\n","                    }\n","                ]\n","            }\n","\n","            # replace the role name\n","            role_input = {\n","                \"name_\": vRoleName\n","            }\n","            role_template = replace_placeholders_in_json(role_template, role_input)\n","\n","            # handle decision rules\n","            df_rules_current = df_role_rules[df_role_rules['id']==vRoleId]\n","            for effect in df_rules_current['decisionRules.effect'].drop_duplicates():\n","\n","                vEffectName = effect\n","\n","                # replace the effect name\n","                effect_input = {\n","                    \"effect_\": vEffectName\n","                }\n","                rules_template = {\n","                    \"effect\": \"{effect_}\",\n","                    \"permission\": [\n","                    ]\n","                }\n","                rules_template = replace_placeholders_in_json(rules_template, effect_input)\n","\n","                # handle effects\n","                df_effect_current = df_rules_current[(df_rules_current['id']==vRoleId) & (df_rules_current['decisionRules.effect']==vEffectName)]\n","\n","                for attribute in df_effect_current['decisionRules.permission.attributeName'].drop_duplicates():\n","\n","                    vAttributeName = attribute\n","\n","                    # replace the attribute\n","                    permission_input = {\n","                        \"attributeName_\": attribute\n","                    }\n","                    permissions_template = {\n","                        \"attributeName\": \"{attributeName_}\",\n","                        \"attributeValueIncludedIn\": [\n","                        ]   \n","                    }\n","                    permissions_template = replace_placeholders_in_json(permissions_template, permission_input)\n","\n","                    # handle attributes\n","                    df_attribute_current = df_effect_current[(df_effect_current['id']==vRoleId) & (df_effect_current['decisionRules.effect']==vEffectName) & (df_effect_current['decisionRules.permission.attributeName']==vAttributeName)] \n","\n","                    for attribute_included_in in df_attribute_current['decisionRules.permission.attributeValueIncludedIn']:\n","                        vAttributeIncludedIn = attribute_included_in\n","                        permissions_template[\"attributeValueIncludedIn\"].append(vAttributeIncludedIn)\n","\n","                    # append the attributes included in to the permission template\n","                    rules_template['permission'].append(permissions_template)\n","\n","                # appedn the rules template to the decision rules in the role template\n","                role_template[\"value\"][0][\"decisionRules\"].append(rules_template)\n","\n","\n","            # handle the entra members\n","            df_entra_member_current = df_entra_members[df_entra_members['id']==vRoleId]\n","            for index, row in df_entra_member_current.iterrows():\n","\n","                vTenantId = row['members.microsoftEntraMembers.tenantId']\n","                vObjectId = row['members.microsoftEntraMembers.objectId']\n","\n","                # set the member template\n","                entra_members_template = {\n","                    \"tenantId\": vTenantId,\n","                    \"objectId\": vObjectId\n","                }\n","\n","                # append the member template to the role template\n","                role_template[\"value\"][0][\"members\"][\"microsoftEntraMembers\"].append(entra_members_template)\n","\n","\n","            # handle the fabric item members\n","            df_item_members_current = df_item_members[df_item_members['id']==vRoleId]\n","            for item_member in df_item_members_current['members.fabricItemMembers.sourcePath'].drop_duplicates():\n","                \n","                vSourcePath = item_member # row['members.fabricItemMembers.sourcePath']\n","                vTargetPath = vTargetWorkspaceId + \"/\" + vLakehouseTargetId \n","\n","                # replace the source path\n","                items_members_template = {\n","                    \"sourcePath\": vTargetPath,\n","                    \"itemAccess\": [\n","                    ]\n","                }\n","\n","                # handle the item access\n","                df_item_access_current = df_item_members_current[df_item_members_current['members.fabricItemMembers.sourcePath']==vSourcePath] \n","                for item_access in df_item_access_current['members.fabricItemMembers.itemAccess'].drop_duplicates():\n","\n","                    vItemAccess = item_access\n","\n","                    # append the item access to the member template\n","                    items_members_template[\"itemAccess\"].append(vItemAccess) \n","\n","                # append the fabric item template to the role template\n","                role_template[\"value\"][0][\"members\"][\"fabricItemMembers\"].append(items_members_template)\n","\n","            # print(json.dumps(role_template, indent=4))\n","            \n","            vJsonBody = role_template\n","\n","            # url\n","            vRoleUrl = f\"workspaces/{vTargetWorkspaceId}/items/{vLakehouseTargetId}/dataAccessRoles\"\n","            vUrl = vBaseUrl + vRoleUrl\n","\n","            create_or_update_fabric_item(vUrl, vHeaders, vJsonBody, 'put', \"creating/updating\", vTargetWorkspaceId, vRoleName, \"onelake role\", vSleepInSeconds, pDebugMode)    \n","\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'create onelake roles in target lakehouses', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'create onelake roles in target lakehouses', datetime.now(), None, vMessage, str(e)]\n","    if pDebugMode == \"yes\":\n","        print(str(e))\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c509e9fc-4677-4a30-9160-3df7f0deadda"},{"cell_type":"markdown","source":["**Create the sql objects in the target SQL endpoint**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1fcb077d-dcd8-4c82-a24a-80673ee68460"},{"cell_type":"code","source":["try:\n","\n","\n","    # sleep time before checking the operation's status in post requests\n","    vSleepInSeconds = 30\n","\n","\n","    # perform the deployment\n","    for lakehouse in df_source_lakehouses['Lakehouse Name']:\n","\n","        # set the lakehouse name\n","        vLakehouseName = lakehouse\n","\n","        # get the lakewarehouse id --> this is different than the lakehouse id\n","        vLakehouseWarehouseTargetId = df_target_lakehouses[df_target_lakehouses['Lakehouse Name']== vLakehouseName].loc[0, 'SQL Endpoint ID']\n","     \n","        # define the target notebook name\n","        vTargetSqlNotebookName = \"nb_\" + vLakehouseName + \"_sql_definition\"\n","\n","        # get the create table statements related to the current lakehouse\n","        df_sql_objects_current = df_sql_objects[df_sql_objects['LakehouseName'] == vLakehouseName]\n","\n","\n","        # update the target notebook and run it only if there are sql statements to run\n","        if not df_sql_objects_current.empty:\n","\n","\n","            # notebook definition template --> this will be a TSQL notebook: add to it the lakewarehouse id as a default warehouse\n","            json_notebook_definition_temp = {\n","                \"nbformat\": 4,\n","                \"nbformat_minor\": 5,\n","                \"cells\": [],\n","                \"metadata\": {\n","                    \"kernel_info\": {\n","                        \"name\": \"sqldatawarehouse\"\n","                    },\n","                    \"kernelspec\": {\n","                        \"name\": \"sqldatawarehouse\",\n","                        \"language\": \"sqldatawarehouse\",\n","                        \"display_name\": \"sqldatawarehouse\"\n","                    },\n","                    \"language_info\": {\n","                        \"name\": \"sql\"\n","                    },\n","                    \"dependencies\": {\n","                        \"warehouse\": {\n","                            \"known_warehouses\": [\n","                                {\n","                                    \"id\": \"{default_lakewarehouse_}\",\n","                                    \"type\": \"Lakewarehouse\"\n","                                }\n","                            ],\n","                            \"default_warehouse\": \"{default_lakewarehouse_}\"\n","                        },\n","                        \"lakehouse\": {}\n","                    }\n","                }\n","            }\n","\n","            # set the url\n","            vUrl = vBaseUrl + f\"workspaces/{vTargetWorkspaceId}/items\"\n","\n","            # this part of the code works for full and incremental deployment\n","            # resolve notebook id\n","            vTargetSqlNotebookId = fabric.resolve_item_id( item_name=vTargetSqlNotebookName, type=\"Notebook\", workspace=vTargetWorkspaceName)\n","\n","            # prepare the default inputs for the notebook definition\n","            default_inputs_for_notebook_definition = {\n","                \"default_lakewarehouse_\" : vLakehouseWarehouseTargetId\n","            }\n","\n","            # check if there are security policies defined\n","            # if yes:\n","            # 1. add a cell to drop the security policy --> this will allow altering the predicate function\n","            # 2. add all other cells to create view, functions, etc..\n","            # 3. add a cell to create the security policy\n","\n","            # create cells for droping security policies\n","            sql_objects_contain_security_policies = (df_sql_objects_current['ObjectType'] == 'Security Policy').any()\n","            if sql_objects_contain_security_policies:\n","                df_sql_objects_current_policies = df_sql_objects_current[df_sql_objects_current['ObjectType'] == 'Security Policy']\n","\n","                # iterate over the sql objects of the current lakehouse\n","                for index, row in df_sql_objects_current_policies.iterrows():\n","\n","                    # get the ddl statement\n","                    vSchemaName = row['SchemaName']\n","                    vObjectName = row['ObjectName']\n","                    vDropStatement = row['DropStatement']\n","\n","                    print(f\"adding a drop security cell for <{vSchemaName}.{vObjectName}>.\")\n","\n","                    new_cell = {\n","                        \"cell_type\": \"code\",\n","                        \"source\": [vDropStatement]\n","                    }\n","                    json_notebook_definition_temp[\"cells\"].append(new_cell)\n","                \n","\n","            # iterate over the sql objects of the current lakehouse\n","            for index, row in df_sql_objects_current.iterrows():\n","\n","                # get the ddl statement\n","                vSchemaName = row['SchemaName']\n","                vObjectName = row['ObjectName']\n","                vDropStatement = row['DropStatement']\n","                vCreateStatement = row['CreateStatement']\n","                vObjectType = row['ObjectType']\n","\n","                print(f\"adding a create cell for <{vSchemaName}.{vObjectName}>.\")\n","\n","                # add the create statement\n","                new_cell = {\n","                    \"cell_type\": \"code\",\n","                    \"source\": [vCreateStatement]\n","                }\n","                json_notebook_definition_temp[\"cells\"].append(new_cell)                    \n","\n","\n","\n","            # replace the placeholders\n","            json_notebook_definition = replace_placeholders_in_json(json_notebook_definition_temp, default_inputs_for_notebook_definition)\n","\n","            # final json definition\n","            json_notebook_definition_new = json.loads(json.dumps(json_notebook_definition, indent=4))\n","            # print(json.dumps(json_notebook_definition, indent=4))\n","\n","            # base64 encoding for the api call\n","            json_notebook_definition_new_encoded = base64.b64encode(json.dumps(json_notebook_definition_new, indent=4).encode('utf-8')).decode('utf-8')\n","\n","\n","            # 3. update the notebook definition\n","\n","            # set the url for the update\n","            vUrl = vBaseUrl + f\"workspaces/{vTargetWorkspaceId}/notebooks/{vTargetSqlNotebookId}/updateDefinition\"\n","\n","            # set the body\n","            vJsonBody = {\n","                \"definition\": {\n","                    \"format\": \"ipynb\",\n","                    \"parts\": [\n","                        {\n","                            \"path\": \"notebook-content.py\",\n","                            \"payload\": f\"{json_notebook_definition_new_encoded}\",\n","                            \"payloadType\": \"InlineBase64\"\n","                        }\n","                    ]\n","                }\n","            }\n","\n","\n","            # update the notebook definition\n","            # the update notebook definition as of 02.2025 has an issue when executin the operation url when the response status code is 202\n","            # it returns an error although the update is successful\n","            create_or_update_fabric_item(vUrl, vHeaders, vJsonBody, 'post', \"updating\", vTargetWorkspaceId, vTargetSqlNotebookName, \"Notebook\", vSleepInSeconds, pDebugMode) \n","\n","            # 4. run the notebook\n","\n","            # set the url\n","            vUrl = vBaseUrl + f\"workspaces/{vTargetWorkspaceId}/items/{vTargetSqlNotebookId}/jobs/instances?jobType=RunNotebook\"\n","\n","            # run the notebook\n","            create_or_update_fabric_item(vUrl, vHeaders, None, 'post', \"executing\", vTargetWorkspaceId, vTargetSqlNotebookName, \"Notebook\", vSleepInSeconds, pDebugMode) \n","\n","        # logging\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'create sql objects in target lakehouses', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'create sql objects in target lakehouses', datetime.now(), None, vMessage, str(e)]\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b65162c3-a325-46c9-afba-a12ce2a51810"},{"cell_type":"markdown","source":["**Delete notebooks created in previous steps**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59cb2579-8d6f-4e79-a8da-56406bb0fc62"},{"cell_type":"code","source":["try:\n","    for lakehouse in df_source_lakehouses['Lakehouse Name']:\n","\n","        # set the lakehouse name\n","        vLakehouseName = lakehouse\n","\n","        # define the target notebook name\n","        vTargetNotebookName = \"nb_\" + vLakehouseName + \"_definition\"\n","        vTargetSqlNotebookName = \"nb_\" + vLakehouseName + \"_sql_definition\"\n","\n","        # delete the notebooks\n","        notebookutils.notebook.delete(vTargetNotebookName, workspaceId=vTargetWorkspaceId)\n","        notebookutils.notebook.delete(vTargetSqlNotebookName, workspaceId=vTargetWorkspaceId)\n","\n","        # logging\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'delete temporary notebooks', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'delete temporary notebooks', datetime.now(), None, vMessage, str(e)]\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6392d38d-47f1-47ea-92be-889ec260ec2e"},{"cell_type":"markdown","source":["**Logging**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c3ac9c82-9c12-453b-b650-9d79ab90ca1b"},{"cell_type":"code","source":["try:\n","    # perform the conversion of columns\n","    dfLogging = dfLogging.astype({\n","            \"LoadId\": \"string\",\t\n","            \"NotebookId\": \"string\", \t\n","            \"NotebookName\": \"string\", \n","            \"WorkspaceId\": \"string\", \n","            \"CellId\": \"string\", \n","            \"Timestamp\": \"datetime64[ns]\", \n","            \"ElapsedTime\": \"string\", \n","            \"Message\": \"string\", \n","            \"ErrorMessage\" : \"string\"\n","        })\n","\n","    # save panda dataframe to a spark dataframe \n","    sparkDF_Logging = spark.createDataFrame(dfLogging) \n","\n","    # save to the lakehouse\n","    sparkDF_Logging.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(\"staging.notebook_logging_cicd\")\n","\n","except Exception as e:\n","    vMessage = \"saving logs to the lakehouse failed\"\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"95915092-87ee-4735-b698-571e2eb13cde"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"widgets":{},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}