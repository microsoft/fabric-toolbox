{"cells":[{"cell_type":"markdown","source":["##### Imports and Utility Functions\n","Ensure the cell below is runs successfully to include all the helper utilities"],"metadata":{},"id":"9763a417-c31f-4eec-bd40-e3998c10f1d1"},{"cell_type":"code","source":["%run workspaceutils"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"f17c4733-24b5-44c0-95e6-0cc3363efe74"},{"cell_type":"markdown","source":["##### Prerequisites\n","Please read the disaster recovery guidance found in <a href=\"https://learn.microsoft.com/en-us/fabric/security/experience-specific-guidance\">the documentation</a> to obtain a general understanding of how to recover lakehouse and warehouse data.\n","When the OneLake DR setting has been enabled at capacity level, lakehouse and warehouse data will be geo-replicated to the secondary (paired) region - which may not be accessible through the normal Fabric UI experience. Therefore, in a DR scenario, this data will need to be recovered into a corresponding (new) workspace in the DR region using the storage endpoints (abfs paths) as depicted in the image below. \n","<!--<div style=\"margin: 0 auto; text-align: center; overflow: hidden;\">\n","<div style=\"float: left;\"> -->\n","<img src=\"https://github.com/hurtn/images/blob/main/reco_from_docs.png?raw=true\" width=\"800\"/>\n","<!--<small><b>Figure 1</b></small></div></div><br> -->\n","\n","To use this recovery notebook please ensure:<p>&nbsp;</p>\n","1. You have imported and run the \"01 - Run in Primary\" notebook in the primary region.\n","<!-- <div style=\"margin: 0 auto; text-align: center; overflow: hidden;\">\n","<div style=\"float: left;\"> -->\n","<img src=\"https://github.com/hurtn/images/blob/main/before_recovery.png?raw=true\" width=\"800\"/>\n","<!-- <small><b>Figure 2</b></small></div></div><br> --><p>&nbsp;</p>\n","2. There is at least one capacity (C2) created in the DR region. \n","<img src=\"https://github.com/hurtn/images/blob/main/starting_reco_stage1v2.png?raw=true\" width=\"800\"/>\n","<!--<small><b>Figure 3</b></small> </div></div><br> --><p>&nbsp;</p>\n","3. A recovery workspace (Reco2) has been created in the secondary region (C2.Reco2) which contains this notebook \"02 - Run in DR\" and attached to a default lakehouse (C2.Reco2.LH_BCDR).\n","<img src=\"https://github.com/hurtn/images/blob/main/starting_reco_stage2v2.png?raw=true\" width=\"800\"/>\n","<!-- <small><b>Figure 4</b></small> </div></div> -->\n","<p>&nbsp;</p>\n","4. Set the values for the notebook parameters below according to your item names. Sample names are shown below if using the naming scheme in the image above. Note: If passing these parameters in from a pipeline these defaults will be overridden:<p>&nbsp;</p>\n","<left>\n","p_bcdr_workspace_src -> Reco1 <br>\n","p_bcdr_workspace_tgt -> Reco2  <br>\n","p_bcdr_lakehouse_src -> LH_BCDR <br>\n","p_bcdr_lakehouse_tgt -> LH_BCDR <br>\n","p_secondary_ws_suffix eg '_SECONDARY' - specify a suffix which will be appended to each workspace created in the DR region. This is to ensure uniqueness of workspace names and is useful for identifying these worksspaces when testing in a non-DR scenario also.  <br>\n","p_recovered_object_suffix eg '_recovered'- specify a suffix that will be appended to each table name created in the recovery lakehouse C2.Reco2.LH_BCDR <br>\n","list_of_workspaces_to_recover eg ['WS1','WS2'] - specify a list of workspaces to specifically recover. This is useful for testing in a non-DR scenario also. Leave empty [''] for all workspaces.  <br>\n"],"metadata":{},"id":"418635da-7b80-4e05-8054-0bfe200966be"},{"cell_type":"code","source":["# Specify the source and target BCDR workspaces and lakehouses\n","p_bcdr_workspace_src = '' #'BCDR'\n","p_bcdr_workspace_tgt = '' #'BCDR_DR'\n","p_bcdr_lakehouse_src = '' #'bcdrmeta'\n","p_bcdr_lakehouse_tgt = '' #'bcdrmeta'\n","\n","# Specify the DR capacity ID or name. If left blank then the capacity of this workspace will be used\n","target_capacity = '' \n","\n","# This variable adds a suffix to the name of the new workspaces created to ensure there are no naming conflicts with the original workspace name. \n","# Ensure that you use a string that will gaurantee uniqueness rather than common terms which may be used by others in day to day activities.  \n","p_secondary_ws_suffix = '_DR'\n","p_recovered_object_suffix = '_recovered'\n","\n","# Determines whether to add role assignments to the new workspaces. If you prefer to apply these at a later stage set the value to False. \n","p_add_ws_role_assignments = True\n","\n","# List parameters below need to be in the format of ['string1','string2',...'stringn']. Empty lists must be declared as []\n","# Specify the list of workspaces to recover, leave empty [] to recover all. For specific workspaces e.g. p_list_of_workspaces_to_recover = ['Workspace1','Workspace2'] \n","p_list_of_workspaces_to_recover = [] #['Prod1','Prod2'] #to specify exact workspaces\n","# Specify an exact list of workspaces to ignore e.g. p_ws_ignore_list = ['Microsoft Fabric Capacity Metrics 26/02/2024 16:15:42','AdminInsights']\n","p_ws_ignore_list = [] # add workspaces to this list to ignore them from the metadata extract process including git details\n","# Specify a list with wildcards using % e.g. to ignore anything with _DEV and _TEST as a suffix p_ws_ignore_like_list = ['%_DEV%','%_TEST%']  \n","p_ws_ignore_like_list  = [] #['%_DEV%','%_TEST%','%CLONE%']   #eg to specify exact ignore list ['BCDR']\n","\n","# Boolean parameter to specify verbose informational messages. \n","# Only set to True if additional logging information required, otherwise notebook may generate significant (print) messages.\n","p_logging_verbose = False"],"outputs":[],"execution_count":null,"metadata":{"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"72a724f2-a06c-4ae0-abcd-cf409ff21ef1"},{"cell_type":"markdown","source":["###### Check default lakehouse"],"metadata":{},"id":"a0038bfb-7895-4972-a9f9-0b2bb0edfa1b"},{"cell_type":"code","source":["if (notebookutils.runtime.context['defaultLakehouseId']==None):\n","    displayHTML('<div style=\"display: flex; align-items: flex-end;\"><img style=\"float: left; margin-right: 10px;\" src=\"https://github.com/hurtn/images/blob/main/stop.png?raw=true\" width=\"50\"><span><h4>Please set a default lakehouse before proceeding</span><img style=\"float: right; margin-left: 10px;\" src=\"https://github.com/hurtn/images/blob/main/stop.png?raw=true\" width=\"50\"></div>')\n","    print('\\n')\n","    raise noDefaultLakehouseException('No default lakehouse found. Please add a lakehouse to this notebook.')\n","else: \n","    print('Default lakehouse is set to '+ notebookutils.runtime.context['defaultLakehouseName'] + '('+ notebookutils.runtime.context['defaultLakehouseId'] + ')')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6976741f-e870-4bcd-93ab-9831c9c64c10"},{"cell_type":"markdown","source":["###### Check target capacity"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ffc1cf66-62b4-42f2-9007-686e1480f16b"},{"cell_type":"code","source":["# Verify target capacity and status\n","if target_capacity != '':\n","    if _is_valid_uuid(target_capacity):\n","        target_capacity_id = target_capacity.lower()\n","    else: \n","        # fetch the capacity ID\n","        target_capacity_id = labs.resolve_capacity_id(target_capacity)\n","else: # if not set then fetch capacity of this workspace\n","    target_capacity_id = labs.resolve_capacity_id()\n","\n","# check capacity status\n","cap_status = get_capacity_status(target_capacity_id)\n","if cap_status == 'Inactive':\n","    raise ValueError(f\"Status of capacity {target_capacity} is {cap_status}. Please resume the capacity and retry\")\n","else:\n","    print(f\"Status of capacity {target_capacity} is {cap_status}\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"09453fea-8b53-4fd2-bdd4-89fbc06b0750"},{"cell_type":"markdown","source":["##### Stage 4: Recover metadata tables from the BCDR workspace\n","\n","In order for the recovery process to begin, it needs the metadata of the primary environment (created by running the Store DR Metadata notebook) such as workspaces. This data will be persisted as tables in the default lakehouse of this notebook.\n","<div>\n","<img src=\"https://github.com/hurtn/images/blob/main/starting_reco_stage2v3.png?raw=true\" width=\"800\"/>\n","</div>\n"],"metadata":{},"id":"25b73794-5e68-416a-b4dc-48e69816deca"},{"cell_type":"code","source":["# Gathers the list of recovers tables and source paths to be copied into the lakehouse associated with this notebook \n","\n","src_path = f'abfss://{p_bcdr_workspace_src}@onelake.dfs.fabric.microsoft.com/{p_bcdr_lakehouse_src}.Lakehouse'\n","\n","table_list = get_lh_object_list(src_path)\n","print('The following tables will attempt to be recovered and persisted as tables in the default lakehouse of this notebook...')\n","display(table_list)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"25e8f044-afb0-4fc0-8f85-ebf4795e8306"},{"cell_type":"code","source":["print('Restoring recovery tables...')\n","res = copy_lh_objects(table_list[table_list['type']=='table'],p_bcdr_workspace_src,p_bcdr_workspace_tgt,\n","                      p_bcdr_lakehouse_src,p_bcdr_lakehouse_tgt,p_recovered_object_suffix,False)\n","print('Done.')\n","display(res)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"575fe555-256f-45e5-a2ff-572bce308a7d"},{"cell_type":"markdown","source":["##### Stage 5: Recreate workspaces\n","Recover the workspaces that used to exist in the primary. The suffix parameter will be appended to the workspace name\n","\n","<div>\n","<img src=\"https://github.com/hurtn/images/blob/main/Stage5_workspaces_created.png?raw=true\" width=\"800\"/>\n","</div>"],"metadata":{},"id":"973da036-09ac-4bf9-8fdf-12ae81d8c23a"},{"cell_type":"code","source":["thisWsId = notebookutils.runtime.context['currentWorkspaceId'] #obtaining this so we don't accidently delete this workspace!\n","\n","recovered_ws_sql = \"SELECT distinct ID,Type,Name,Capacity_Id FROM workspaces\" + p_recovered_object_suffix + \" where id != '\"+thisWsId+\"' and name != '\"+p_bcdr_workspace_src+\"'\"\n","\n","if len(p_list_of_workspaces_to_recover)>0:\n","  recovered_ws_sql = recovered_ws_sql+\" and Name in ('\" +  \"', '\".join(p_list_of_workspaces_to_recover)+ \"') \"\n","\n","if len(p_ws_ignore_list)>0:\n","   recovered_ws_sql = recovered_ws_sql+ \" and Name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        recovered_ws_sql  = recovered_ws_sql + \" and name not like '\" + notlike + \"'\"\n","\n","print('Recreating workspaces with suffix of '+ p_secondary_ws_suffix + '...')\n","#print(recovered_ws_sql)\n","df = spark.sql(recovered_ws_sql).collect()\n","for i in df:\n","    #print(i['ID'])\n","    if i['Type'] == 'Workspace':\n","      try:\n","        if p_logging_verbose:\n","          print(\"Creating workspace: \" + i['Name']+p_secondary_ws_suffix + \" in target capacity \"+ target_capacity +\"...\")\n","        response = fabric.create_workspace(i['Name']+p_secondary_ws_suffix,target_capacity_id)\n","        if p_logging_verbose:\n","          print(\"Created workspace with ID: \" + response)\n","      except Exception as error:\n","          errmsg =  \"Failed to recreate workspace \" + i['Name'] +p_secondary_ws_suffix + \" with capacity ID (\"+ i['Capacity_Id'] + \") due to: \"+str(error)\n","          print(errmsg)\n","print('Now reloading workspace metadata table...')\n","# Now popupate the workspace metadata table\n","saveWorkspaceMeta()\n","print('Done.')\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b612897d-7c19-4bf3-9838-4cc4a16e7625"},{"cell_type":"markdown","source":["#### Stage 6: Connect Git and Initialise\n","\n","\n","Iterate through workspaces and connect them to the same branch they were connected to in primary.\n","Then intialise and update from git to start synchronising items from the repo.\n","<div>\n","<img src=\"https://github.com/hurtn/images/blob/main/Stage6_gitconnected.png?raw=true\" width=\"800\"/>\n","</div>"],"metadata":{},"id":"01f11b9f-959d-4fd7-931f-cf2e1b1af158"},{"cell_type":"code","source":["gitsql = \"select gt.gitConnectionState,gt.gitProviderDetails, wks.name Workspace_Name, wks.id Workspace_ID from gitconnections_recovered gt \" \\\n","         \"inner join workspaces wks on gt.Workspace_Name = replace(wks.name,'\" + p_secondary_ws_suffix+ \"','') \" \\\n","         \"where gt.gitConnectionState = 'ConnectedAndInitialized' and wks.name like '%\"+p_secondary_ws_suffix+\"' and wks.id != '\"+thisWsId+\"'\" \n","\n","if len(p_list_of_workspaces_to_recover)>0:\n","    gitsql = gitsql+\" and gt.Workspace_Name in ('\"\"\" +  \"', '\".join(p_list_of_workspaces_to_recover)+ \"') \"\n","\n","if len(p_ws_ignore_list)>0:\n","    gitsql = gitsql+\" and gt.Workspace_Name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        gitsql  = gitsql + \" and name not like '\" + notlike + \"'\"\n","\n","print('Reconnecting workspaces to Git...')\n","df = spark.sql(gitsql).collect()\n","\n","for idx,i in enumerate(df):\n","    if i['gitConnectionState'] == 'ConnectedAndInitialized':\n","        \n","        url = \"/v1/workspaces/\" + i['Workspace_ID'] + \"/git/connect\"\n","        payload = '{\"gitProviderDetails\": ' + i['gitProviderDetails'] + '}'\n","        #print(str(payload))\n","\n","        try:\n","            if p_logging_verbose:\n","                print('Attempting to connect workspace '+ i['Workspace_Name'])\n","            response = client.post(url,json= json.loads(payload))\n","            if p_logging_verbose:\n","                print(str(response.status_code) + response.text) \n","            success = True\n","            \n","        except Exception as error:\n","            error_string = str(error)\n","            error_index = error_string.find(\"Error:\")\n","\n","            # Extract the substring after \"Error:\"\n","            error_message = error_string[error_index + len(\"Error:\"):].strip()\n","            headers_index = error_message.find(\"Headers:\")\n","\n","            # Extract the substring before \"Headers:\"\n","            error_message = error_message[:headers_index].strip()\n","            error_data = json.loads(error_message)\n","            # Extract the error message\n","            error_message = error_data.get(\"message\", \"\")\n","\n","            errmsg =  \"Couldn't connect git to workspace \" + i['Workspace_Name'] + \"(\"+ i['Workspace_ID'] + \"). Error: \"+str(error_message)\n","            print(str(errmsg))\n","            success = False\n","        # If connection successful then try to initialise    \n","        if (success):\n","            url = \"/v1/workspaces/\" + i['Workspace_ID'] + \"/git/initializeConnection\"\n","            payload = {\"initializationStrategy\":\"PreferRemote\"}\n","            try:\n","                if p_logging_verbose:\n","                    print('Attempting to initialize git connection for workspace '+ i['Workspace_Name'])\n","                response = client.post(url,json= payload)\n","                #print(str(response.status_code) + response.text) \n","                commithash = response.json()['remoteCommitHash']\n","                if p_logging_verbose:\n","                    print('Successfully initialized. Updating with commithash '+commithash)\n","                if commithash!='':\n","                    url = \"/v1/workspaces/\" + i['Workspace_ID'] + \"/git/updateFromGit\"\n","                    payload = '{\"remoteCommitHash\": \"' + commithash + '\",\"conflictResolution\": {\"conflictResolutionType\": \"Workspace\",\"conflictResolutionPolicy\": \"PreferWorkspace\"},\"options\": {\"allowOverrideItems\": true}}'\n","                    response = client.post(url,json= json.loads(payload))\n","                    if response.status_code==202:\n","                        print('Successfully started sync, LRO in progress...')\n","                        location_url = response.headers.get(\"Location\")\n","                        print(f\"Polling URL to track operation status is {location_url}\")\n","                        time.sleep(15)\n","                        response = long_running_operation_polling(location_url, 15)\n","\n","            except Exception as error:\n","                success = False\n","                error_string = str(error)\n","                error_index = error_string.find(\"Error:\")\n","\n","                # Extract the substring after \"Error:\"\n","                error_message = error_string[error_index + len(\"Error:\"):].strip()\n","                headers_index = error_message.find(\"Headers:\")\n","\n","                # Extract the substring before \"Headers:\"\n","                error_message = error_message[:headers_index].strip()\n","                error_data = json.loads(error_message)\n","                # Extract the error message\n","                error_message = error_data.get(\"message\", \"\")\n","                errmsg =  \"Couldn't initialize git for workspace \" + i['Workspace_Name'] + \"(\"+ i['Workspace_ID'] + \"). Error: \"+str(error_message)\n","                print(str(errmsg))\n","\n","                        \n","if success:        \n","    print('Done')\n","else:\n","    print('Completed with errors.')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5c1dc688-abc1-4323-a57b-0c068f4d803a"},{"cell_type":"markdown","source":["##### After git sync completes then capture environment metadata (items and reports)  \n","#TODO: Need to write a method to check the sync is complete before attempting to extract the items. For now you will need to wait until the sync has complete and all workspaces have been updated from git. For now adding a sleep into the process to make sure it waits a minute for the sync to complete."],"metadata":{},"id":"68e2d4a5-95ac-49f2-b33b-e14c29158b21"},{"cell_type":"code","source":["import time\n","print('Gathering metadata about reports and items... ')\n","saveItemMeta(verbose_logging=p_logging_verbose, ws_ignore_list=p_ws_ignore_list,ws_ignore_like_list=p_ws_ignore_like_list,list_of_workspaces_to_recover=p_list_of_workspaces_to_recover)\n","#saveReportMeta(verbose_logging=p_logging_verbose,only_secondary=True,ws_ignore_list=p_ws_ignore_list,ws_ignore_like_list=p_ws_ignore_like_list)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b655488c-9891-4eb3-ac20-05c463f17d5e"},{"cell_type":"markdown","source":["##### Stage 7: Recover lakehouse data (files and tables) to corresponding recovered workspaces\n","\n","Copy lakehouse data (files and tables) from source abfs path to target lakehouse abfs path\n","\n","<div>\n","<img src=\"https://github.com/hurtn/images/blob/main/lh_reocvery.png?raw=true\" width=\"800\"/>\n","</div>"],"metadata":{},"id":"80e7c7f2-cc43-49bc-812b-c668cc156f50"},{"cell_type":"code","source":["thisWsId = notebookutils.runtime.context['currentWorkspaceId'] #failsafe: obtaining this workspace id so we don't accidently overwrite objects in this workspace!\n","\n","data_recovery_sql = \"\"\"select wr.Name primary_ws_name, ir.type primary_type, ir.DisplayName primary_name,  \n","wr.id primary_ws_id, ir.id primary_id,wks.id secondary_ws_id, wks.Name secondary_ws_name, it.id secondary_id, \n","it.DisplayName secondary_name, cr.display_name capacity_name\n","from items_recovered ir \n","    inner join workspaces_recovered wr on wr.Id = ir.WorkspaceId \n","    inner join capacities_recovered cr on wr.capacity_id = cr.id\n","    inner join workspaces wks on wr.Name = replace(wks.name,'\"\"\" + p_secondary_ws_suffix+ \"\"\"','')\n","    inner join items it on ir.DisplayName = it.DisplayName and it.WorkspaceId = wks.Id \n","where ir.type in ('Lakehouse','Warehouse') and it.type = ir.type \"\"\"\n","\n","if len(p_list_of_workspaces_to_recover):\n","  data_recovery_sql = data_recovery_sql + \"\"\" and wr.Name in ('\"\"\" +  \"', '\".join(p_list_of_workspaces_to_recover)+ \"\"\"')\"\"\"\n","\n","data_recovery_sql = data_recovery_sql + \"\"\"and wks.name like '%\"\"\"+p_secondary_ws_suffix+\"\"\"'  and wks.id != '\"\"\"+thisWsId+\"\"\"' order by ir.type,primary_ws_name\"\"\"\n","\n","#print(data_recovery_sql)\n","df_recovery_items=spark.sql(data_recovery_sql)\n","if df_recovery_items.count()>0:\n","    print(\"The subsequent notebook cells attempt to recover lakehouse and warehouse data for the following items...\")\n","else:\n","    print(\"No lakehouses and warehouses found to recover.\")\n","# populate dataframes for lakehouse metadata and warehouse metadata respectively \n","lakehousedf = df_recovery_items.filter(\"primary_type='Lakehouse'\").collect()\n","warehousedf = df_recovery_items.filter(\"primary_type='Warehouse'\").collect()\n","\n","display(df_recovery_items.select(\"primary_ws_name\",\"primary_type\",\"primary_name\",\"secondary_ws_name\",\"secondary_name\", \"secondary_ws_id\", \"capacity_name\"))\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"37080690-4215-47d1-b5a2-be095cf0005a"},{"cell_type":"markdown","source":["Traceback (most recent call last) Cell In[37], \n","line 30     \n","28 table_list = get_wh_object_list(schema_list['name'],src_path)     \n","29 #recover tables---> \n","30 copy_lh_objects(table_list[table_list['type']=='table'],i['primary_ws_id'], i['secondary_ws_id'] ,i['primary_id'],i['secondary_id'],p_recovered_object_suffix,False,True)     \n","31 table_list = get_lh_object_list(src_path,['Files'])     \n","32 #recover files File ~/cluster-env/trident_env/lib/python3.11/site-packages/pandas/core/frame.py:3893, in DataFrame.__getitem__(self, key)   \n","3891 if self.columns.nlevels > 1:   3892     return self._getitem_multilevel(key)-> 3893 indexer = self.columns.get_loc(key)   3894 if is_integer(indexer):   3895     indexer = [indexer] File ~/cluster-env/trident_env/lib/python3.11/site-packages/pandas/core/indexes/range.py:418, in RangeIndex.get_loc(self, key)    416         raise KeyError(key) from err    417 if isinstance(key, Hashable):--> 418     raise KeyError(key)    419 self._check_indexing_error(key)    420 raise KeyError(key) KeyError: 'type' \n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d0409700-6467-499f-910d-dd7bffced193"},{"cell_type":"code","source":["def isSchemaEnabledLakehouse(p_workspace, p_lakehouse):\n","    if _is_valid_uuid(p_workspace):\n","        p_workspace_id = p_workspace\n","    else:\n","        p_workspace_id = fabric.resolve_workspace_id(p_workspace)\n","\n","    if _is_valid_uuid(p_lakehouse):\n","        p_lakehouse_id = p_lakehouse\n","    else:\n","        p_lakehouse_id = fabric.resolve_item_id(p_lakehouse,'Lakehouse',p_workspace)\n","    jres = client.get(f'v1/workspaces/{p_workspace_id}/lakehouses/{p_lakehouse_id}').json()\n","    if \"defaultSchema\" in jres['properties']:\n","        return True\n","    else:\n","        return False\n","\n","print('The following lakehouse(s) will attempt to be recovered... ')\n","display(lakehousedf)\n","\n","for idx,i in enumerate(lakehousedf):\n","    # Define the full abfss source path of the primary BCDR workspace and lakehouse \n","    src_path = f'abfss://'+i['primary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['primary_id']\n","    if p_logging_verbose:\n","        print('Attempting to recover items for workspace: ' + i['primary_ws_name'] + ', lakehouse: ' + i['primary_name'] + ' into target workspace ' + i['secondary_ws_name'] + ' lakehouse ' + i['secondary_name'])\n","    if isSchemaEnabledLakehouse(i['primary_ws_id'],i['primary_id']):\n","        schema_list = get_lh_object_list(src_path,['Tables'])\n","        table_list = get_wh_object_list(schema_list['name'],src_path)\n","        if len(table_list)>0:         #check there are tables present in the lakehouse before attempt to copy\n","            #recover tables\n","            copy_lh_objects(table_list[table_list['type']=='table'],i['primary_ws_id'], i['secondary_ws_id'] ,i['primary_id'],i['secondary_id'],p_recovered_object_suffix,False,True)\n","            table_list = get_lh_object_list(src_path,['Files'])\n","            #recover files\n","            copy_lh_objects(table_list[table_list['type']=='file'],i['primary_ws_id'], i['secondary_ws_id'] ,i['primary_id'],i['secondary_id'],p_recovered_object_suffix,False,True)\n","        else:\n","            print(f'No tables found in lakehouse {i[\"primary_name\"]}')\n","    else:\n","        table_list = get_lh_object_list(src_path)\n","        copy_lh_objects(table_list[table_list['type']=='file'],i['primary_ws_id'], i['secondary_ws_id'] ,i['primary_id'],i['secondary_id'],p_recovered_object_suffix,False,True)\n","        #recover tables\n","        copy_lh_objects(table_list[table_list['type']=='table'],i['primary_ws_id'], i['secondary_ws_id'] ,i['primary_id'],i['secondary_id'],p_recovered_object_suffix,False,True)\n","\n","print('Done')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e468785b-0d41-459a-83a2-39e48275d8f8"},{"cell_type":"markdown","source":["##### Stage 8: Prepare warehouse recovery\n","###### Important: \n","This process creates a staging lakehouse to store shortcuts which point back to the DR copy of the tables (actually delta folders) - in testing this will point back to folders in the primary region but in a true DR scenario where Microsoft has failed over the OneLake endpoints, they will point back to folders in the secondary (paired) storage region.\n","A parameterised pipeline is injected into each target workspace which will load target tables from these shortcuts.\n","This staging lakehouse and pipeline can be deleted manually after the datawarehouse tables have been successfully recovered.\n","<div>\n","<img src=\"https://github.com/hurtn/images/blob/main/wh_recovery.png?raw=true\" width=\"1000\"/>\n","</div>"],"metadata":{},"id":"c15613f9-024a-4f0e-9973-d508341df2c2"},{"cell_type":"code","source":["def get_token(audience=\"pbi\"):\n","    return notebookutils.credentials.getToken(audience)\n","\n","storageclient = fabric.FabricRestClient(token_provider=get_token)\n","\n","print('The following warehouse(s) will attempt to be recovered... ')\n","display(warehousedf)\n","print('\\nPreparing for recovery...\\n')\n","# interate through all the data warehouses to recover\n","for idx,i in enumerate(warehousedf):\n","    if p_logging_verbose:\n","        print('Setting up for recovery of warehouse '+i['primary_ws_name'] + '.'+i['primary_name'] + ' into ' + i['secondary_ws_name'] + '.'+i['secondary_name'] )\n","\n","    src_path = f'abfss://'+i['primary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['primary_id']\n","    tgt_path = f'abfss://'+i['secondary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['secondary_id']\n","\n","    # extract the list of schemas per data \n","    schema_list = get_lh_object_list(src_path,['Tables'])\n","    # extract a list of warehouse objects per schema and store in a list\n","    table_list = get_wh_object_list(schema_list['name'],src_path)\n","  \n","    # create a temporary staging lakehouse per warehouse to create shortcuts into, \n","    # which point back to original warehouse data currently in the DR storage account\n","    lhname = 'temp_rlh_' + i['primary_ws_name']+'_'+i['primary_name']\n","    # check if it exists before attempting create\n","    if p_logging_verbose:\n","        print('Checking whether the temporary lakehouse \"'+ lhname +'\" exists in workspace '+i['secondary_ws_name']+'...')\n","    temp_lh_id = getItemId(i['secondary_ws_id'],lhname,'Lakehouse')\n","    if temp_lh_id == 'NotExists':\n","        lhname = 'temp_rlh_' + i['primary_ws_name']+'_'+i['primary_name'][:256] # lakehouse name should not exceed 256 characters\n","        payload = payload = '{\"displayName\": \"' + lhname + '\",' \\\n","        + '\"description\":  \"Interim staging lakehouse for primary warehouse recovery: ' \\\n","        + i['primary_ws_name']+'_'+i['primary_name'] + 'into workspace '+ i['secondary_ws_name'] + '(' + i['secondary_ws_id'] +')\"}'\n","        try:\n","            lhurl = \"v1/workspaces/\" + i['secondary_ws_id'] + \"/lakehouses\"\n","            lhresponse = client.post(lhurl,json= json.loads(payload))\n","            temp_lh_id = lhresponse.json()['id']\n","            if p_logging_verbose:\n","                print('Temporary lakehouse \"'+ lhname +'\" created with Id ' + temp_lh_id + ': ' + str(lhresponse.status_code) + ' ' + str(lhresponse.text))\n","        except Exception as error:\n","            print(error.errorCode)\n","    else:\n","        if p_logging_verbose:\n","            print('Temporary lakehouse '+lhname+' (' + temp_lh_id + ') already exists.')\n","        \n","\n","    # Create shortcuts for every table in the format of schema_table under the tables folder\n","    for index,itable in table_list.iterrows():\n","        shortcutExists=False\n","        # Check if shortcut exists\n","        try:\n","            url = \"v1/workspaces/\" + i['secondary_ws_id'] + \"/items/\" + temp_lh_id + \"/shortcuts/Tables/\"+itable['schema']+'_'+itable['name']\n","            tlhresponse = storageclient.get(url)\n","            shortcutExists = True\n","            if p_logging_verbose:\n","                print('Shortcut '+itable['schema']+'_'+itable['name'] +' already exists')\n","        except Exception as error:\n","            shortcutExists = False    \n","\n","        if not shortcutExists: \n","            # Create shortcuts - one per table per schema\n","            url = \"v1/workspaces/\" + i['secondary_ws_id'] + \"/items/\" + temp_lh_id + \"/shortcuts\"\n","            scpayload = '{' \\\n","            '\"path\": \"Tables/\",' \\\n","            '\"name\": \"'+itable['schema']+'_'+itable['name']+'\",' \\\n","            '\"target\": {' \\\n","            '\"oneLake\": {' \\\n","                '\"workspaceId\": \"' + i['primary_ws_id'] + '\",' \\\n","                '\"itemId\": \"'+ i['primary_id'] +'\",' \\\n","                '\"path\": \"/Tables/' + itable['schema']+'/'+itable['name'] + '\"' \\\n","                '}}}' \n","            try:\n","                #print(scpayload)                \n","                shctresponse = storageclient.post(url,json= json.loads(scpayload))\n","                if p_logging_verbose:\n","                    print('Shortcut '+itable['schema']+'_'+itable['name'] + ' created.' )\n","\n","            except Exception as error:\n","                print('Error creating shortcut '+itable['schema']+'_'+itable['name']+' due to '+str(error) + ':' + shctresponse.text)\n","    \n","    recovery_pipeline_prefix= 'plRecover_WH6'       \n","    # recovery pipeline name should not exceed 256 characters\n","    recovery_pipeline = recovery_pipeline_prefix+'_'+i['primary_ws_name'] + '_'+i['primary_name'][:256]\n","    if p_logging_verbose:\n","        print('Attempting to deploy a copy pipeline in the target workspace to load the target warehouse tables from the shortcuts created above... ')\n","    # Create the pipeline in the target workspace that loads the target warehouse from shortcuts created above \n","    plid = getItemId( i['secondary_ws_id'],recovery_pipeline,'DataPipeline')\n","    #print(plid)\n","    if plid == 'NotExists':\n","      plid = createDWrecoverypl(i['secondary_ws_id'],recovery_pipeline_prefix+'_'+i['primary_ws_name'] + '_'+i['primary_name'])\n","      if p_logging_verbose:\n","          print('Recovery pipeline ' + recovery_pipeline + ' created with Id '+plid)\n","    else:\n","      if p_logging_verbose:\n","          print('Datawarehouse recovery pipeline \"' + recovery_pipeline + '\" ('+plid+') already exist in workspace \"'+i['secondary_ws_name'] + '\" ('+i['secondary_ws_id']+')')  \n","          print('\\n')\n","print('Done')     \n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"616c878c-a3c1-4072-9dc9-70898af090be"},{"cell_type":"code","source":["# wait for sql endpooint sync to reflect the new shortcuts that have just been created\n","time.sleep(60)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"267400ca-5d10-4f9f-94b3-2a6520112f00"},{"cell_type":"markdown","source":["##### Stage 9: Recover warehouse data by running copy pipelines"],"metadata":{},"id":"c6c3e8aa-a57e-4785-b549-907b4fca30b5"},{"cell_type":"code","source":["print('Starting warehouse recovery pipelines...')\n","# interate through all the data warehouses to recover\n","for idx,i in enumerate(warehousedf):\n","    if p_logging_verbose:\n","        print('Invoking pipeline to copy warehouse data from  '+i['primary_ws_name'] + '.'+i['primary_name'] + ' into ' + i['secondary_ws_name'] + '.'+i['secondary_name'] )\n","\n","    src_path = f'abfss://'+i['primary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['primary_id']\n","    #tgt_path = f'abfss://'+i['secondary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['secondary_id']\n","\n","    # extract the list of schemas per data \n","    schema_list = get_lh_object_list(src_path,['Tables'])\n","    #display(schema_list)\n","    # extract a list of warehouse objects per schema and store in a list\n","    table_list = get_wh_object_list(schema_list['name'].to_list(),src_path)\n","\n","    tablesToCopyParam = table_list[['schema','name']].to_json( orient='records')\n","    # ensure the temporary lakehouse exists\n","    lhname = 'temp_rlh_' + i['primary_ws_name']+'_'+i['primary_name']\n","    temp_lh_id = getItemId(i['secondary_ws_id'],lhname,'Lakehouse')\n","    #temp_lh_id ='0f0f6b7c-1761-41e6-896e-30014f16ff6d'\n","    \n","    # obtain the connection string for the lakehouse to pass to the copy pipeline\n","    whurl  = \"v1/workspaces/\" + i['secondary_ws_id'] + \"/lakehouses/\" + temp_lh_id\n","    whresponse = client.get(whurl)\n","    lhconnStr = whresponse.json()['properties']['sqlEndpointProperties']['connectionString']\n","\n","    # get the SQLEndpoint ID of the lakehouse to pass to the copy pipeline\n","    items = fabric.list_items(workspace=i['secondary_ws_id'])\n","    temp_lh_sqle_id = items[(items['Type'] == 'SQLEndpoint') & (items['Display Name']==lhname)]['Id'].values[0]\n","\n","\n","    # obtain the connection string for the warehouse to pass to the copy pipeline    \n","    whurl  = \"v1/workspaces/\" + i['secondary_ws_id'] + \"/warehouses/\" + i['secondary_id']\n","    whresponse = client.get(whurl)\n","    whconnStr = whresponse.json()['properties']['connectionInfo']\n","\n","    recovery_pipeline = recovery_pipeline_prefix+'_'+i['primary_ws_name'] + '_'+i['primary_name'][:256]\n","    # obtain the pipeline id created to recover this warehouse\n","    plid = getItemId( i['secondary_ws_id'],recovery_pipeline,'DataPipeline')\n","    if plid == 'NotExists':\n","        print('Error: Could not execute pipeline '+recovery_pipeline+ ' as the ID could not be obtained ')\n","    else:\n","        # pipeline url including pipeline Id unique to each warehouse\n","        plurl = 'v1/workspaces/'+i['secondary_ws_id'] +'/items/'+plid+'/jobs/instances?jobType=Pipeline'\n","        #print(plurl)\n","\n","        payload_data = '{' \\\n","            '\"executionData\": {' \\\n","                '\"parameters\": {' \\\n","                    '\"lakehouseId\": \"' + temp_lh_sqle_id + '\",' \\\n","                    '\"tablesToCopy\": ' + tablesToCopyParam + ',' \\\n","                    '\"workspaceId\": \"' + i['secondary_ws_id'] +'\",' \\\n","                    '\"warehouseId\": \"' + i['secondary_id'] + '\",' \\\n","                    '\"lakehouseConnStr\": \"' + lhconnStr + '\",' \\\n","                    '\"warehouseConnStr\": \"' + whconnStr + '\"' \\\n","                    '}}}'\n","        print(payload_data)\n","        plresponse = client.post(plurl, json=json.loads(payload_data))\n","        if p_logging_verbose:\n","            print(str(plresponse.status_code))      \n","print('Done')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"096ae22f-06f5-489c-b208-74744c7402ef"},{"cell_type":"markdown","source":["##### Stage 10: Add workspace roles assignments to the new workspaces"],"metadata":{},"id":"24a050ff-6bd3-4202-9df1-b6f8b4436655"},{"cell_type":"code","source":["if p_add_ws_role_assignments:\n","    ws_role_sql = \"SELECT wks.ID new_workspace_id, wks.name new_workspace, rar.* FROM wsroleassignments_recovered rar inner join workspaces wks on rar.workspaceName = replace(wks.Name,'\" + p_secondary_ws_suffix+ \"','')\" \\\n","                \"where wks.name like '%\"+p_secondary_ws_suffix+\"' and wks.id != '\"+thisWsId+\"'\" \n","\n","    # Only apply roles to the (new) workspaces based the list of workspaces defined in the parameter section at the top of this notebook. \n","    # Note that the list is based on the workspace name defined in the primary but will be translated to the associated (new) workspace recently created in the secondary.\n","    if len(p_list_of_workspaces_to_recover)>0:\n","        ws_role_sql = ws_role_sql+\" and rar.workspaceName in ('\" +  \"', '\".join(p_list_of_workspaces_to_recover)+ \"') \"\n","\n","    # Ingore workspaces based on the ignore list defined in the parameter section at the top of this notebook\n","    if len(p_ws_ignore_list)>0:\n","        ws_role_sql = ws_role_sql+ \" and rar.workspaceName not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","    if len(p_ws_ignore_like_list)>0:\n","        for notlike in p_ws_ignore_like_list:\n","            ws_role_sql  = ws_role_sql + \" and name not like '\" + notlike + \"'\"\n","    \n","    print('Adding workspace role assignments...')\n","\n","    #print(ws_role_sql)\n","    dfwsrole = spark.sql(ws_role_sql).collect()\n","    for idx,i in enumerate(dfwsrole):\n","        wsroleurl = \"/v1/workspaces/\" + i['new_workspace_id'] + \"/roleAssignments\"\n","        wsrolepayload = '{\"principal\": {\"id\": \"'+i['principalId']+'\", \"type\": \"'+i['principalType']+'\"},\"role\": \"'+i['role']+'\"}'\n","        #print(str(wsrolepayload))\n","        \n","        try:\n","            if p_logging_verbose:\n","                print(\"Attempting to add role assignments \" + i['role'] + \" for \" +  i['principalType'] + \" princpal \" +i['displayName'] + \" (\" +i['principalId'] + \") to workspace \" + i['new_workspace'] + \"(\"+ i['new_workspace_id'] + \")...\")\n","\n","            response = client.post(wsroleurl,json= json.loads(wsrolepayload))\n","\n","            success = True\n","        except Exception as error:\n","            error_string = str(error)\n","            error_index = error_string.find(\"Error:\")\n","\n","            # Extract the substring after \"Error:\"\n","            error_message = error_string[error_index + len(\"Error:\"):].strip()\n","            headers_index = error_message.find(\"Headers:\")\n","\n","            # Extract the substring before \"Headers:\"\n","            error_message = error_message[:headers_index].strip()\n","            error_data = json.loads(error_message)\n","            # Extract the error message\n","            error_message = error_data.get(\"message\", \"\")\n","            if error_message is not None:\n","                errmsg =  \"Couldn't add role assignment \" + i['role'] + \" for princpal \" +i['displayName'] + \" to workspace \" + i['workspaceName'] + \"(\"+ i['workspaceId'] + \"). Error: \"+error_message\n","            else:\n","                errmsg =  \"Couldn't add role assignment \" + i['role'] + \" for princpal \" +i['displayName'] + \" to workspace \" + i['workspaceName'] + \"(\"+ i['workspaceId'] + \").\"\n","            print(str(errmsg))\n","            success = False\n","print('Done')\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4c5b32b8-3fa8-4c09-a242-abfe2832f67c"},{"cell_type":"markdown","source":["##### Update default lakehouse for notebooks\n","\n","Supports both standard and T-SQL notebooks"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"baea79df-8dae-4519-ad43-3fb2ba9803bc"},{"cell_type":"code","source":["## Load lookup dataframe for primary to DR workspace conversion\n","dfoldws = spark.sql(\"\"\"SELECT wr.Id old_ws_id, wr.Name old_ws_name, wks.Id new_ws_id, wks.Name new_ws_name \n","                       FROM workspaces_recovered wr inner join workspaces wks on wr.Name = replace(wks.Name,'\"\"\" + p_secondary_ws_suffix+ \"\"\"','')\n","                       where wks.name like '%\"\"\"+p_secondary_ws_suffix+\"\"\"' and wks.id != '\"\"\"+thisWsId+\"\"\"'\n","                        AND wks.name != '\"\"\" + p_bcdr_workspace_src + \"\"\"' \n","                       \"\"\" )\n","#display(dfoldws)\n","\n","# Load lookup dataframe for primary item details such as workspace \n","dfolditms = spark.sql(\"\"\"SELECT ir.id old_item_id, ir.displayName old_item_name, wr.Id old_ws_id, wr.Name old_ws_name, \n","                                wks.Id new_ws_id, wks.Name new_ws_name \n","                       FROM items_recovered ir INNER JOIN workspaces_recovered wr on ir.workspaceId = wr.Id \n","                       INNER JOIN workspaces wks on wr.Name = replace(wks.Name,'\"\"\" + p_secondary_ws_suffix+ \"\"\"','')\n","                        AND wks.name != '\"\"\" + p_bcdr_workspace_src + \"\"\"'  \n","                       where wks.name like '%\"\"\"+p_secondary_ws_suffix+\"\"\"' and wks.id != '\"\"\"+thisWsId+\"\"\"'\"\"\" )\n","\n","\n","ws_nb_sql = \"SELECT wks.ID new_workspace_id, wks.name new_workspace_name, itm.* \" + \\\n","                \"FROM items itm inner join workspaces wks on itm.workspaceId = wks.Id and wks.name != '\" + p_bcdr_workspace_src + \"'  \" + \\\n","                \"AND itm.type = 'Notebook' and wks.name like '%\"+p_secondary_ws_suffix+\"' and wks.id != '\"+thisWsId+\"'\"\n","\n","\n","# Ingore workspaces based on the ignore list defined in the parameter section at the top of this notebook\n","if len(p_ws_ignore_list)>0:\n","    ws_nb_sql = ws_nb_sql+ \" and wks.name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        ws_nb_sql  = ws_nb_sql + \" and wks.name not like '%\" + notlike + \"%'\"\n","\n","print('Looping through notebooks...')\n","\n","#print(ws_nb_sql)\n","dfwsnb = spark.sql(ws_nb_sql).collect()\n","#display(dfwsnb)\n","for idx,i in enumerate(dfwsnb):\n","    print(f\"Updating notebook {i['displayName']}\")\n","    # Get the current notebook definition\n","    notebook_def = notebookutils.notebook.getDefinition(i['displayName'],workspaceId=i['new_workspace_id'])\n","\n","    json_payload = json.loads(notebook_def)\n","    # update default lakehouse if exists\n","    if 'dependencies' in json_payload['metadata'] \\\n","        and 'lakehouse' in json_payload['metadata']['dependencies'] \\\n","        and json_payload['metadata'][\"dependencies\"][\"lakehouse\"] is not None:\n","        # Remove all lakehouses\n","        current_lakehouse = json_payload['metadata']['dependencies']['lakehouse']\n","\n","        if 'default_lakehouse_name' in current_lakehouse:\n","            # look up corresponding lakehouse workspace details\n","            old_lakehouse_id = current_lakehouse['default_lakehouse']\n","            #old_lakehouse_ws_name = dfoldws.filter(f\"old_ws_id=='{old_lakehouse_ws_id}'\").collect()[0][1]\n","            #new_lakehouse_ws_id = dfoldws.filter(f\"old_ws_id=='{old_lakehouse_ws_id}'\").collect()[0][2]\n","            #new_lakehouse_ws_name = dfoldws.filter(f\"old_ws_id=='{old_lakehouse_ws_id}'\").collect()[0][3]\n","\n","            old_lakehouse_ws_id,old_lakehouse_ws_name, new_lakehouse_ws_id, new_lakehouse_ws_name = dfolditms.filter(f\"old_item_id=='{old_lakehouse_id}'\").collect()[0][2:6]\n","\n","            print('Converting notebook ' + i['displayName'] + ' in workspace ' + i['new_workspace_name'] + '('+ i['new_workspace_id'] + ') with default lakehouse ' + \n","            current_lakehouse['default_lakehouse_name']+ ' in previous workspace '+old_lakehouse_ws_name + '('+ old_lakehouse_ws_id + ') to ' + \n","            new_lakehouse_ws_name + '(' + new_lakehouse_ws_id +')')\n","\n","            json_payload['metadata']['dependencies']['lakehouse'] = {}\n","            \n","            #Update new notebook definition after removing existing lakehouses and with new default lakehouseId\n","            (notebookutils.notebook.updateDefinition(\n","                        name = i['displayName'],\n","                        content  = json.dumps(json_payload),  \n","                        defaultLakehouse = current_lakehouse['default_lakehouse_name'],\n","                        defaultLakehouseWorkspace = new_lakehouse_ws_id,\n","                        workspaceId = new_lakehouse_ws_id\n","                        )\n","                )\n","            print(f\"Updated notebook {i['displayName']}\")\n","\n","        else:\n","            print(f\"No default lakehouse was found in the source notebook {i['displayName']}\")\n","\n","\n","    # update default warehouse if existss\n","    elif 'dependencies' in json_payload['metadata'] \\\n","        and 'warehouse' in json_payload['metadata']['dependencies'] \\\n","        and json_payload['metadata'][\"dependencies\"][\"warehouse\"] is not None:\n","\n","        # Fetch existing details\n","        current_warehouse = json_payload['metadata']['dependencies']['warehouse']\n","        old_warehouse_id = current_warehouse['default_warehouse']\n","        old_wh_name,old_warehouse_ws_id,old_warehouse_ws_name, new_warehouse_ws_id, new_warehouse_ws_name = dfolditms.filter(f\"old_item_id=='{old_warehouse_id}'\").collect()[0][1:6]\n","\n","        target_wh_id = fabric.resolve_item_id(item_name = old_wh_name,type='Warehouse',workspace=new_warehouse_ws_id)\n","\n","        if 'default_warehouse' in current_warehouse:\n","            print('Converting notebook ' + i['displayName'] + ' in workspace ' + i['new_workspace_name'] + '('+ i['new_workspace_id'] + ') with default warehouse ' + \n","            old_wh_name + ' in previous workspace '+old_warehouse_ws_name + '('+ old_warehouse_ws_id + ') to ' + \n","            old_wh_name + ' in new workspace '+new_warehouse_ws_name + '(' + new_warehouse_ws_id +')')\n","\n","        \n","            #Update new notebook definition after removing existing lakehouses and with new default lakehouseId\n","            json_payload['metadata']['dependencies']['warehouse']['default_warehouse'] = target_wh_id\n","            for warehouse in json_payload['metadata']['dependencies']['warehouse']['known_warehouses']:\n","                if warehouse['id'] == old_warehouse_id:\n","                    warehouse['id'] = target_wh_id\n","            #print(json.dumps(json_payload, indent=4))\n","            (notebookutils.notebook.updateDefinition(\n","                    name = i['displayName'],\n","                    content  = json.dumps(json_payload),\n","                    workspaceId = i['new_workspace_id']\n","                    )\n","            )\n","            print(f\"Updated notebook {i['displayName']}\")\n","\n","        else:\n","            print(f\"No default warehouse was found in the source notebook {i['displayName']}\")\n","\n","    else:\n","        print(f\"No default lakehouse/warehouse set for notebook {i['displayName']}, ignoring.\")\n","print('Done')\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"id":"889da16e-9163-44cd-ab98-bc9f7ddb818f"},{"cell_type":"markdown","source":["##### Update direct lake model lakehouse/warehouse connection\n","\n","This only converts the connection for non-default semantic models\n","\n","https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.directlake.html#sempy_labs.directlake.update_direct_lake_model_lakehouse_connection\n","    "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f547e4a9-ba87-4fbd-ab14-6e65b1005907"},{"cell_type":"code","source":["ws_sm_sql = \"SELECT wks.ID new_workspace_id, wks.name new_workspace_name, itm.* \" + \\\n","                \"FROM items itm inner join workspaces wks on itm.workspaceId = wks.Id \" + \\\n","                \"AND itm.type = 'SemanticModel' and wks.name like '%\"+p_secondary_ws_suffix+\"' \" \\\n","                \"AND wks.id != '\"+thisWsId+\"'\"\n","\n","# Ingore workspaces based on the ignore list defined in the parameter section at the top of this notebook\n","if len(p_ws_ignore_list)>0:\n","    ws_sm_sql = ws_sm_sql+ \" and wks.name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        ws_sm_sql  = ws_sm_sql + \" and wks.name not like '%\" + notlike + \"%'\"\n","\n","#print(ws_sm_sql)\n","dfwssm = spark.sql(ws_sm_sql).collect()\n","#display(dfwssm)\n","\n","# Iterate over each dataset in the dataframe\n","for idx,row in enumerate(dfwssm):\n","\n","    # Check if the dataset is not the default semantic model\n","    if not labs.is_default_semantic_model(row['displayName'], row['new_workspace_id']):\n","        print('Updating semantic model connection ' + row['displayName'] + ' in workspace '+ row['new_workspace_name'])\n","        old_ws_name =row['new_workspace_name'].replace(p_secondary_ws_suffix,'')\n","        labs.directlake.update_direct_lake_model_connection(dataset=row['displayName'], \n","                                                                        workspace= row['new_workspace_name'],\n","                                                                        source=get_direct_lake_source(row['displayName'], workspace= old_ws_name)[1], \n","                                                                        source_type=get_direct_lake_source(row['displayName'], workspace= old_ws_name)[0], \n","                                                                        source_workspace=row['new_workspace_name'])\n","        labs.refresh_semantic_model(dataset=row['displayName'], workspace= row['new_workspace_name'])\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"2c18c200-c678-422e-a07e-4922504d90c9"},{"cell_type":"markdown","source":["##### Update data pipeline source & sink connections\n","\n","Support changes lakehouses, warehouses, notebooks and connections from source to target. <br>\n","Connections changes should be expressed as an array of tuples [(from_1:to_1),)from_N:to_N)]\n","\n","Example usage:\n","    p_new_json = swap_pipeline_connection(pipeline_json,dfwsitms,\n","            ['Warehouse','Lakehouse','Notebook'],\n","            [('CONNECTION_ID_FROM','CONNECTION_ID_TO'),('CONNECTION_ID_FROM','CONNECTION_NAME_TO'),('CONNECTION_NAME_FROM','CONNECTION_ID_TO')]) \n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c9e590b0-0f78-435b-b6c5-a9e01ecc94e9"},{"cell_type":"code","source":["connections_from_to = [('1b478585-f04d-49bf-b31d-cd57c3d8ca25','11dd2898-018a-4d52-843a-3cac828984c4')]\n","\n","ws_itms_sql = \"\"\"select itm.type,itm_rec.Id old_item_id, itm.Id new_item_id, itm.displayName itm_name,\n"," wks_rec.Id old_ws_id, wks_rec.Name old_ws_name, wks.Id new_ws_id, wks.Name new_ws_name \n","from items itm\n","inner join workspaces wks on itm.workspaceId = wks.Id\n","inner join items_recovered itm_rec on itm.displayName = itm_rec.displayName and itm.type = itm_rec.type \n","inner join workspaces_recovered wks_rec on wks_rec.Name = replace(wks.Name,'\"\"\" + p_secondary_ws_suffix+ \"\"\"','') \n","and wks.id != '\"\"\"+thisWsId+\"\"\"' and wks_rec.Id = itm_rec.workspaceId\n","where itm_rec.Id != itm.Id and wks.Name != wks_rec.Name\"\"\"\n","\n","# Ingore workspaces based on the ignore list defined in the parameter section at the top of this notebook\n","if len(p_ws_ignore_list)>0:\n","    ws_itms_sql = ws_itms_sql+ \" and wks.name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        ws_itms_sql  = ws_itms_sql + \" and wks.name not like '%\" + notlike + \"%'\"\n","\n","dfwsitms = spark.sql(ws_itms_sql)\n","dfwspl =  dfwsitms.filter(f\"type =='DataPipeline'\").collect()\n","print('Looping through pipelines...')\n","#display(dfwsitms)\n","\n","for idx,i in enumerate(dfwspl):\n","\n","    #if i['itm_name']=='plRecover_WH6_Prod2_Warehouse2_fixed':\n","    pipeline_json = json.loads(labs.get_data_pipeline_definition(i['itm_name'],i['new_ws_name']))\n","    pipeline_json_bak = json.dumps(pipeline_json)\n","    p_new_json = swap_pipeline_connection(pipeline_json,dfwsitms,\n","            ['Warehouse','Lakehouse','Notebook'],\n","            connections_from_to) \n","    #print(json.dumps(pipeline_json, indent=4))\n","    if pipeline_json_bak != json.dumps(p_new_json): #only updae if there were changes\n","        update_data_pipeline_definition(name=i['itm_name'],pipeline_content=pipeline_json, workspace=i['new_ws_name'])\n","    else:\n","        print(f\"No changes made for pipeline {i['itm_name']}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"04bde15c-e0a1-4614-8281-cba16e4d8d4b"},{"cell_type":"markdown","source":["##### Rebind reports in new DR workspace\n","\n","https://semantic-link-labs.readthedocs.io/en/latest/sempy_labs.report.html#sempy_labs.report.report_rebind"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be81edc8-8d8d-4d78-a3d5-f483cd4a3dc3"},{"cell_type":"code","source":["# Load lookup dataframe for primary item details such as workspace \n","dfolditms = spark.sql(\"\"\"SELECT ir.id old_item_id, ir.displayName old_item_name, wr.Id old_ws_id, wr.Name old_ws_name, \n","                                wks.Id new_ws_id, wks.Name new_ws_name \n","                       FROM items_recovered ir INNER JOIN workspaces_recovered wr on ir.workspaceId = wr.Id \n","                       INNER JOIN workspaces wks on wr.Name = replace(wks.Name,'\"\"\" + p_secondary_ws_suffix+ \"\"\"','')\n","                       where wks.name like '%\"\"\"+p_secondary_ws_suffix+\"\"\"' and wks.id != '\"\"\"+thisWsId+\"\"\"'\"\"\" )\n","\n","dfnewitms = spark.sql(\"\"\"SELECT itm.id new_item_id, itm.displayName new_item_name, wks.Id new_ws_id, wks.Name new_ws_name\n","                       FROM items itm INNER JOIN workspaces wks on itm.workspaceId = wks.Id \n","                       where wks.name like '%\"\"\"+p_secondary_ws_suffix+\"\"\"' and wks.id != '\"\"\"+thisWsId+\"\"\"'\"\"\" )\n","                       \n","ws_sm_sql = \"SELECT wks.ID new_workspace_id, wks.name new_workspace_name, itm.* \" + \\\n","                \"FROM items itm inner join workspaces wks on itm.workspaceId = wks.Id \" + \\\n","                \"AND itm.type = 'Report' and wks.name like '%\"+p_secondary_ws_suffix+\"' \" \\\n","                \"AND wks.id != '\"+thisWsId+\"' order by 1\"\n","\n","ws_sm_sql = \"\"\"select rep.Name, rep.Dataset_Id old_dataset_id, rep_rec.Id old_rep_id, wks_rec.Id old_ws_id, wks_rec.Name old_ws_name, wks.Id new_ws_id, wks.Name new_ws_name \n","from reports rep inner join items itm on rep.Id = itm.Id \n","inner join workspaces wks on itm.workspaceId = wks.Id\n","inner join reports_recovered rep_rec on rep.Name = rep_rec.Name \n","inner join items_recovered itm_rec on itm_rec.Id = rep_rec.Id\n","inner join workspaces_recovered wks_rec on wks_rec.Name = replace(wks.Name,'\"\"\" + p_secondary_ws_suffix+ \"\"\"','') \n","and wks_rec.Id = itm_rec.workspaceId\n","where rep_rec.Id != rep.Id and wks.Name != wks_rec.Name\"\"\"\n","\n","# Ingore workspaces based on the ignore list defined in the parameter section at the top of this notebook\n","if len(p_ws_ignore_list)>0:\n","    ws_sm_sql = ws_sm_sql+ \" and wks.name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        ws_sm_sql  = ws_sm_sql + \" and wks.name not like '%\" + notlike + \"%'\"\n","\n","#print(ws_sm_sql)\n","dfwssm = spark.sql(ws_sm_sql).collect()\n","#display(dfwssm)\n","current_workspace_id =''\n","\n","# Iterate over each dataset in the dataframe\n","for idx,row in enumerate(dfwssm):\n","    if not dfolditms.filter(f\"old_item_id=='{row['old_dataset_id']}'\").isEmpty(): #check to see whether the report dataset is in a workspace from primary\n","        old_dataset_name,old_dataset_ws_id,old_dataset_ws_name, new_dataset_ws_id, new_dataset_ws_name = dfolditms.filter(f\"old_item_id=='{row['old_dataset_id']}'\").collect()[0][1:6]\n","        #old_report_name,old_report_ws_id,old_report_ws_name, new_report_ws_id, new_report_ws_name = dfolditms.filter(f\"old_item_id=='{row['Id']}\").collect()[0][1:6]\n","\n","        print(f\"Rebinding report {row['Name']} in workspace {row['new_ws_name']} with dataset name {old_dataset_name} in target workspace {new_dataset_ws_name}\")\n","        labs.report.report_rebind(report=row['Name'],dataset=old_dataset_name, report_workspace=row['new_ws_name'], dataset_workspace=new_dataset_ws_name)\n","    else:\n","        new_item_name,new_ws_id, new_ws_name = dfnewitms.filter(f\"new_item_id=='{row['old_dataset_id']}'\").collect()[0][1:4]\n","        print(f\"Report {row['Name']} in workspace {row['new_ws_name']} has a dataset {new_item_name} in a workspace {new_ws_name} ({new_ws_id})\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8d488cde-a7d6-463c-9ec0-79d10587f3cf"},{"cell_type":"markdown","source":["##### Clean up - deletes recovered workspaces!!\n","<div style=\"display: flex; align-items: flex-end;\"><img style=\"float: left; margin-right: 10px;\" src=\"https://github.com/hurtn/images/blob/main/stop.png?raw=true\" width=\"50\"><span><h6>Only run the cell below if you are re-testing this process  or do not wish to keep the recovered workspaces in the secondary.<br>Keeping the cell frozen ensures it is not run when the Run all button is used.</span></div>"],"metadata":{},"id":"4050a844-4b4c-4ed6-a3bc-7983920e3afa"},{"cell_type":"code","source":["# Please ensure you have run the workspaceutils command at the top of this notebook before running this cell to ensure all necessary imports and variables are loaded.\n","\n","print('Refreshing the workspaces metadata table...')\n","# Refresh the list of current workspaces\n","saveWorkspaceMeta()\n","\n","thisWsId = notebookutils.runtime.context['currentWorkspaceId'] #obtaining this so we don't accidently delete this workspace!\n","\n","delete_ws_sql = \"SELECT distinct ID,Type,Name FROM workspaces where Name like '%\"+p_secondary_ws_suffix+\"' and id != '\"+thisWsId+\"' and Name != '\" +p_bcdr_workspace_src+\"'\" \n","\n","# Ingore workspaces based on the ignore list defined in the parameter section at the top of this notebook\n","if len(p_ws_ignore_list)>0:\n","    delete_ws_sql = delete_ws_sql+ \" and name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        delete_ws_sql  = delete_ws_sql + \" and name not like '\" + notlike + \"'\"\n","\n","\n","print('Deleting workspaces...')\n","# Get all workspaces created with the prefix set in the parameters at the top so that they can be deleted, except for this workspace of course!\n","df = spark.sql(delete_ws_sql).collect()\n","for i in df:\n","    #print(i['ID'])\n","    if i['Type'] == 'Workspace':\n","      workspaceId = i['ID']\n","      if p_logging_verbose:\n","        print(\"Deleting workspace \"+i['Name'] + '(' + i['ID'] + ')')\n","      response = client.delete(\"/v1/workspaces/\"+workspaceId)\n","      if p_logging_verbose and response.status_code ==200:\n","        print('Successfully deleted')\n","\n","print('Refreshing the workspaces metadata table after deleting recovered workspaces...')\n","# now refresh workspaces\n","saveWorkspaceMeta()\n","print('Done')\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"ed3bdf17-4247-45b7-808c-bd6d8055c578"}],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"sessionKeepAliveTimeout":0,"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"a365ComputeOptions":null,"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}